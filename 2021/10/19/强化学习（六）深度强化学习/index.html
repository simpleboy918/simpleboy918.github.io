

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="从这篇开始，我们将会进入深度强化学习。所谓深度强化学习，其实就是用深度神经网络辅助强化学习。">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <meta name="description" content="从这篇开始，我们将会进入深度强化学习。所谓深度强化学习，其实就是用深度神经网络辅助强化学习。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习（六）深度强化学习">
<meta property="og:url" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="小韭菜">
<meta property="og:description" content="从这篇开始，我们将会进入深度强化学习。所谓深度强化学习，其实就是用深度神经网络辅助强化学习。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlsu1e2msj60ee03qq2x02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlsv928lvj60dy04qt8n02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlswizt2mj60kb0du74y02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlsygmj15j61400kr75s02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlt6gjr5kj61400k00xo02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlt6zer2sj609z037glr02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlt8ghi59j607n07faa402.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvltam1i55j60ft08smxp02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlup26uhoj61400o4taa02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvluqrxwayj60m40eigm802.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlur7tjiaj60j90beglw02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlut9iwnaj60mc09yq3702.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvluv9p3xlj60l602wglm02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlvb3zu86j60mc09yq3702.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvm2iqegncj60ps06adg902.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvm2j2dtg0j30el02mmx8.jpg">
<meta property="article:published_time" content="2021-10-19T13:11:42.000Z">
<meta property="article:modified_time" content="2021-10-21T06:58:13.948Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="DQN">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://www.xiaojiucai.cn/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlsu1e2msj60ee03qq2x02.jpg">
  
  <title>强化学习（六）深度强化学习 - 小韭菜</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"www.xiaojiucai.cn","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":120,"cursorChar":"|","loop":true},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>小韭菜</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                时光轴
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于我
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="强化学习（六）深度强化学习">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-10-19 21:11" pubdate>
        2021年10月19日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      18k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      57 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">强化学习（六）深度强化学习</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2 天前
                
              </p>
            
            <div class="markdown-body">
              <p>从这篇开始，我们将会进入<strong>深度</strong>强化学习。所谓<strong>深度</strong>强化学习，其实就是用<strong>深度神经网络</strong>辅助强化学习。</p>
<span id="more"></span>

<p>但有同学可能会对深度神经网络还不是很了解。这里强烈推荐同学看一下吴恩达老师的课程。对于入门同学非常友好。</p>
<p>但如果同学希望对神经网络有一个梗概的了解，那可以看看本篇。本篇将以<strong>手写数字识别</strong>作为示例，看一下如何直观理解深度神经网络。</p>
<h2 id="神奇的深度神经网络"><a href="#神奇的深度神经网络" class="headerlink" title="神奇的深度神经网络"></a>神奇的深度神经网络</h2><p>不知道大家有没有发现，在日常生活中，我们明明知道一些东西和另外一些东西是有关联的，但我们却无法描述。</p>
<p>举个例子： 手写数字的辨识：我们明明知道一个手写数字是什么，但我们却没有办法用语言描述。</p>
<p>数字8</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlsu1e2msj60ee03qq2x02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>不同人写的数字8，长得都很像，所以大家能够一眼分辨出来8长的是什么样子。 但具体来说，每个8的样子都长的不太一样：虽然我们知道8由两个圈圈组成，一上一下，但精确到某个像素应该白色还是黑色？圈圈的大小如何？一定要完美的圈圈才是数字8吗？这些问题我们却很难回答。</p>
<p>再举个例子，语音翻译： 我们希望记录人们的语音，然后翻译为对应的文字。 其实我们明知道语音和文字之间是有关系的，不同人对同样的文字发音不完全相同，但我们人类是能识别并翻译出来。 我们确没有办法精确描述某个发音（声纹）准确获得对应的文字。</p>
<p>不能描述的后果很严重，这以为着我们没有办法用代码的形式把规则写下来。也就意味着，我们不能把这部分工作<strong>自动化</strong>。</p>
<p>这就是为什么我们的机器能够替代体力劳动的工作，但不能替代脑力劳动。这是因为脑力劳动的工作，相对体力劳动要更复杂。</p>
<p>但深度学习出现后，改变了这个想法。</p>
<h2 id="超简深度神经网原理"><a href="#超简深度神经网原理" class="headerlink" title="超简深度神经网原理"></a>超简深度神经网原理</h2><p>假如，我们明明知道X，y有关系，那么我们不妨先设这个关系可以通过函数Magic(X)获得。也就是说Magic(X)=y。</p>
<p>这在手写数字识别中，X就是需要识别的图片，y就是识别出来的数字分类。</p>
<p>我们的任务就是需要求这个Magic函数。</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlsv928lvj60dy04qt8n02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>在传统方法，我们需要写一些系列规则来表达这个函数。但在深度神经网络，我们会改变一下思路。</p>
<p>现在我们假设有另外一个函数Magic’(),这个函数是由深度神经网络构成。至于是怎样构成，我们后面再说。</p>
<p>在刚开始的时候，很明显Magic’(X) 并不等于y，例如输入手写图片8，Magic’()计算后，认为数字8只有20%，但数字9有40%。</p>
<p>…但这没所谓，因为这是刚开始。我们的任务是让Magic’(X)产生的结果y’ 和 y尽量接近。</p>
<p>y’和真实y之间的差距，我们叫损失，也就是loss。有时候我们也会把y称为目标(target)，因为我们的任务就是让Magic’(X)越来越靠近这个目标。</p>
<p>衡量loss的方法有很多，定义不同loss对神经网络学习有着重大差别，这个话题太大，我们暂时不展开。</p>
<p>loss越大，表示和目标差距越远；loss越小，表示和目标越近，当小到一定值，那么我们就可以认为Magic’(X)和我们要的magic(X)函数非常接近，因为我们可以通过Magic’(X)计算出y。</p>
<p>当我们有许许多多这样的y，经过许许多多轮后。Magic’就越来越贴近Magic。也就是说X和y之间的关系就能越来越好地表达出来。</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlswizt2mj60kb0du74y02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>最后，我们就说这个Magic函数，学习到某种能力。</p>
<p>当然，上面的描述实在太简化，这个Magic函数能否训练出来，我们选取的loss函数，神经网络的结构，数据的大小，质量等很多很多学问。如果想入门，再次强烈推荐吴恩达老师的课程。</p>
<p>但我们目前把关注点放在强化学习的算法，我们不妨先把神经网络看成一个万能的Magic函数。我们后续在分析代码的时候，会把一些简单的原理介绍给大家。</p>
<h2 id="放大镜下深度神经网络"><a href="#放大镜下深度神经网络" class="headerlink" title="放大镜下深度神经网络"></a>放大镜下深度神经网络</h2><p>为了用代码构建深度神经网络，我们有必要解剖一下，深度神经网络到底是怎样构造的。</p>
<p>现在我们可以把深度神经网络的Magic函数，看成是一个数据加工厂。而X就是要进行加工的数据。</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlsygmj15j61400kr75s02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>为了让这个数据加工厂运行得更快，通常我们需要把要加工的数据X变得更‘标准’一些。</p>
<p>例如图片的尺寸大小，有多少通道的颜色等等，然后分批(一般称为batch)，输入工厂。</p>
<p>在输入工厂的时候，会有一个‘大门’，我们称为输入层，去检查数据是否已经按照工厂的标准整理好。</p>
<p>数据工厂里有很多车间，按照流水线排列。和一般的自动化车间一样，我们需要定义好这个车间的操作标准。</p>
<p>我们一般称这些车间叫<strong>层</strong>。这些层都已经封装好在tensorflow、tensorlayer、pytorch等里面了。常用的层包括：Dense、Conv2D、LSTM、Reshape、Flatten等。</p>
<p>最终，数据工厂会把原数据X，加工成产品y’(也叫做：logits)。从源数据加工成产品的过程，我们叫<strong>正向传播</strong>。</p>
<p>但产品y’是否是一个合格的产品，我们还需要我们真正的y(也叫做:lables)作为标准去鉴定。我们把鉴定出来的差距就是loss。</p>
<p>工厂根据鉴定结果，以<strong>梯度下降</strong>的方式，<strong>反向传递</strong>给每个车间，告诉车间要如何调整各自的参数，让源数据和产出y’能够对应起来。</p>
<p>经过N个批次（batch）的数据输入，然后鉴别，工厂调整。最后工厂就能达到我们的生产标准了。也就是说magic函数已经被训练好了。</p>
<h2 id="构建深层神经网络"><a href="#构建深层神经网络" class="headerlink" title="构建深层神经网络"></a>构建深层神经网络</h2><p>我们以fashion_mnist为例，看看我们应该怎样构建深层的神经网络。</p>
<p><em>其实构建深层神经网络的方法是多种多样的，有许多更为简便的方式，例如keras。 在这里，我们以tensorlayer作为例子，主要有两个考量： 1. 在我们往后的示例代码，基本上都用tensorlayer，我们可以借此熟悉一下； 2. 这里用上了自动求导的方式，会更加灵活，也让大家看清楚这个基本流程。</em></p>
<h3 id="Fahsion-MNIST-数据介绍"><a href="#Fahsion-MNIST-数据介绍" class="headerlink" title="Fahsion_MNIST 数据介绍"></a>Fahsion_MNIST 数据介绍</h3><p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlt6gjr5kj61400k00xo02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>Fashion MNIST是一个非常适合入门，又好玩的数据集，做完之后保证成就感满满。</p>
<p>一般来说，数据集包含了训练集和测试集。如图：</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlt6zer2sj609z037glr02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>在有监督学习下，一般我们会把数据分为训练集和测试集。就如名字一样，训练集就是用来训练模型的，测试集就是为了测试模型是否真的管用的。</p>
<p>其中每种数据集合中包含了图片和标签，例如：</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlt8ghi59j607n07faa402.jpg" srcset="/img/loading.gif" lazyload></p>
<p>图片，对应的标签是[鞋子]分类。</p>
<p>当然并不是所有的数据集，都是图片，也有可能是数据。例如天气预测，数据可能是以往的天气数据，例如前一天的气温，气压，湿度等。标签就是当天的天气。</p>
<p>在Fashion MNIST里，训练集和测试集里面的数据，都已经为大家对应好了。</p>
<p>现在我们要训练一个神经网络，用训练集的数据，学习根据图像，去判别这是什么服装类型。然后把这个神经网络用在测试集上，看看正确率有多少。</p>
<h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a><strong>加载数据集</strong></h3><p>现在，我们可以用这么一个语句，来加载Fashion MNIST数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()<br></code></pre></td></tr></table></figure>

<p>train_images：训练集图片 train_labels：训练集标签 test_images：测试集图片 test_labels：测试集标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(train_images.shape)<br><span class="hljs-built_in">print</span>(train_labels.shape)<br><span class="hljs-built_in">print</span>(test_images.shape)<br><span class="hljs-built_in">print</span>(test_labels.shape)<br></code></pre></td></tr></table></figure>

<p>打印结果：</p>
<p>(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)</p>
<p>也就是说 训练集含有：60,000张图像，对应60,000个标签； 测试集含有：10,000张图像对应10,000个标签。</p>
<p>而每张图的大小为 28 x 28。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>注意：在神经网络的相关编程里面，有两个东西大家必须要留意，一个是数据的shape，一个是数据的type。</p>
<p>我们输入神经网络的数据，往往不是一个单一的数值。而是很多数值组成的形状。</p>
<p>例如一张图28x28大小灰度图。从数据层面来看它是由28x28个数值表示表示出来的。那么我们说这个数据的shape就是28x28。</p>
<p>我们说一张灰度图是二维的，也有三维数据，例如彩色图片，是由RGB三层数据表示。</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvltam1i55j60ft08smxp02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>当然，也有更高纬度的数据。数据在输入到神经网络之前，都要整理好形状。</p>
<p>而我们说的数据的type，是指数据的每一个数值的类型。一般来说，我们会用32位浮点数(float32)。</p>
<p>在Fashion_Mnist每张图的大小为28x28，每个像素值处于0到255之间。</p>
<p>因为我们的像素，除以0到255之间，因此我们需要除以255，让数据保持在0-1之间。</p>
<h3 id="建立深度神经网络的一般步骤"><a href="#建立深度神经网络的一般步骤" class="headerlink" title="建立深度神经网络的一般步骤"></a>建立深度神经网络的一般步骤</h3><h4 id="1、创建一个深度神经网络和优化器"><a href="#1、创建一个深度神经网络和优化器" class="headerlink" title="1、创建一个深度神经网络和优化器"></a>1、<strong>创建一个深度神经网络和优化器</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_model</span>(<span class="hljs-params">inputs_shape</span>):</span><br>    inputs = tl.layers.Input(inputs_shape)<br>    cnn = tl.layers.Conv2d(n_filter=<span class="hljs-number">32</span>,filter_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),act=<span class="hljs-string">&#x27;relu&#x27;</span>,padding=<span class="hljs-string">&#x27;SAME&#x27;</span>)(inputs)<br>    flatten = tl.layers.Flatten()(cnn)<br>    fc = tl.layers.Dense(n_units=<span class="hljs-number">128</span>, act=<span class="hljs-string">&#x27;relu&#x27;</span>)(flatten)<br>    outputs = tl.layers.Dense(n_units=<span class="hljs-number">10</span>,act=tf.nn.softmax)(fc)<br>    <span class="hljs-keyword">return</span> tl.models.Model(inputs=inputs, outputs=outputs)<br></code></pre></td></tr></table></figure>

<p>这里，我们采用的是函数式的方式建立深度神经网络。</p>
<p>我们之前讲过，神经网络的层，就相当于一个流水线工厂的车间，每个车间独立承担了对数据的一个操作。</p>
<p>inputs = tl.layers.Input(inputs_shape) 输入层相当于工厂的大门，检测输入数据是否符合工厂定义的标准。在Fashion MNIST中，我们的输入层需要接受一个batch大小的数据，每个数据是28 x 28 大小。batch大小我们暂时不设定，可以预留，我们用None表示。所以，我们的imputs_shapes=[None,28,28]</p>
<p>flatten = tl.layers.Flatten()(inputs) flatten层，我们需要把这28 x 28像素的图，变成一个长条形的数据，以方便后面接入全连接层。所以我们用上Flatten层，把数据变成[None,784]</p>
<p>fc = tl.layers.Dense(n_units=128, act=’relu’)(flatten) 现在我们加上全连接层，全连接层有128个单元，和原来的784个单元连接。在前面inputs和flatten两层，我们并没有对数据内容进行加工。而Dense是一种对数据内容进行计算的操作。</p>
<p>outputs = tl.layers.Dense(n_units=10,act=tf.nn.softmax)(fc) 在输出层，我们同样用上全连接层，输出10个单元，对应我们需要输出的10个分类，并用上softmax作为激活函数。</p>
<p>细心读代码的同学可能已经明白，以fc层为例：代码前一段：tl.layers.Dense(n_units=128, act=’relu’)是相当于车间。后面(flatten)是上一层车间产出的结果。函数式的模型定义方式，就是把上一层的输出，放入到输入，从而连成一条数据的流水线。</p>
<p>tl.models.Model(inputs=inputs, outputs=outputs) 流水线都定义好了，我们还需要一行代码，定义好工厂。工厂的定义很简单，只需要把工厂的大门和输入定义好就可以了。</p>
<h4 id="2、开始训练"><a href="#2、开始训练" class="headerlink" title="2、开始训练"></a>2、开始训练</h4><p>在训练的时候，我们会用到tensorflow2.0的<strong>自动求导机制</strong>。</p>
<p>我们先定义梯度带，并在梯度带中，描述数据从源数据开始到产品logits的，然后通过loss函数，求loss的全过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>     logits = network(images)<br>     loss = tf.losses.sparse_categorical_crossentropy(labels,logits)<br></code></pre></td></tr></table></figure>

<p>loss函数的定义是非常高深的学问，幸好，常用的loss函数已经被提取成工具，对于新手的我们只需要按需使用即可。</p>
<p>常用的loss函数有4个，在使用之前我们需要先考虑，我们在解决的是分类问题还是回归问题。 <strong>分类问题</strong>就是把原数据按照标签分类的任务： 一种是像我们正在解决的Fashion MNIST，是要分成多个类别的。这时候我们一般sparse_categorical_crossentropy 如果只需要分两类，那么我们可以用。 sigmoid_cross_entropy_with_logits</p>
<p><strong>回归问题</strong>是试图从凌乱的数据中寻着规律的问题。 虽然不太准确，但大家可以想象一下：我们的神经网络在产生一个曲线，神经网络在训练的时候扭动曲线（或者是曲面或者是更高纬度的东西），去连接每一个数据点。</p>
<p>如果你不清楚，很简单，你发现不是分类问题，那么一般来说就是回归问题。</p>
<p>对于回归问题，我们可以MSE均值方差。 loss = tf.reduce_mean(tf.square(logits, labels))</p>
<p>我们都定义loss的计算过程，就可以利用梯度带自动求导了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">grads = tape.gradient(loss, network.trainable_weights)<br>optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(grads,network.trainable_weights))<br></code></pre></td></tr></table></figure>

<p>第一行，我们是用梯度带求导。获得对网络中每个参数的导数。 第二行，我们用优化器，对参数进行求导。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = tf.optimizers.Adam()<br></code></pre></td></tr></table></figure>

<p>而优化器（optimizer），我们在之前定义了使用Adam()，</p>
<p>这行代码可以变化的不多，如果大家不明白，可以先记起来。</p>
<h4 id="3-展示训练效果"><a href="#3-展示训练效果" class="headerlink" title="3. 展示训练效果"></a>3. 展示训练效果</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>():</span><br>    acc = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(test_lables.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">if</span> np.argmax(predictions[i])==test_lables[i]:<br>            acc += <span class="hljs-number">1</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;accurcy:%f&#x27;</span>%(acc/test_lables.shape[<span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure>

<p>最终，测试集准确率约90%。事实上这是个很一般的成绩，也有很多可以优化的地方。</p>
<p>这篇文章旨在给新手同学一些关于深度神经网络的指引，有兴趣的同学可以优化模型。</p>
<p>如果感觉到困难，也不用慌，我们将会在每个算法的实战篇中详细介绍，不断加深大家对<strong>深度</strong>强化学习的理解。</p>
<p>接下来，我们将会回到DQN。</p>
<h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>上面，我们了解了深层神经网络到底是啥一回事。现在我们把深层神经网络这套工具，用到强化学习中。</p>
<p>我们回想一下我们Qleaning，我们需要解决连续状态的问题。那么我们能否用神经网络解决呢？这个就是DQN的思路。</p>
<h3 id="Deep-network-Qlearning-DQN"><a href="#Deep-network-Qlearning-DQN" class="headerlink" title="Deep network + Qlearning = DQN"></a>Deep network + Qlearning = DQN</h3><p>我们先回归一下Qlearning。</p>
<p>在Qlearning中，我们有一个Qtable，记录着在每一个状态下，各个动作的Q值。</p>
<p>Qtable的作用是当我们输入状态S，我们通过<strong>查表</strong>返回能够获得最大Q值的动作A。也就是我们需要找一个S-A的对应关系。</p>
<p>这种方式很适合格子游戏。因为格子游戏中的每一个格子就是一个状态，但在现实生活中，很多状态并不是<strong>离散</strong>而是<strong>连续</strong>的。而且当</p>
<p>例如在GYM中经典的CartPole游戏，杆子的角度是<strong>连续</strong>而不是<strong>离散</strong>的。在Atari游戏中，状态也是连续的。</p>
<p>遇到这些情况，Qtable就没有办法解决。</p>
<p>我们刚才说了Qtable的作用就是找一个S-A的对应关系。所以我们就可以用一个函数F表示，我们有F(S) = A。这样我们就可以不用查表了，而且还有个好处，函数允许<strong>连续</strong>状态的表示。</p>
<p>这时候，我们深度神经网络就可以派上用场了。因为我们之前说过，神经网络就可以看成一个万能的函数。</p>
<p>这个万能函数接受输入一个状态S，它能告诉我，每个动作的Q值是怎样的。</p>
<p>为了大家能更好理解神经网络的作用，我们可以从另外一个角度理解。 把Qtable三维可视化，就会得到这样一个图。</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlup26uhoj61400o4taa02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>图中每根柱子的高度，表示状态S下，选择动作A的Q值。</p>
<p>现在我们用函数来表示，<strong>相当于我们要扭曲一条曲线，这条曲线穿过了离散状态下的所有点。</strong></p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvluqrxwayj60m40eigm802.jpg" srcset="/img/loading.gif" lazyload></p>
<p>我们从二维的角度再看一下：</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlur7tjiaj60j90beglw02.jpg" srcset="/img/loading.gif" lazyload></p>
<p><em>当然，在深度强化学习中，有很多数据的纬度相当高的。这里只是提供一个思考的角度。让大家能够可视化地理解DQN的思路。</em></p>
<p>现在我们就很清楚了，其实Qlearning和DQN并没有根本的区别。只是DQN用神经网络，也就是一个函数替代了原来Qtable而已。</p>
<h3 id="神经网络的目标"><a href="#神经网络的目标" class="headerlink" title="神经网络的目标"></a>神经网络的目标</h3><p>我们之前也说过，神经网络需要解决一个问题，就是更新目标怎么设置。</p>
<p>在手写数字识别等有监督学习的数据集中，有标签好的数据，也就是我们的目标是很明确的。那么在DQN中，我们的目标是什么呢？</p>
<p>答案其实就在Qlearning一样。 还记得吗？在Qlearning，我们用<strong>下一状态St+1的最大Q值替代St+1的V值</strong>。<strong>V(St+1)加上状态转移产生的奖励R</strong>。就是Q(S,a)的更新目标。</p>
<p>我们来看下图：</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlut9iwnaj60mc09yq3702.jpg" srcset="/img/loading.gif" lazyload></p>
<p>假设我们需要更新当前状态St下的某动作A的Q值：Q(S,A),我们可以这样做： 1. 执行A，往前一步，到达St+1; 2. 把St+1输入Q网络，计算St+1下所有动作的Q值； 3. 获得最大的Q值加上奖励R作为更新目标； 4. 计算损失：Q(S,A)相当于有监督学习中的logits； maxQ(St+1) + R 相当于有监督学习中的lables ； 用mse函数，得出两者的loss 5. 用loss更新Q网络。</p>
<p>也就是，我们用Q网络估算出来的两个相邻状态的Q值，他们之间的距离，就是一个r的距离。</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvluv9p3xlj60l602wglm02.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>其实DQN就是Qlearning扔掉Qtable，换上深度神经网络。</li>
<li>我们知道，解决连续型问题，如果表格不能表示，就用函数，而最好的函数就是深度神经网络。</li>
<li>和有监督学习不同，深度强化学习中，我们需要自己找更新目标。通常在马尔可夫链体系下，两个相邻状态状态差一个奖励r经常能被利用。</li>
</ol>
<p>DQN其实没有什么神秘的，不是吗？</p>
<h2 id="DQN代码实战"><a href="#DQN代码实战" class="headerlink" title="DQN代码实战"></a>DQN代码实战</h2><p>于是我们可以用Qlearning算法修改一下，变成DQN的算法。</p>
<p>其实和Qlearning很像，我们只修改一下。 </p>
<ol>
<li>初始化一个网络，用于计算Q值。（在Qlearning，我们用Qtable） </li>
<li>我们开始一场游戏。 </li>
<li>随机从一个状态s开始。 </li>
<li>我们把s输入到Q，计算s状态下，a的Q值 Q（s） </li>
<li>我们选择能得到最大Q值的动作，a </li>
<li>我们把a输入到环境，获得新状态s’,r,done </li>
<li>计算目标 y = r + gamma * maxQ(s’) 8、训练Q网络，缩小Q（s，a）和y 的大小 </li>
<li>开始新一步，不断更新</li>
</ol>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvlvb3zu86j60mc09yq3702.jpg" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Deep Q-Network Q(a, s)</span><br><span class="hljs-string">-----------------------</span><br><span class="hljs-string">TD Learning, Off-Policy, e-Greedy Exploration (GLIE).</span><br><span class="hljs-string">Q(S, A) &lt;- Q(S, A) + alpha * (R + lambda * Q(newS, newA) - Q(S, A))</span><br><span class="hljs-string">delta_w = R + lambda * Q(newS, newA)</span><br><span class="hljs-string">See David Silver RL Tutorial Lecture 5 - Q-Learning for more details.</span><br><span class="hljs-string">Reference</span><br><span class="hljs-string">----------</span><br><span class="hljs-string">original paper: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</span><br><span class="hljs-string">EN: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.5m3361vlw</span><br><span class="hljs-string">CN: https://zhuanlan.zhihu.com/p/25710327</span><br><span class="hljs-string">Note: Policy Network has been proved to be better than Q-Learning, see tutorial_atari_pong.py</span><br><span class="hljs-string">Environment</span><br><span class="hljs-string">-----------</span><br><span class="hljs-string"># The FrozenLake v0 environment</span><br><span class="hljs-string">https://gym.openai.com/envs/FrozenLake-v0</span><br><span class="hljs-string">The agent controls the movement of a character in a grid world. Some tiles of</span><br><span class="hljs-string">the grid are walkable, and others lead to the agent falling into the water.</span><br><span class="hljs-string">Additionally, the movement direction of the agent is uncertain and only partially</span><br><span class="hljs-string">depends on the chosen direction. The agent is rewarded for finding a walkable</span><br><span class="hljs-string">path to a goal tile.</span><br><span class="hljs-string">SFFF       (S: starting point, safe)</span><br><span class="hljs-string">FHFH       (F: frozen surface, safe)</span><br><span class="hljs-string">FFFH       (H: hole, fall to your doom)</span><br><span class="hljs-string">HFFG       (G: goal, where the frisbee is located)</span><br><span class="hljs-string">The episode ends when you reach the goal or fall in a hole. You receive a reward</span><br><span class="hljs-string">of 1 if you reach the goal, and zero otherwise.</span><br><span class="hljs-string">Prerequisites</span><br><span class="hljs-string">--------------</span><br><span class="hljs-string">tensorflow&gt;=2.0.0a0</span><br><span class="hljs-string">tensorlayer&gt;=2.0.0</span><br><span class="hljs-string">To run</span><br><span class="hljs-string">-------</span><br><span class="hljs-string">python tutorial_DQN.py --train/test</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> gym<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> tensorlayer <span class="hljs-keyword">as</span> tl<br><br><span class="hljs-comment"># add arguments in command  --train/test</span><br><span class="hljs-comment"># 关于argparase的应用，可以看看我这篇知乎专栏：</span><br><span class="hljs-comment"># 小段文讲清argparse模块基本用法[小番外]</span><br><span class="hljs-comment"># https://zhuanlan.zhihu.com/p/111010774</span><br><span class="hljs-comment"># 注意：原代码默认为test，我改为了train。</span><br>parser = argparse.ArgumentParser(description=<span class="hljs-string">&#x27;Train or test neural net motor controller.&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--train&#x27;</span>, dest=<span class="hljs-string">&#x27;train&#x27;</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>, default=<span class="hljs-literal">True</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--test&#x27;</span>, dest=<span class="hljs-string">&#x27;test&#x27;</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>, default=<span class="hljs-literal">False</span>)<br>args = parser.parse_args()<br><br>tl.logging.set_verbosity(tl.logging.DEBUG)<br><br><span class="hljs-comment">#####################  hyper parameters  ####################</span><br>lambd = <span class="hljs-number">.99</span>  <span class="hljs-comment"># 折扣率(decay factor)</span><br>e = <span class="hljs-number">0.1</span>  <span class="hljs-comment"># epsilon-greedy算法参数，越大随机性越大，越倾向于探索行为。</span><br>num_episodes = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 迭代次数</span><br>render = <span class="hljs-literal">False</span>  <span class="hljs-comment"># 是否渲染游戏</span><br>running_reward = <span class="hljs-literal">None</span><br><br><br><span class="hljs-comment">##################### DQN ##########################</span><br><br><span class="hljs-comment">## 把分类的数字表示，变成onehot表示。</span><br><span class="hljs-comment"># 例如有4类，那么第三类变为：[0,0,1,0]的表示。</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_one_hot</span>(<span class="hljs-params">i, n_classes=<span class="hljs-literal">None</span></span>):</span><br>    a = np.zeros(n_classes, <span class="hljs-string">&#x27;uint8&#x27;</span>)  <span class="hljs-comment"># 这里先按照分类数量构建一个全0向量</span><br>    a[i] = <span class="hljs-number">1</span>  <span class="hljs-comment"># 然后点亮需要onehot的位数。</span><br>    <span class="hljs-keyword">return</span> a<br><br><br><span class="hljs-comment">## Define Q-network q(a,s) that ouput the rewards of 4 actions by given state, i.e. Action-Value Function.</span><br><span class="hljs-comment"># encoding for state: 4x4 grid can be represented by one-hot vector with 16 integers.</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_model</span>(<span class="hljs-params">inputs_shape</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    定义Q网络模型：</span><br><span class="hljs-string">    1. 注意输入的shape和输出的shape</span><br><span class="hljs-string">    2. W_init和b_init是模型在初始化的时候，控制初始化参数的随机。该代码中用正态分布，均值0，方差0.01的方式初始化参数。</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    ni = tl.layers.Input(inputs_shape, name=<span class="hljs-string">&#x27;observation&#x27;</span>)<br>    nn = tl.layers.Dense(<span class="hljs-number">4</span>, act=<span class="hljs-literal">None</span>, W_init=tf.random_uniform_initializer(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>), b_init=<span class="hljs-literal">None</span>, name=<span class="hljs-string">&#x27;q_a_s&#x27;</span>)(ni)<br>    <span class="hljs-keyword">return</span> tl.models.Model(inputs=ni, outputs=nn, name=<span class="hljs-string">&quot;Q-Network&quot;</span>)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_ckpt</span>(<span class="hljs-params">model</span>):</span>  <span class="hljs-comment"># save trained weights</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    保存参数</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    tl.files.save_npz(model.trainable_weights, name=<span class="hljs-string">&#x27;dqn_model.npz&#x27;</span>)<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_ckpt</span>(<span class="hljs-params">model</span>):</span>  <span class="hljs-comment"># load trained weights</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    加载参数</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    tl.files.load_and_assign_npz(name=<span class="hljs-string">&#x27;dqn_model.npz&#x27;</span>, network=model)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br><br>    qnetwork = get_model([<span class="hljs-literal">None</span>, <span class="hljs-number">16</span>])  <span class="hljs-comment"># 定义inputshape[None,16]。16是state数量</span><br>    qnetwork.train()  <span class="hljs-comment"># 调用tensorlayer的时候，需要标注这个模型是否可以训练。(再次吐槽tenorlayers...)</span><br>    train_weights = qnetwork.trainable_weights  <span class="hljs-comment"># 模型的参数</span><br><br>    optimizer = tf.optimizers.SGD(learning_rate=<span class="hljs-number">0.1</span>)  <span class="hljs-comment"># 定义优化器</span><br>    env = gym.make(<span class="hljs-string">&#x27;FrozenLake-v0&#x27;</span>)  <span class="hljs-comment"># 定义环境</span><br><br>    <span class="hljs-comment"># ======开始训练=======</span><br>    <span class="hljs-keyword">if</span> args.train:<br>        t0 = time.time()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_episodes):<br>            <span class="hljs-comment">## 重置环境初始状态</span><br>            s = env.reset()<br>            rAll = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">99</span>):  <span class="hljs-comment"># 最多探索99步。因为环境状态比较少，99步一般也够探索到最终状态了。</span><br>                <span class="hljs-keyword">if</span> render: env.render()<br><br>                <span class="hljs-comment">## 把state放入network，计算Q值。</span><br>                <span class="hljs-comment">## 注意，这里先把state进行onehote处理，这里注意解释下什么是onehot</span><br>                <span class="hljs-comment">## 输出，这个状态下，所有动作的Q值，也就是说，是一个[None,4]大小的矩阵</span><br>                allQ = qnetwork(np.asarray([to_one_hot(s, <span class="hljs-number">16</span>)], dtype=np.float32)).numpy()<br><br>                <span class="hljs-comment"># 在矩阵中找最大的Q值的动作</span><br>                a = np.argmax(allQ, <span class="hljs-number">1</span>)<br><br>                <span class="hljs-comment"># e-Greedy：如果小于epsilon，就智能体随机探索。否则，就用最大Q值的动作。</span><br>                <span class="hljs-keyword">if</span> np.random.rand(<span class="hljs-number">1</span>) &lt; e:<br>                    a[<span class="hljs-number">0</span>] = env.action_space.sample()<br><br>                <span class="hljs-comment"># 输入到环境，获得下一步的state，reward，done</span><br>                s1, r, d, _ = env.step(a[<span class="hljs-number">0</span>])<br><br>                <span class="hljs-comment"># 把new-state 放入，预测下一个state的**所有动作**的Q值。</span><br>                Q1 = qnetwork(np.asarray([to_one_hot(s1, <span class="hljs-number">16</span>)], dtype=np.float32)).numpy()<br><br>                <span class="hljs-comment">##=======计算target=======</span><br>                <span class="hljs-comment">## 构建更新target：</span><br>                <span class="hljs-comment">#    Q&#x27;(s,a) &lt;- Q(s,a) + alpha(r + lambd * maxQ(s&#x27;,a&#x27;) - Q(s, a))</span><br>                maxQ1 = np.<span class="hljs-built_in">max</span>(Q1)  <span class="hljs-comment"># 下一个状态中最大Q值.</span><br>                targetQ = allQ  <span class="hljs-comment"># 用allQ(现在状态的Q值)构建更新的target。因为只有被选择那个动作才会被更新到。</span><br>                targetQ[<span class="hljs-number">0</span>, a[<span class="hljs-number">0</span>]] = r + lambd * maxQ1<br><br>                <span class="hljs-comment">## 利用自动求导 进行更新。</span><br>                <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>                    _qvalues = qnetwork(np.asarray([to_one_hot(s, <span class="hljs-number">16</span>)], dtype=np.float32))  <span class="hljs-comment"># 把s放入到Q网络，计算_qvalues。</span><br>                    <span class="hljs-comment"># _qvalues和targetQ的差距就是loss。这里衡量的尺子是mse</span><br>                    _loss = tl.cost.mean_squared_error(targetQ, _qvalues, is_mean=<span class="hljs-literal">False</span>)<br>                    <span class="hljs-comment"># 同梯度带求导对网络参数求导</span><br>                grad = tape.gradient(_loss, train_weights)<br>                <span class="hljs-comment"># 应用梯度到网络参数求导</span><br>                optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(grad, train_weights))<br><br>                <span class="hljs-comment"># 累计reward，并且把s更新为newstate</span><br>                rAll += r<br>                s = s1<br><br>                <span class="hljs-comment"># 更新epsilon，让epsilon随着迭代次数增加而减少。</span><br>                <span class="hljs-comment"># 目的就是智能体越来越少进行“探索”</span><br>                <span class="hljs-keyword">if</span> d == <span class="hljs-literal">True</span>:<br>                    e = <span class="hljs-number">1.</span> / ((i / <span class="hljs-number">50</span>) + <span class="hljs-number">10</span>)<br>                    <span class="hljs-keyword">break</span><br><br>            <span class="hljs-comment">## 这里的running_reward用于记载每一次更新的总和。为了能够更加看清变化，所以大部分是前面的。只有一部分是后面的。</span><br>            running_reward = rAll <span class="hljs-keyword">if</span> running_reward <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> running_reward * <span class="hljs-number">0.99</span> + rAll * <span class="hljs-number">0.01</span><br>            <span class="hljs-comment"># print(&quot;Episode [%d/%d] sum reward: %f running reward: %f took: %.5fs &quot; % \</span><br>            <span class="hljs-comment">#     (i, num_episodes, rAll, running_reward, time.time() - episode_time))</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Episode: &#123;&#125;/&#123;&#125;  | Episode Reward: &#123;:.4f&#125; | Running Average Reward: &#123;:.4f&#125;  | Running Time: &#123;:.4f&#125;&#x27;</span> \<br>                  .<span class="hljs-built_in">format</span>(i, num_episodes, rAll, running_reward, time.time() - t0))<br>        save_ckpt(qnetwork)  <span class="hljs-comment"># save model</span><br><br>    <span class="hljs-comment">##============这部分是正式游戏了========</span><br>    <span class="hljs-comment"># 这部分就不讲解了，和训练一样。只是少了epsilon-greedy。</span><br>    <span class="hljs-keyword">if</span> args.test:<br>        t0 = time.time()<br>        load_ckpt(qnetwork)  <span class="hljs-comment"># load model</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_episodes):<br>            <span class="hljs-comment">## Reset environment and get first new observation</span><br>            episode_time = time.time()<br>            s = env.reset()  <span class="hljs-comment"># observation is state, integer 0 ~ 15</span><br>            rAll = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">99</span>):  <span class="hljs-comment"># step index, maximum step is 99</span><br>                <span class="hljs-keyword">if</span> render: env.render()<br><br>                <span class="hljs-comment">## Choose an action by greedily (with e chance of random action) from the Q-network</span><br>                allQ = qnetwork(np.asarray([to_one_hot(s, <span class="hljs-number">16</span>)], dtype=np.float32)).numpy()<br>                a = np.argmax(allQ, <span class="hljs-number">1</span>)  <span class="hljs-comment"># no epsilon, only greedy for testing</span><br><br>                <span class="hljs-comment">## Get new state and reward from environment</span><br>                s1, r, d, _ = env.step(a[<span class="hljs-number">0</span>])<br>                rAll += r<br>                s = s1<br>                <span class="hljs-comment">## Reduce chance of random action if an episode is done.</span><br>                <span class="hljs-keyword">if</span> d == <span class="hljs-literal">True</span>:<br>                    <span class="hljs-comment"># e = 1. / ((i / 50) + 10)  # reduce e, GLIE: Greey in the limit with infinite Exploration</span><br>                    <span class="hljs-keyword">break</span><br><br><br>            <span class="hljs-comment">## Note that, the rewards here with random action</span><br>            running_reward = rAll <span class="hljs-keyword">if</span> running_reward <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> running_reward * <span class="hljs-number">0.99</span> + rAll * <span class="hljs-number">0.01</span><br>            <span class="hljs-comment"># print(&quot;Episode [%d/%d] sum reward: %f running reward: %f took: %.5fs &quot; % \</span><br>            <span class="hljs-comment">#     (i, num_episodes, rAll, running_reward, time.time() - episode_time))</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Episode: &#123;&#125;/&#123;&#125;  | Episode Reward: &#123;:.4f&#125; | Running Average Reward: &#123;:.4f&#125;  | Running Time: &#123;:.4f&#125;&#x27;</span>\<br>            .<span class="hljs-built_in">format</span>(i, num_episodes, rAll, running_reward,  time.time()-t0 ))<br></code></pre></td></tr></table></figure>

<h3 id="Epsilon-greedy"><a href="#Epsilon-greedy" class="headerlink" title="Epsilon-greedy"></a>Epsilon-greedy</h3><p>我们先来说一个在示例代码中的实做技巧。</p>
<p>还记得我们之前在Qlearning使用了noisy-greedy的方式，保持探索和开发之间的平衡么？</p>
<p>智能体会根据Q值表选择Q值最大的动作。但在选择动作之前，会先给Q值加上一个随机的噪音，使得最终的最大值带有一定的随机性；随着游戏进行，噪音会逐渐减少，最终噪音将会小得不再影响智能体的决定。</p>
<p>在DQN，你同样可以使用增加噪音的方式，不过，在示例代码中，我们用另外一种方式取进行。这就是更常用的Epsilon-greedy。其目的是相同的，就是为了保证大部分的state都能被探索到的基础上，最终也能够按照智能体学习到的方式取进行。</p>
<p>请大家先留一下示例代码中：e = 0.1，这里的e就代表Epsilon，可以理解为一个门槛。</p>
<p>在选择动作的时候，会先随机一个[0,1]之间的值： 如果随机出来的值，高于这个门槛，智能体将会按照原来的<strong>贪婪算法</strong>，直接选择最大的Q值的动作。这就是开发性的动作。 </p>
<p>如果随机出来的值，低于这个门槛，智能体将会忽略掉Q值，而直接随机一个动作。这就是<strong>探索性的动作</strong>。 </p>
<p>Epsilon-greedy用大白话说就是：如果随机出来的值大于Epsilon这个门槛，我们就用greedy算法吧！</p>
<p>我们看示例代码中，每一次迭代（每一次游戏）进行会调整一次e。</p>
<p>e = 1. / ((i / 50) + 10) </p>
<p>随着i越来越大，e将会越来越小。也就是说，门槛会随着迭代次数，越来越小，我们执行greedy算法的机会将会越来越多。让智能体逐渐从<strong>探索</strong>变为<strong>开发</strong>。</p>
<h3 id="具体算法的解释："><a href="#具体算法的解释：" class="headerlink" title="具体算法的解释："></a>具体算法的解释：</h3><p>Qlearning的更新流程一样。其实，如果你已经明白了深度神经网络和Qlearning，那么代码看起来是很简单了。</p>
<p>1、我们先建立一个</p>
<h3 id="建立网络"><a href="#建立网络" class="headerlink" title="建立网络"></a>建立网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_model</span>(<span class="hljs-params">inputs_shape</span>):</span><br>    ni = tl.layers.Input(inputs_shape, name=<span class="hljs-string">&#x27;observation&#x27;</span>)<br>    nn = tl.layers.Dense(<span class="hljs-number">4</span>, act=<span class="hljs-literal">None</span>, W_init=tf.random_uniform_initializer(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>), b_init=<span class="hljs-literal">None</span>, name=<span class="hljs-string">&#x27;q_a_s&#x27;</span>)(ni)<br>    <span class="hljs-keyword">return</span> tl.models.Model(inputs=ni, outputs=nn, name=<span class="hljs-string">&quot;Q-Network&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><em>示例代码用上了tensorlayer，一般来说用tensorflow的也很方便。这里必须吐槽一下，tensorlayer和tensorflow，同样的层，同样功能的参数，名字居然不一样！还有keras里面的layer。各种坑。 但我们后面的代码都用上了tensorlayer，so。。。</em></p>
<p>输入层：示例代码中用ni表示。inputs_shape，就是输入矩阵的形状。在强化学习中，我们需要把状态输入到网络中，所以inputs_shape就是状态的形状了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">qnetwork = get_model([<span class="hljs-literal">None</span>, <span class="hljs-number">16</span>])<br></code></pre></td></tr></table></figure>

<p>后面我们调用的时候，输入的是[None, 16]。注意，前面是状态的数量，也就是说，会输入一个batch大小的数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn = tl.layers.Dense(<span class="hljs-number">4</span>, act=<span class="hljs-literal">None</span>, W_init=tf.random_uniform_initializer(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>), b_init=<span class="hljs-literal">None</span>, name=<span class="hljs-string">&#x27;q_a_s&#x27;</span>)(ni)<br></code></pre></td></tr></table></figure>

<p>和之前Fasion MNIST一样，我们用Dense作为输出。这里没有加一个Dense层，主要是因为这里的状态空间只有16个，这样的模型拟合能力已经足够。</p>
<p>在更新的时候，有两个地方我们要注意。</p>
<p><strong>一、 热独编码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">allQ = qnetwork(np.asarray([to_one_hot(s, <span class="hljs-number">16</span>)], dtype=np.float32)).numpy()<br></code></pre></td></tr></table></figure>

<p>一、这里我们用了to_one_hot函数，把原来的编码，变成<strong>热独编码</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_one_hot</span>(<span class="hljs-params">i, n_classes=<span class="hljs-literal">None</span></span>):</span><br>    a = np.zeros(n_classes, <span class="hljs-string">&#x27;uint8&#x27;</span>)<br>    a[i] = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> a<br></code></pre></td></tr></table></figure>

<p>那什么是热独编码呢？</p>
<p>现在我们有16个不同的state，如果按照一般的分类，可能会把这16个state变成，1,2,3,…,15,16 类。</p>
<p>在机器学习里面，0,1,2,3..这些数字其实是包含了一个大小的关系的，但我们在分类问题里面，其实每一类都是平等的，并没有大小关系。因此，我们需要把他变成热独编码(One hot)的形式。</p>
<p>热独编码这个名字其实很形象,就是<strong>只有一点是热的</strong>。</p>
<p>我们构造一个矩阵，只有该分类的标志位为1，其他全部为0.矩阵的大小，就是类别的个数。所以热独热独，热就是1表示的位置。</p>
<p>例如 类别1：[1 0 0 0 0 0 0 0 0….0] 类别2：[0 1 0 0 0 0 0 0 0….0] 类别16：[0 0 0 0 0 0 0 0 0….1]</p>
<p>回到我们示例代码的to_one_hot函数就很好理解了： 我们先构造一个全零的矩阵，大小是[1,n_classes] (n_classes类别个数)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.zeros(n_classes, <span class="hljs-string">&#x27;uint8&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>然后在对应的标志位上，把0变成1，并返回。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">a[i] = 1<br></code></pre></td></tr></table></figure>

<p><strong>二、数据的形状和格式</strong></p>
<p>输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.asarray([to_one_hot(s, <span class="hljs-number">16</span>)], dtype=np.float32)<br></code></pre></td></tr></table></figure>

<p>我们之前说过，神经网络其实可以看做是一个数据的工厂，而在实际写代码的过程中，这个数据工厂有时候并不那么透明，或者并不那么可读。可以说，数据工厂就像一个黑盒子一样，在debug中出来问题，还是比较棘手的。</p>
<p>所以我们要在输入之前，保证这个工厂的数据是标准的。最好做到以下3点：</p>
<ol>
<li>神经网络输入的形状需要和input_shape对应;</li>
<li>输入格式必须是一个array;</li>
<li>数据的格式，最好都变成float32的形式。</li>
</ol>
<p>在输入之前能先处理好这3点，能够减少很多不必要的麻烦。</p>
<p>输出：**.numpy()**</p>
<p>我们数据在输入神经网络这个工厂之后，将会变成另外一种格式：tensor（张量）。最终产生的数据格式也是tensor。</p>
<p>tensor可以理解为工厂专用的array吧。但有时候，numpy对array的操作并不能直接用到tensor上。所以，在后面加上 .numpy(),把tensor转为array的。</p>
<p><strong>三、【敲黑板】构造更新目标</strong></p>
<p>我把关键代码提出来大家看看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">DQN:<br>allQ = qnetwork(np.asarray([to_one_hot(s, <span class="hljs-number">16</span>)], dtype=np.float32)).numpy()<br>a = np.argmax(allQ, <span class="hljs-number">1</span>)<br>...<br>Q1 = qnetwork(np.asarray([to_one_hot(s1, <span class="hljs-number">16</span>)], dtype=np.float32)).numpy()<br>maxQ1 = np.<span class="hljs-built_in">max</span>(Q1)  <br>targetQ = allQ<br>targetQ[<span class="hljs-number">0</span>, a[<span class="hljs-number">0</span>]] = r + lambd * maxQ1<br></code></pre></td></tr></table></figure>

<p>我们举个例子： 假设我们需要更新s状态的Q值(allQ),allQ是所有动作的Q值[0.1 0.2 0.1 0.6]。 如果我们用贪婪算法，我们会选出的动作是【下】，因为【下】的Q值最大。但很遗憾，由于我们采用epsilon-greedy，这次我们随机并随机到a是动作2【右】</p>
<p>然后我们求出下一状态s1的Q值(Q1),现在我们有下图：</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvm2iqegncj60ps06adg902.jpg" srcset="/img/loading.gif" lazyload></p>
<p>其中，绿色格子代表动作a。蓝色格子是Q1的最大值，也就是maxQ1 = np.max(Q1)。 </p>
<p>现在我们需要构造target，因此我们把allQ先复制给targetQ。然后把r + lambd * maxQ1复制给targetQ中对应a的位置。</p>
<p>所以，我们现在有allQ向targetQ更新：</p>
<p><img src="/2021/10/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/008i3skNly1gvm2j2dtg0j30el02mmx8.jpg" srcset="/img/loading.gif" lazyload></p>
<p>我们可视化一下，就可以看到Q网络的调整方向：动作1,3,4将会不变，而将会把动作2的Q值向targetQ的动作2方向靠近。 </p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>DQN = Qlearning + network</p>
<p>Qlearning需要依赖Qtable，所以没办法处理连续state的问题。而我们说，神经网络可以理解成一个magic函数。这个函数式支持连续型的输入的。</p>
<p>我们用Qlearning的方法，最终可以把这个神经网络，拟合成一个曲面。而每个state就是曲面的一个截面。我们可以从截面看到每个动作的Q值分布。</p>
<p>而在更新的过程中，我们的算法仍然是TD（0）算法。</p>
<p>但实际上，示例代码偷了个了懒，虽然DQN是可以解决连续state的问题。但我们用的环境，依然是离散的。大家可以试着改一下代码，完成CartPole-v0环境。</p>
<p>如果失败了，也没关系。后面我将会介绍几个DQN的变种，包括DoubleDQN，DuelingDQN， 。在这个过程中我们也会学到Experience replay和Fixed Targets Network的技术，把DQN的技术用到极致。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
                    
                      <a class="hover-with-bg" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
                    
                      <a class="hover-with-bg" href="/tags/DQN/">DQN</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，只作学习笔记和分享交流作用，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/10/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">机器学习</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/10/18/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89Q-Learning%E7%AE%97%E6%B3%95/">
                        <span class="hidden-mobile">强化学习（五）Q-Learning算法</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"cTKjlyyUHtaSnlWTUahyiu2y-gzGzoHsz","appKey":"aNtyxf95epQJYLQAkuHkbd6K","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->

  <div class="col-lg-7 mx-auto nopadding-x-md">
    <div class="container custom post-custom mx-auto">
      <img src="https://tva1.sinaimg.cn/large/008i3skNly1gv1h4ngfimj60hm0hkmyf02.jpg" srcset="/img/loading.gif" lazyload title="打赏一下！" class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
    </div>
  </div>


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <i class="iconfont icon-love"></i> <a href="http://www.xiaojiucai.cn" target="_blank" rel="nofollow noopener"><span>一棵小韭菜</span></a> <i class="iconfont icon-love"></i> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
