

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="爬虫是通过编写程序，模拟浏览器上网，然后让其去互联网上抓取数据的过程。">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <meta name="description" content="爬虫是通过编写程序，模拟浏览器上网，然后让其去互联网上抓取数据的过程。">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫【更新中】">
<meta property="og:url" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="小韭菜">
<meta property="og:description" content="爬虫是通过编写程序，模拟浏览器上网，然后让其去互联网上抓取数据的过程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB1.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB2.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB3.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB4.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB6.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB7.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB8.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB9.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB10.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB11.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB12.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB13.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB14.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB15.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB16.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB17.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB18.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB19.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB20.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB21.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB22.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB23.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB24.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB25.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB26.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB27.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB28.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB29.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB30.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB31.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB32.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB33.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB34.png">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv26ssxyjpj60ow129abr02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv26vq3kppj611u0cuaat02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv27a0nkysj60u00xnq5x02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv27db6os1j61ey0oc0xs02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6var2nydj60jg0dqjsf02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6prvw9t7j60k80ciaam02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6zonz4qwj61440a275n02.jpg">
<meta property="og:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6var2nydj60jg0dqjsf02.jpg">
<meta property="article:published_time" content="2021-05-07T04:19:25.000Z">
<meta property="article:modified_time" content="2021-10-14T02:39:28.692Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://www.xiaojiucai.cn/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB1.png">
  
  <title>爬虫【更新中】 - 小韭菜</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"www.xiaojiucai.cn","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":120,"cursorChar":"|","loop":true},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>小韭菜</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                时光轴
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于我
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="爬虫【更新中】">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-05-07 12:19" pubdate>
        2021年5月7日 中午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      93k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      290 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">爬虫【更新中】</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：10 天前
                
              </p>
            
            <div class="markdown-body">
              <p>爬虫是通过<strong>编写</strong>程序，<strong>模拟</strong>浏览器上网，然后让其去互联网上<strong>抓取</strong>数据的过程。</p>
<span id="more"></span>

<h2 id="爬虫简介"><a href="#爬虫简介" class="headerlink" title="爬虫简介"></a>爬虫简介</h2><p>视频课：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ha4y1H7sx">https://www.bilibili.com/video/BV1ha4y1H7sx</a></p>
<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>通过<strong>编写</strong>程序，<strong>模拟</strong>浏览器上网，然后让其去互联网上<strong>抓取</strong>数据的过程。</p>
<h3 id="价值"><a href="#价值" class="headerlink" title="价值"></a>价值</h3><ol>
<li>批量获取信息</li>
<li>实际应用</li>
<li>就业</li>
</ol>
<h3 id="合法性"><a href="#合法性" class="headerlink" title="合法性"></a>合法性</h3><ol>
<li>在法律中不禁止</li>
<li>具有违法风险<ol>
<li>爬虫干扰被访问网站的正常运营</li>
<li>爬虫抓取了受法律保护的特定数据或信息</li>
</ol>
</li>
<li>善意爬虫   恶意爬虫</li>
</ol>
<h3 id="爬虫在使用场景中的分类"><a href="#爬虫在使用场景中的分类" class="headerlink" title="爬虫在使用场景中的分类"></a>爬虫在使用场景中的分类</h3><h4 id="通用爬虫"><a href="#通用爬虫" class="headerlink" title="通用爬虫"></a>通用爬虫</h4><p>抓取系统的重要组成部分，抓取的是一整页面数据。</p>
<h4 id="聚焦爬虫"><a href="#聚焦爬虫" class="headerlink" title="聚焦爬虫"></a>聚焦爬虫</h4><p>建立在通用爬虫的基础上，抓取的是页面中的特定局部内容。</p>
<h4 id="增量式爬虫"><a href="#增量式爬虫" class="headerlink" title="增量式爬虫"></a>增量式爬虫</h4><p>检测网站中数据更新的情况，只会抓取网站中更新出来的数据。</p>
<h3 id="爬虫的矛与盾"><a href="#爬虫的矛与盾" class="headerlink" title="爬虫的矛与盾"></a>爬虫的矛与盾</h3><h4 id="反爬机制"><a href="#反爬机制" class="headerlink" title="反爬机制"></a>反爬机制</h4><p>反爬机制，即UA检测。门户网站可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的怕取。</p>
<h4 id="反反爬策略"><a href="#反反爬策略" class="headerlink" title="反反爬策略"></a>反反爬策略</h4><p>反反爬策略，即UA伪装。爬虫程序可以制定策略或使用技术手段，破解门户网站中的反爬机制，从而可以获取门户网站的信息或数据。</p>
<h4 id="君子协议"><a href="#君子协议" class="headerlink" title="君子协议"></a>君子协议</h4><p>robots.txt协议被称为君子协议，规定网站中可被爬取或者不可被爬取的数据或者信息。</p>
<h2 id="Http-amp-Https协议"><a href="#Http-amp-Https协议" class="headerlink" title="Http&amp;Https协议"></a>Http&amp;Https协议</h2><h3 id="Http协议"><a href="#Http协议" class="headerlink" title="Http协议"></a>Http协议</h3><h4 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h4><p>服务器与客户端进行数据交互的一种形式。</p>
<h4 id="常用的请求头信息"><a href="#常用的请求头信息" class="headerlink" title="常用的请求头信息"></a>常用的请求头信息</h4><ol>
<li>User- Agent：请求载体的身份标识，浏览器的身份标识</li>
<li>Connection：请求完毕后，是断开连接还是保持连接</li>
</ol>
<h4 id="Http交互方法"><a href="#Http交互方法" class="headerlink" title="Http交互方法"></a>Http交互方法</h4><p>Http协议定义了客户端和服务器交互的不同方法，最基本的方法就是GET和POST。顾名思义，GET可以根据某链接获得内容，POST用于发送内容。然而，GET也可以向链接提交内容，与POST的区别如下：</p>
<ol>
<li>GET方式可以通过URL提交数据，待提交的数据是URL的一部分，安全性较低；POST方式的待提交数据放置在HTML HEADER内，安全性较高。</li>
<li>GET方式提交的数据最多不超过1024字节，POST没有对提交内容的长度限制。</li>
</ol>
<h4 id="常用的响应头信息"><a href="#常用的响应头信息" class="headerlink" title="常用的响应头信息"></a>常用的响应头信息</h4><ol>
<li>Content-Type：服务器响应会客户端的数据类型</li>
</ol>
<h3 id="Https协议"><a href="#Https协议" class="headerlink" title="Https协议"></a>Https协议</h3><p>安全的Http协议，加密方式：</p>
<ol>
<li>对称密钥加密：客户端将钥匙和锁一同发给服务器</li>
<li>非对称密钥加密：<ol>
<li>每次信息交互时，都有一堆密钥（公钥和私钥）</li>
<li>经公钥加密的信息，只有对应的私钥才可解密</li>
<li>接收方需提前生成一对密钥，并将公钥发给发送方</li>
<li>发送方用接收的公钥加密发送给接收方，接收方用私钥解密即可</li>
<li>隐患：中间公钥可能会被第三方替换成自己的公钥</li>
</ol>
</li>
<li>证书密钥加密：<ol>
<li>为解决非对称密钥加密的隐患，引入接收方和发送方都信任的证书机构</li>
<li>接收方发送公钥时，经过证书机构对公钥进行数字签名，防伪</li>
</ol>
</li>
</ol>
<h2 id="Requests模块"><a href="#Requests模块" class="headerlink" title="Requests模块"></a>Requests模块</h2><p>基于网络请求的模块有<strong>urllib模块</strong>和<strong>requests模块</strong>，urllib模块早期应用模块，较为繁琐，本文只讲requests模块。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>requests模块</strong>：python中原生的一款基于网络请求的模块，功能非常强大，简洁高效。</p>
<p><strong>作用</strong>：模拟浏览器发送请求。</p>
<p><strong>编程流程</strong>：</p>
<ol>
<li>指定访问地址URL</li>
<li>对URL发起请求<ol>
<li>get请求</li>
<li>post请求</li>
</ol>
</li>
<li>获取响应数据</li>
<li>存储响应数据</li>
</ol>
<p>调用requests库前，需要在编译环境中安装requests库。</p>
<h3 id="实战案例"><a href="#实战案例" class="headerlink" title="实战案例"></a>实战案例</h3><h4 id="爬取搜狗首页数据"><a href="#爬取搜狗首页数据" class="headerlink" title="爬取搜狗首页数据"></a>爬取搜狗首页数据</h4><ol>
<li>需求：爬取搜狗首页页面的数据</li>
<li>地址：<a target="_blank" rel="noopener" href="https://www.sogou.com/">https://www.sogou.com</a></li>
<li>编程流程四部曲</li>
<li>代码详见如下</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment">#1.指定url</span><br>    url = <span class="hljs-string">&quot;https://www.sogou.com&quot;</span><br><br>    <span class="hljs-comment">#2.发送请求</span><br>    <span class="hljs-comment">#requests发送get请求，返回响应对象</span><br>    response = requests.get(url=url)<br><br>    <span class="hljs-comment">#3.响应数据</span><br>    <span class="hljs-comment">#response对象调用text属性，返回字符串</span><br>    page_text = response.text<br>    <span class="hljs-built_in">print</span>(page_text)<br><br>    <span class="hljs-comment">#4.存储数据</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./sogou.html&quot;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(page_text)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;爬取完成！&quot;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="简易网页采集器"><a href="#简易网页采集器" class="headerlink" title="简易网页采集器"></a>简易网页采集器</h4><ol>
<li><p>需求：爬取搜狗指定词条的搜索结果页面</p>
</li>
<li><p>地址：<a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a></p>
</li>
<li><p>反反爬策略：使用UA伪装（<strong>User- Agent：请求载体的身份标识</strong>）将程序发起的请求伪装成正常浏览器的请求，由此可以突破门户网站服务器的检测拦截。请求载体分为<strong>基于浏览器</strong>和<strong>基于爬虫程序</strong>两种，门户网站服务器允许基于浏览器的请求，拒绝基于爬虫程序的请求。</p>
</li>
<li><p>查询浏览器的UA步骤如下：</p>
<p>a. 网页输入词条，右键点击“检查”，如下:</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB1.png" srcset="/img/loading.gif" lazyload alt="爬虫1"></p>
<p>b. 在标头中的请求里，找到User-Agent</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB2.png" srcset="/img/loading.gif" lazyload alt="爬虫2"></p>
</li>
<li><p>代码如下:</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment">#1.指定url</span><br>    url = <span class="hljs-string">&quot;https://www.baidu.com/s?&quot;</span><br>    <span class="hljs-comment">#处理url携带的参数:封装到字典中</span><br>    <span class="hljs-comment">#原地址为：https://www.baidu.com/s?wd=顾钧</span><br>    <span class="hljs-comment">#删除&#x27;?wd=顾钧&#x27;（&#x27;?&#x27;可删可不删）</span><br>    <span class="hljs-comment">#改为：https://www.baidu.com/s</span><br>    <span class="hljs-comment">#将删除的内容封装到字典中</span><br>    kw = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;enter a word:&quot;</span>)<br>    param = &#123;<br>        <span class="hljs-string">&#x27;wd&#x27;</span>:kw<br>    &#125;<br>    <br>    <span class="hljs-comment">#2.发起请求</span><br>    <span class="hljs-comment">#requests库的get函数有三个参数：</span><br>    <span class="hljs-comment">#requests.get(url=xxx,params=xxx,headers=xxx)</span><br>    <span class="hljs-comment">#参数url是字符串类型的网址，参数params是字典数据类型</span><br>    <span class="hljs-comment">#两者是动态拼接的，因此参数params是缺省的</span><br>    <span class="hljs-comment">#引入反反爬策略，进行UA伪装</span><br>    <span class="hljs-comment">#参数headers是字典数据类型,将User-Agent封装到字典中</span><br>    <span class="hljs-comment">#一般来说需要通过反爬机制时，不可缺省</span><br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>    response = requests.get(url=url,params=param,headers=headers)<br><br>    <span class="hljs-comment">#3.响应数据</span><br>    page_text = response.text<br><br>    <span class="hljs-comment">#4.保存数据</span><br>    fileName = kw + <span class="hljs-string">&#x27;.html&#x27;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fileName,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(page_text)<br></code></pre></td></tr></table></figure>

<ol start="6">
<li><p>改进地方以及函数应用讲解：</p>
<p>a. 处理url携带的参数:封装到字典中,原地址为：<u><a target="_blank" rel="noopener" href="https://www.baidu.com/s?wd=%E9%A1%BE%E9%92%A7">https://www.baidu.com/s?wd=顾钧</a></u>    其中，删除’?wd=顾钧’（’?’可删可不删）改为：<a target="_blank" rel="noopener" href="https://www.baidu.com/s">https://www.baidu.com/s</a>    将删除的内容封装到字典中</p>
<p>b. requests库的get函数有三个参数：requests.get(url=xxx,params=xxx,headers=headers)</p>
<p>参数url是字符串类型的网址，参数params是字典数据类型，两者是动态拼接的，因此参数params是缺省的。</p>
<p>c. 引入反反爬策略，进行UA伪装：参数headers是字典数据类型,将User-Agent封装到字典中,一般来说需要通过反爬机制时，不可缺省</p>
</li>
</ol>
<h4 id="破解百度翻译"><a href="#破解百度翻译" class="headerlink" title="破解百度翻译"></a>破解百度翻译</h4><ol>
<li><p>需求：爬取文本输入框内的翻译数据结果</p>
</li>
<li><p>地址：</p>
</li>
<li><p>页面局部刷新：发送Ajax请求，鼠标右键—&gt;检查—&gt;打开网页数据抓包工具—&gt;Network—&gt;选择选项卡：</p>
<p>a. All：当前网页页面所有请求数据包</p>
<p>b.XHR：Ajax对应的请求数据包</p>
</li>
<li><p>选择“XHR”选项卡，在文本输入框输入“dog”，url=”<a target="_blank" rel="noopener" href="https://fanyi.baidu.com/sug&quot;%EF%BC%8C%E8%AF%B7%E6%B1%82%E7%B1%BB%E5%9E%8B%E4%B8%BA%E2%80%9CPOST%E2%80%9D%EF%BC%8C%E5%A6%82%E4%B8%8B%EF%BC%9A">https://fanyi.baidu.com/sug&quot;，请求类型为“POST”，如下：</a></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB3.png" srcset="/img/loading.gif" lazyload alt="爬虫3"></p>
</li>
<li><p>在底部请求数据中“kw”可以看到“dog”，响应数据类型：json:</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB4.png" srcset="/img/loading.gif" lazyload alt="爬虫4"></p>
</li>
<li><p>代码如下：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment">#1.指定地址</span><br>    post_url = <span class="hljs-string">&quot;https://fanyi.baidu.com/sug&quot;</span><br><br>    <span class="hljs-comment">#2.发送请求</span><br>    <span class="hljs-comment">#设置请求参数，进行UA伪装</span><br>    keyword = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;输入需要翻译的内容：&quot;</span>)<br>    data = &#123;<br>        <span class="hljs-string">&quot;kw&quot;</span>:keyword<br>    &#125;<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>    <span class="hljs-comment">#requests调用post方法，创建response对象</span><br>    <span class="hljs-comment">#post方法类似与get方法类似，具有三个参数</span><br>    <span class="hljs-comment">#requests.get(url=xxx,params=xxx,headers=xxx)</span><br>    <span class="hljs-comment">#requests.post(url=xxx,data=xxx,headers=xxx)</span><br>    <span class="hljs-comment">#get方法的params是拼接在参数url之后，在地址栏显示</span><br>    <span class="hljs-comment">#post方法的data则在地址栏中是隐藏的</span><br>    response = requests.post(url = post_url, data = data, headers = headers)<br><br><br><br>    <span class="hljs-comment">#3.响应数据</span><br>    <span class="hljs-comment">#通过content-type知道响应数据类型为json</span><br>    <span class="hljs-comment">#使用json()方法返回一个对象(obj)，确认响应数据类型为json才可以使用json()方法</span><br>    dict_obj = response.json()<br><br>    <span class="hljs-comment">#4.存储数据</span><br>    <span class="hljs-comment">#导入json库</span><br>    fileName = keyword + <span class="hljs-string">&#x27;.json&#x27;</span><br>    fp = <span class="hljs-built_in">open</span>(fileName,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br><br>    <span class="hljs-comment">#json.dump()用于将dict(字典)类型的数据转换为str(字符串)，并写入json文件</span><br>    <span class="hljs-comment">#缺省值ensure_ascii,当有中文时，设置为False</span><br>    json.dump(dict_obj,fp=fp,ensure_ascii=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>

<ol start="7">
<li><p>改进地方以及函数应用讲解：</p>
<p>a. requests调用post方法，创建response对象，post方法类似与get方法类似，具有三个参数：</p>
<p>requests.get(url=xxx,params=xxx,headers=xxx)</p>
<p>requests.post(url=xxx,data=xxx,headers=xxx)</p>
<p>get方法的params是拼接在参数url之后，在地址栏显示</p>
<p>post方法的data则在地址栏中是隐藏的</p>
<p>b. 通过content-type知道响应数据类型为json</p>
<p>使用json()方法返回一个对象(obj)，确认响应数据类型为json才可以使用json()方法，json.dump()用于将<strong>其他类型的数据</strong>转换为str(字符串)，并写入json文件，缺省值ensure_ascii,当有中文时，设置为False</p>
</li>
<li><p>思考：keyword为什么不调用get方法直接拼接地址url爬取呢？</p>
<p>答：<strong>要首先</strong>通过抓包工具查看数据发送请求的方式：get请求还是post请求。</p>
</li>
</ol>
<h4 id="豆瓣电影分类排行榜"><a href="#豆瓣电影分类排行榜" class="headerlink" title="豆瓣电影分类排行榜"></a>豆瓣电影分类排行榜</h4><ol>
<li><p>需求：爬取电影详情数据</p>
</li>
<li><p>地址：<a target="_blank" rel="noopener" href="https://movie.douban.com/j/chart/top_list">https://movie.douban.com/j/chart/top_list</a></p>
</li>
<li><p>说明解释：右键检查：get请求、url参数处理同案例<a href="#%E7%88%AC%E5%8F%96%E6%90%9C%E7%8B%97%E9%A6%96%E9%A1%B5%E6%95%B0%E6%8D%AE">爬取搜狗首页数据</a>一致、查看响应数据类型不再赘述。查看param参数并封装如下图：</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB6.png" srcset="/img/loading.gif" lazyload alt="爬虫6"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB7.png" srcset="/img/loading.gif" lazyload alt="爬虫7"></p>
</li>
<li><p>代码如下：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment">#1.指定地址</span><br>    get_url = <span class="hljs-string">&quot;https://movie.douban.com/j/chart/top_list&quot;</span><br><br>    <span class="hljs-comment">#2.发送请求</span><br>    param = &#123;<br>        <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;24&quot;</span>,<br>        <span class="hljs-string">&quot;interval_id&quot;</span>: <span class="hljs-string">&quot;100:90&quot;</span>,<br>        <span class="hljs-string">&quot;action&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<br>        <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-string">&quot;40&quot;</span>,<br>        <span class="hljs-string">&quot;limit&quot;</span>: <span class="hljs-string">&quot;20&quot;</span>,<br>    &#125;<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br><br>    response = requests.get(url=get_url,params=param,headers=headers)<br><br>    <span class="hljs-comment">#3.响应数据</span><br>    list_obj = response.json()<br>    <br>		<span class="hljs-comment">#4.存储数据</span><br>    fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./douban.json&quot;</span>,<span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>    json.dump(list_obj,fp=fp,ensure_ascii=<span class="hljs-literal">False</span>)<br><br></code></pre></td></tr></table></figure>

<h4 id="肯德基餐厅信息"><a href="#肯德基餐厅信息" class="headerlink" title="肯德基餐厅信息"></a>肯德基餐厅信息</h4><ol>
<li><p>需求：爬取指定地点的肯德基餐厅数据信息</p>
</li>
<li><p>地址：<a target="_blank" rel="noopener" href="http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx</a></p>
</li>
<li><p>说明：练习案例，包括两部分程序（a和b），代码如下：</p>
<p>a. 主程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> dict2Excel <span class="hljs-keyword">import</span> *<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>  <span class="hljs-comment">#eavl()函数处理不了字符串空值，需声明null</span><br>    <span class="hljs-keyword">global</span> null<br>    null = <span class="hljs-string">&quot;&quot;</span><br><br>    <span class="hljs-comment">#1.url</span><br>    post_url = <span class="hljs-string">&quot;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&quot;</span><br><br>    <span class="hljs-comment">#2.发送请求</span><br>    city = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;请输入城市名称：&quot;</span>)<br>    numbers = <span class="hljs-number">1</span><br>    page_num = <span class="hljs-number">0</span><br>    result_list = []<br>    <span class="hljs-keyword">while</span> page_num &lt;= numbers:<br>        page_num += <span class="hljs-number">1</span><br>        data = &#123;<br>            <span class="hljs-string">&quot;cname&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;pid&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;keyword&quot;</span>: city,<br>            <span class="hljs-string">&quot;pageIndex&quot;</span>: <span class="hljs-built_in">str</span>(page_num),<br>            <span class="hljs-string">&quot;pageSize&quot;</span>: <span class="hljs-string">&quot;10&quot;</span>,<br>        &#125;<br>        headers = &#123;<br>            <span class="hljs-string">&#x27;User-Agent&#x27;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>        &#125;<br>        response = requests.post(url=post_url,data=data,headers=headers)<br><br>        <span class="hljs-comment">#3.响应请求</span><br>        data_text = response.text<br><br>        <span class="hljs-comment">#4.存储数据</span><br>        dictionary = <span class="hljs-built_in">eval</span>(data_text)<br>        list_dict = dictionary[<span class="hljs-string">&quot;Table&quot;</span>]<br>        <span class="hljs-comment">#结果列表拼接</span><br>        result_list += dictionary[<span class="hljs-string">&quot;Table1&quot;</span>]<br>        <br>        <span class="hljs-comment">#获取店铺总数</span><br>        numbers = (list_dict[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;rowcount&quot;</span>])//<span class="hljs-number">10</span><br><br>    fileName = city + <span class="hljs-string">&quot;.xlsx&quot;</span><br>    save2Excel(result_list,fileName)<br></code></pre></td></tr></table></figure>

<p>b. 程序包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#openpyxl是python用于处理Excel的一个模块</span><br><span class="hljs-comment">#可以参考https://www.jianshu.com/p/537ae962f3a0学习</span><br><span class="hljs-comment">#从openpyxl库导入Workbook用于产生Excel表格</span><br><span class="hljs-keyword">from</span> openpyxl <span class="hljs-keyword">import</span> Workbook<br><br><span class="hljs-comment">#声明Excel的列名</span><br>Label = [<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>, <span class="hljs-string">&quot;E&quot;</span>, <span class="hljs-string">&quot;F&quot;</span>, <span class="hljs-string">&quot;G&quot;</span>, <span class="hljs-string">&quot;H&quot;</span>, <span class="hljs-string">&quot;I&quot;</span>, <span class="hljs-string">&quot;J&quot;</span>]<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">inputData: 列表，包含多个字典；例如：[&#123;&quot;key1&quot;:&quot;123&#125;,&#123;&quot;key2&quot;:&quot;456&quot;&#125;]</span><br><span class="hljs-string">outputFile: 输出文件名，例如：data.xlsx</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save2Excel</span>(<span class="hljs-params">inputData,outputFile</span>):</span><br>  	<span class="hljs-comment">#获取一个Excel工作簿</span><br>    wb = Workbook()<br>    <br>    <span class="hljs-comment">#一个工作簿（workbook）在创建的同时</span><br>    <span class="hljs-comment">#也至少创建了一个工作表（worksheet）</span><br>    <span class="hljs-comment">#可以通过active的调用得到当前正在运行的工作表</span><br>    sheet = wb.active<br>    <br>    <span class="hljs-comment">#对第一个工作表Sheet1标题命名</span><br>    sheet.title = <span class="hljs-string">&quot;Sheet1&quot;</span><br>    <br>    <span class="hljs-comment">#在输入文件获取第一个字典</span><br>    item_0 = inputData[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment">#对A1、B1、···、F1填充inputData的列名</span><br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> item_0.keys():<br>        sheet[Label[i]+<span class="hljs-built_in">str</span>(<span class="hljs-number">1</span>)].value = key<br>        i += <span class="hljs-number">1</span><br>    <br>    <span class="hljs-comment">#对每一个字典进行操作</span><br>    j = <span class="hljs-number">1</span><br>    <span class="hljs-comment">#遍历每一个字典</span><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> inputData:<br>        k = <span class="hljs-number">0</span><br>        <span class="hljs-comment">#将item中的数据写入每一行</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> item:<br>            sheet[Label[k]+<span class="hljs-built_in">str</span>(j+<span class="hljs-number">1</span>)].value = item[key]<br>            k += <span class="hljs-number">1</span><br>        j += <span class="hljs-number">1</span><br>    wb.save(outputFile)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;数据写入完毕！&quot;</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
<h4 id="国家药监局数据"><a href="#国家药监局数据" class="headerlink" title="国家药监局数据"></a>国家药监局数据</h4><ol>
<li><p>需求：爬取化妆品生产许可证的相关信息</p>
</li>
<li><p>地址：<a target="_blank" rel="noopener" href="http://scxk.nmpa.gov.cn:81/xk/">http://scxk.nmpa.gov.cn:81/xk/</a></p>
</li>
<li><p>思路：</p>
<p>a. 首先确定网页的信息是静态的还是动态的</p>
<p>​    i. 如果是静态网页，则数据信息返回text即可获取</p>
<p>​    ii.如果是动态网页，则数据信息是动态加载的，Ajax动态请求的，则需要进一步分析抓包工具</p>
<p>​    iii.确定方法：右键检查，通过抓包工具的请求响应来判断。</p>
<p>b. 通过抓包工具，找出逻辑关系的url和关键词ID</p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> dict2Excel <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&#x27;User-Agent&#x27;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>    post_url = <span class="hljs-string">&quot;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList&quot;</span><br>    id_list = []<br>    re_list = []<br><br>    <span class="hljs-comment">#该网站在第51页及以后的数据异常，无法获取（写于2021年8月18日 16:34:48）</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">51</span>):<br>        data = &#123;<br>            <span class="hljs-string">&quot;on&quot;</span>:<span class="hljs-string">&quot; true&quot;</span>,<br>            <span class="hljs-string">&quot;page&quot;</span>:<span class="hljs-built_in">str</span>(i),<br>            <span class="hljs-string">&quot;pageSize&quot;</span>:<span class="hljs-string">&quot; 15&quot;</span>,<br>            <span class="hljs-string">&quot;productName&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;conditionType&quot;</span>:<span class="hljs-string">&quot; 1&quot;</span>,<br>            <span class="hljs-string">&quot;applyname&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<br>            <span class="hljs-string">&quot;applysn&quot;</span>:<span class="hljs-string">&quot;&quot;</span>,<br>        &#125;<br>        <span class="hljs-built_in">dict</span> = requests.post(url=post_url,data=data,headers=headers).json()<br><br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">dict</span>[<span class="hljs-string">&quot;list&quot;</span>]:<br>            id_list.append(<span class="hljs-built_in">id</span>[<span class="hljs-string">&#x27;ID&#x27;</span>])<br><br>    <span class="hljs-comment">#print(len(id_list),id_list)</span><br><br>    re_url = <span class="hljs-string">&quot;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById&quot;</span><br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> id_list:<br>        re_data = &#123;<br>            <span class="hljs-string">&quot;id&quot;</span>:<span class="hljs-built_in">id</span><br>        &#125;<br>        re_dict = requests.post(url=re_url,data=re_data,headers=headers).json()<br>        re_list.append(re_dict)<br><br>    <span class="hljs-comment">#print(re_list)</span><br><br><br>    fileName = <span class="hljs-string">&quot;化妆品&quot;</span> + <span class="hljs-string">&quot;.xlsx&quot;</span><br>    save2Excel(re_list,fileName)<br><br></code></pre></td></tr></table></figure>

 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openpyxl <span class="hljs-keyword">import</span> Workbook<br>Label = [<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>, <span class="hljs-string">&quot;E&quot;</span>, <span class="hljs-string">&quot;F&quot;</span>, <span class="hljs-string">&quot;G&quot;</span>, <span class="hljs-string">&quot;H&quot;</span>, <span class="hljs-string">&quot;I&quot;</span>, <span class="hljs-string">&quot;J&quot;</span>, <span class="hljs-string">&quot;H&quot;</span>, <span class="hljs-string">&quot;I&quot;</span>, <span class="hljs-string">&quot;J&quot;</span>, <span class="hljs-string">&quot;K&quot;</span>, <span class="hljs-string">&quot;L&quot;</span>, <span class="hljs-string">&quot;M&quot;</span>, <span class="hljs-string">&quot;N&quot;</span>, <span class="hljs-string">&quot;O&quot;</span>, <span class="hljs-string">&quot;P&quot;</span>, <span class="hljs-string">&quot;Q&quot;</span>, <span class="hljs-string">&quot;R&quot;</span>, <span class="hljs-string">&quot;S&quot;</span>, <span class="hljs-string">&quot;T&quot;</span>, <span class="hljs-string">&quot;U&quot;</span>, <span class="hljs-string">&quot;V&quot;</span>, <span class="hljs-string">&quot;W&quot;</span>, <span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>, <span class="hljs-string">&quot;Z&quot;</span>, <span class="hljs-string">&quot;AA&quot;</span>, <span class="hljs-string">&quot;AB&quot;</span>, <span class="hljs-string">&quot;AC&quot;</span>, <span class="hljs-string">&quot;AD&quot;</span>, <span class="hljs-string">&quot;AE&quot;</span>, <span class="hljs-string">&quot;AF&quot;</span>, <span class="hljs-string">&quot;AG&quot;</span>, <span class="hljs-string">&quot;AH&quot;</span>, <span class="hljs-string">&quot;AI&quot;</span>, <span class="hljs-string">&quot;AJ&quot;</span>, <span class="hljs-string">&quot;AK&quot;</span>, <span class="hljs-string">&quot;AL&quot;</span>, <span class="hljs-string">&quot;AM&quot;</span>, <span class="hljs-string">&quot;AN&quot;</span>]<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">inputData: 列表，包含多个字典；例如：[&#123;&quot;key1&quot;:&quot;123&#125;,&#123;&quot;key2&quot;:&quot;456&quot;&#125;]</span><br><span class="hljs-string">outputFile: 输出文件名，例如：data.xlsx</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">#如果值为null，改bug</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save2Excel</span>(<span class="hljs-params">inputData,outputFile</span>):</span><br>    wb = Workbook()<br>    sheet = wb.active<br>    sheet.title = <span class="hljs-string">&quot;表格1&quot;</span><br>    item_0 = inputData[<span class="hljs-number">0</span>]<br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> item_0.keys():<br>        sheet[Label[i]+<span class="hljs-built_in">str</span>(<span class="hljs-number">1</span>)].value = key<br>        i += <span class="hljs-number">1</span><br>    j = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> inputData:<br>        k = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> item:<br>            sheet[Label[k]+<span class="hljs-built_in">str</span>(j+<span class="hljs-number">1</span>)].value = item[key]<br>            k += <span class="hljs-number">1</span><br>        j += <span class="hljs-number">1</span><br>    wb.save(outputFile)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;数据写入完毕！&quot;</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
<h2 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h2><p>数据解析应用于<strong>聚焦爬虫</strong>：爬取解析页面中指定的局部内容，数据解析有以下三种方式：<strong>正则表达式、bs4、xpath</strong>。其中，<strong>xpath</strong>是通用型最强的数据解析方式，其他编程语言也可以应用。</p>
<p>在加入数据解析后，聚焦爬虫的编码流程分为以下5步：</p>
<ol>
<li>指定URL</li>
<li>发起请求</li>
<li>响应数据</li>
<li>数据解析</li>
<li>存储数据</li>
</ol>
<h3 id="数据解析原理"><a href="#数据解析原理" class="headerlink" title="数据解析原理"></a>数据解析原理</h3><ol>
<li><p>解析网页局部的文本内容都会在标签之间或者标签对应的属性中进行存储</p>
<p>a. 进行指定标签的定位</p>
<p>b. 标签或者标签对应的属性中存储的数据进行提取（解析）</p>
</li>
</ol>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><h4 id="测试工具"><a href="#测试工具" class="headerlink" title="测试工具"></a>测试工具</h4><p>工具网址：<a target="_blank" rel="noopener" href="https://regex101.com/">https://regex101.com/</a></p>
<p>学习网址：<a target="_blank" rel="noopener" href="https://www.codejiaonang.com/#/course/regex_chapter1/">https://www.codejiaonang.com/#/course/regex_chapter1/</a></p>
<h4 id="限定符"><a href="#限定符" class="headerlink" title="限定符"></a>限定符</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs tex">a*					a出现0次或者多次<br>a+					a出现1次或者多次<br>a?					a出现0次或者1次<br>a&#123;6&#125;				a出现6次<br>a&#123;2,6&#125;			a出现2次到6次<br>a&#123;2,&#125;				a出现2次及以上<br></code></pre></td></tr></table></figure>

<h4 id="或运算符"><a href="#或运算符" class="headerlink" title="或运算符"></a>或运算符</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs tex">(a|b)				 匹配a或者b<br>(ab)|(cd)		 匹配ab或者cd<br></code></pre></td></tr></table></figure>

<h4 id="字符类"><a href="#字符类" class="headerlink" title="字符类"></a>字符类</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs tex">[abc]				匹配a或者b或者c<br>[a-c]				匹配a或者b或者c<br>[a-zA-Z0-9]	匹配小写大写英文字母和数字<br>[<span class="hljs-built_in">^</span>0-9]			匹配非数字字符<br></code></pre></td></tr></table></figure>

<h4 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs te">\d					匹配数字字符<br>\D					匹配非数字字符<br>\w					匹配单词字符（英文、数字、下划线）<br>\W					匹配非单词字符<br>\s					匹配空白符（包含换行符、Tab）<br>\S					匹配非空白符<br>.						匹配任意字（换行符除外）,单个字符<br>^						匹配行首<br>$						匹配行尾<br>\bword\b		\b标注字符的边界（全字匹配）<br></code></pre></td></tr></table></figure>

<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB8.png" srcset="/img/loading.gif" lazyload alt="爬虫8"></p>
<h4 id="贪婪-懒惰匹配"><a href="#贪婪-懒惰匹配" class="headerlink" title="贪婪/懒惰匹配"></a>贪婪/懒惰匹配</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs tex">&lt;.+&gt;				默认贪婪匹配”任意字符“，尽可能匹配多的字符<br>&lt;.+?&gt;				懒惰匹配”任意字符“，尽可能匹配少的字符<br></code></pre></td></tr></table></figure>

<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB9.png" srcset="/img/loading.gif" lazyload alt="爬虫9"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB10.png" srcset="/img/loading.gif" lazyload alt="爬虫10"></p>
<h4 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h4><ol>
<li><p>捕获分组</p>
<p>使用<code>()</code>实现分组，达到捕获提取数据的功能。</p>
<p>如下样例：</p>
</li>
</ol>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB11.png" srcset="/img/loading.gif" lazyload alt="爬虫11"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB12.png" srcset="/img/loading.gif" lazyload alt="爬虫12"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB13.png" srcset="/img/loading.gif" lazyload alt="爬虫13"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB14.png" srcset="/img/loading.gif" lazyload alt="爬虫14"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB15.png" srcset="/img/loading.gif" lazyload alt="爬虫15"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB16.png" srcset="/img/loading.gif" lazyload alt="爬虫16"></p>
<ol start="2">
<li><p>非捕获分组</p>
<p>使用<code>(?:表达式)</code>可以不捕获分组，只实现分组功能。</p>
<p>如下样例：</p>
</li>
</ol>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB17.png" srcset="/img/loading.gif" lazyload alt="爬虫17"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB18.png" srcset="/img/loading.gif" lazyload alt="爬虫18"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB19.png" srcset="/img/loading.gif" lazyload alt="爬虫19"></p>
<ol start="3">
<li><p>分组的回溯引用</p>
<p>在匹配HTML标签对时，<code>0123&lt;font&gt;提示&lt;/font&gt;abcd</code>的正则表达式可以为<code>&lt;\w+&gt;.*?&lt;/\w+&gt;</code>，但是<code>0123&lt;font&gt;提示&lt;/otherlabel&gt;abcd</code>即使标签对不匹配也会被正则表达式识别。</p>
<p>此时，使用分组的回溯引用解决HTML标签对匹配问题，并且当存在多个不同标签对时，使用<code>\N</code>引用编号为<code>N</code>的分组。上述例子的正则表达式可以表示为<code>&lt;(\w+)&gt;.*?&lt;/\1&gt;</code></p>
<p>样例如下：</p>
</li>
</ol>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB20.png" srcset="/img/loading.gif" lazyload alt="爬虫20"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB21.png" srcset="/img/loading.gif" lazyload alt="爬虫21"></p>
<h4 id="断言"><a href="#断言" class="headerlink" title="断言"></a>断言</h4><p><strong>断言</strong>又称为<strong>环视</strong>或者<strong>预搜索</strong>，分为先行断言和后行断言。其中先行断言和后行断言又可以划分为<strong>正向先行断言</strong>、<strong>反向先行断言</strong>、<strong>正向后行断言</strong>、<strong>反向后行断言</strong>四类。</p>
<ol>
<li><p>正向先行断言</p>
<p>格式为 <code>(?=表达式)</code>，在某个位置向右看，表示所在位置右侧必须能匹配<code>表达式</code></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ruby">(<span class="hljs-string">?=</span>.*<span class="hljs-string">?[</span>a-z])(<span class="hljs-string">?=</span>.*<span class="hljs-string">?[</span>A-Z])(<span class="hljs-string">?=</span>.*<span class="hljs-string">?[</span><span class="hljs-number">0</span>-<span class="hljs-number">9</span>])\S&#123;<span class="hljs-number">8</span>,&#125;<br><span class="hljs-comment">#上面正则表达式满足：</span><br><span class="hljs-number">1</span>.至少一个小写字母<br><span class="hljs-number">2</span>.至少一个大写字母<br><span class="hljs-number">3</span>.至少一个数字<br><span class="hljs-number">4</span>.至少<span class="hljs-number">8</span>个字符<br><span class="hljs-number">5</span>.不能存在空格符<br></code></pre></td></tr></table></figure></li>
</ol>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB22.png" srcset="/img/loading.gif" lazyload alt="爬虫22"></p>
<ol start="2">
<li><p>反向先行断言</p>
<p>格式为<code>(?!表达式)</code>，在某个位置向右看，表示所在位置右侧不能出现<code>表达式</code></p>
</li>
</ol>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB23.png" srcset="/img/loading.gif" lazyload alt="爬虫23"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB24.png" srcset="/img/loading.gif" lazyload alt="爬虫24"></p>
<ol start="3">
<li><p>正向后行断言</p>
<p>格式为<code>(?&lt;=表达式)</code>，在某个位置向左看，表示所在位置左侧必须匹配<code>表达式</code>。</p>
</li>
</ol>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB25.png" srcset="/img/loading.gif" lazyload alt="爬虫25"></p>
<ol start="4">
<li><p>反向后行断言</p>
<p>格式为<code>(?&lt;!表达式)</code>，在某个位置向左看，表示所在位置左侧不能出现<code>表达式</code> 。</p>
</li>
</ol>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB26.png" srcset="/img/loading.gif" lazyload alt="爬虫26"></p>
<h4 id="实战案例-1"><a href="#实战案例-1" class="headerlink" title="实战案例"></a>实战案例</h4><h5 id="爬取单张图片"><a href="#爬取单张图片" class="headerlink" title="爬取单张图片"></a>爬取单张图片</h5><ol>
<li><p>需求：爬取网站单张图片</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="https://cdn2.p5h.net/wp-content/uploads/img/gtdyaipt/1.jpg">https://cdn2.p5h.net/wp-content/uploads/img/gtdyaipt/1.jpg</a></p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br><br>    url = <span class="hljs-string">&quot;https://cdn2.p5h.net/wp-content/uploads/img/gtdyaipt/1.jpg&quot;</span><br><br>    <span class="hljs-comment">#content返回的是二进制形式的图片</span><br>    <span class="hljs-comment">#text返回字符串  content返回二进制    json()返回对象</span><br>    image_data = requests.get(url=url).content<br><br>    <span class="hljs-comment">#wb以二进制形式写入文件</span><br>    <span class="hljs-comment">#open()创建打开一个图片格式的文件</span><br>    <span class="hljs-comment">#write()将二进制图片数据信息写入open()创建的图片格式文件</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./beauty.jpg&quot;</span>,<span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(image_data)<br></code></pre></td></tr></table></figure></li>
<li><p>说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">get(XXX).text					<span class="hljs-comment">#返回字符串</span><br>get(XXX).content			<span class="hljs-comment">#返回二进制图片</span><br>get(XXX).json()				<span class="hljs-comment">#返回json对象</span><br><br><span class="hljs-comment">#wb以二进制形式写入文件</span><br><span class="hljs-comment">#open()创建打开一个图片格式的文件</span><br><span class="hljs-comment">#write()将二进制图片数据信息写入open()创建的图片格式文件</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./beauty.jpg&quot;</span>,<span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(image_data)<br></code></pre></td></tr></table></figure></li>
</ol>
<h5 id="爬取分页图片"><a href="#爬取分页图片" class="headerlink" title="爬取分页图片"></a>爬取分页图片</h5><ol>
<li><p>需求：分页爬取网页图片，并保存至分页文件夹路径</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="https://www.1bie.com/category/qingchun/">https://www.1bie.com/category/qingchun/</a></p>
</li>
<li><p>图片逻辑：</p>
<ol>
<li>进入“青春美女”类别下</li>
<li>在该类别下，每一页存在多个美女套图</li>
<li>进入单个套图，爬取该套图下每一张图片</li>
<li>文件路径逻辑：图片文件夹/页码文件夹/套图文件夹/图片.jpg</li>
</ol>
</li>
<li><p>代码如下：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-comment">#导入正则模块</span><br><span class="hljs-keyword">import</span> re<br><br><span class="hljs-comment">#导入文件系统</span><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><br>    <span class="hljs-comment">#创建图片文件夹根目录路径：./pics</span><br>    <span class="hljs-comment">#if条件提前判断是否已经存在，容错处理</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&quot;./pics&quot;</span>):<br>        os.mkdir(<span class="hljs-string">&quot;./pics&quot;</span>)<br>    <br>    <span class="hljs-comment">#UA伪装</span><br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>:<span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>		<br>    <span class="hljs-comment">#右键检查，可以看出不同页码的地址的特点</span><br>    <span class="hljs-comment">#https://www.1bie.com/category/qingchun/page/2</span><br>    <span class="hljs-comment">#https://www.1bie.com/category/qingchun/page/3</span><br>    <span class="hljs-comment">#https://www.1bie.com/category/qingchun/page/4</span><br>    <span class="hljs-comment">#仅仅是修改页码参数即可</span><br>    <span class="hljs-comment">#用%d声明整型页码变量，建立页码地址通用模板</span><br>    page_url = <span class="hljs-string">&quot;https://www.1bie.com/category/qingchun/page/%d&quot;</span><br>		<br>    <span class="hljs-comment">#页码循环</span><br>    <span class="hljs-keyword">for</span> pageNum <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>):<br>      	<span class="hljs-comment">#format()格式函数,用法发生变化，可自查学习</span><br>        <span class="hljs-comment">#也可以改为下述代码：</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    page_url = &quot;https://www.1bie.com/category/qingchun/page/&#123;&#125;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    for pageNum in range(3,4):</span><br><span class="hljs-string">        html_url = page_url.format(pageNum)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>        html_url = <span class="hljs-built_in">format</span>(page_url%pageNum)<br><br>        <span class="hljs-comment">#使用通用爬虫对html_url每个页码网页爬取整张页面</span><br>        html_text = requests.get(url=html_url,headers=headers).text<br><br>				<span class="hljs-comment">#创建页码文件夹目录路径：./pics/1/</span><br>        page_path = <span class="hljs-string">&quot;./pics/&quot;</span> + <span class="hljs-built_in">str</span>(pageNum) + <span class="hljs-string">&quot;/&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(page_path):<br>            os.mkdir(page_path)<br>				<br>        <span class="hljs-comment">#在页码网页文本代码中，找到每个套图的地址特点</span><br>        <span class="hljs-comment">#用正则表达式ex提取所在页码下所有套图的地址列表</span><br>        ex = <span class="hljs-string">&#x27;.*?&lt;a class=&quot;entry-thumbnail&quot; href=&quot;(.*?)&quot;&gt;.*?&lt;/div&gt;&#x27;</span><br>        <span class="hljs-comment">#re.findall(ex,text,pattern)</span><br>        <span class="hljs-comment">#re库使用findall()函数，在网页text文本中，按照爬pattern方式查找符合ex要求的文本字符串,结果返回列表形式</span><br>        <span class="hljs-comment">#参数ex目标正则表达式</span><br>        <span class="hljs-comment">#参数text为网页html的文本形式</span><br>        <span class="hljs-comment">#参数re.S为单行条件查找，另外还有多行条件查找re.M</span><br>        a_list = re.findall(ex,html_text,re.S)<br>        <span class="hljs-comment">#print(len(a_list),a_list)</span><br><br>        <span class="hljs-comment">#用正则表达式ex2提取所在套图下所有图片的地址列表</span><br>        ex2 = <span class="hljs-string">&#x27;.*?src=&quot;(.*?)&quot; alt=.*?&#x27;</span><br>	<br>  			<span class="hljs-comment">#遍历每个套图地址列表</span><br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> a_list:<br>            <span class="hljs-comment">#把套图地址根据‘/’划分为字符串列表</span><br>            path = url.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>]<br>            img_path = page_path + path[<span class="hljs-number">0</span>:<span class="hljs-number">4</span>] + <span class="hljs-string">&#x27;/&#x27;</span><br>            <span class="hljs-comment">#创建套图文件夹目录路径：./pics/1/4368/</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(img_path):<br>                os.mkdir(img_path)<br><br>            <span class="hljs-comment">#对每个套图地址列表发送请求</span><br>            img_text = requests.get(url=url,headers=headers).text<br>            <span class="hljs-comment">#对请求响应结果的网页text正则提取图片src列表</span><br>            img_src = re.findall(ex2,img_text)<br><br>						<span class="hljs-comment">#遍历请求每个图片src，get().content得到图片的二进制数据</span><br>            <span class="hljs-keyword">for</span> src <span class="hljs-keyword">in</span> img_src:<br>                img_data = requests.get(url=src,headers=headers).content<br>                img_name = src.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>]<br>                fileName = img_path + img_name<br>                <span class="hljs-comment">#wb以二进制形式写入文件</span><br>    						<span class="hljs-comment">#open()创建打开一个图片格式的文件</span><br>    						<span class="hljs-comment">#write()将二进制图片数据信息写入open()创建的图片格式文件</span><br>                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fileName,<span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>                    fp.write(img_data)<br>                    <span class="hljs-built_in">print</span>(img_name,<span class="hljs-string">&quot;下载成功！&quot;</span>)<br></code></pre></td></tr></table></figure></li>
<li><p>说明：在书写正则表达式提取文本内容时，可以利用<code>https://regex101.com/</code>正则工具进行调试。B站视频教程中的较为简单，原理同上，这里不再复现。</p>
</li>
</ol>
<h3 id="bs4"><a href="#bs4" class="headerlink" title="bs4"></a>bs4</h3><p>bs4，它是python**<u>独有</u>**的数据解析方式。传统的数据解析原理：1. 标签定位；2. 提取标签、标签属性中存储的数据值。</p>
<h4 id="bs4数据解析原理"><a href="#bs4数据解析原理" class="headerlink" title="bs4数据解析原理"></a>bs4数据解析原理</h4><ol>
<li>实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中</li>
<li>通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取。</li>
</ol>
<h4 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h4><ol>
<li><code>pip3 install bs4</code></li>
<li><code>pip3 install lxml</code></li>
</ol>
<h4 id="实例化BeautifulSoup对象"><a href="#实例化BeautifulSoup对象" class="headerlink" title="实例化BeautifulSoup对象"></a>实例化BeautifulSoup对象</h4><ol>
<li><p>导入BeautifulSoup</p>
<p><code>from bs4 import BeautifulSoup</code></p>
</li>
<li><p>对象实例化</p>
<ol>
<li><p>本地html文件中的数据加载到该对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment">#将本地的html文档中的数据加载到该对象中</span><br>    fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./sogou.html&quot;</span>,<span class="hljs-string">&quot;r&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>    <span class="hljs-comment">#获取sogou.html文件源码</span><br>    <span class="hljs-comment">#BeautifulSoup(fileName,&quot;lxml&quot;)实例对象</span><br>    <span class="hljs-comment">#fileName是html文件名；lxml是编译格式，可以处理XML和HTML文件</span><br>    soup = BeautifulSoup(fp,<span class="hljs-string">&quot;lxml&quot;</span>)<br>    <span class="hljs-built_in">print</span>(soup)<br></code></pre></td></tr></table></figure></li>
<li><p>将互联网上页面源码加载到该对象中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>  url = <span class="hljs-string">&quot;xxx&quot;</span><br>  response = requests.get(url=xxx,params=xxx,headers=xxx)<br>  page_text = response.text<br>  soup = BeautifulSoup(page_text,<span class="hljs-string">&quot;lxml&quot;</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>对象的属性和方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">soup = BeautifulSoup(page_text,<span class="hljs-string">&quot;lxml&quot;</span>)<br><br><span class="hljs-comment">###查找标签</span><br><span class="hljs-comment">#soup.tagName	soup调用标签名字</span><br><span class="hljs-comment">#调用&lt;a&gt;&lt;/a&gt;标签，返回第一次出现的，并非所有的</span><br>soup.a		<span class="hljs-comment">#返回page_text中第一个出现的&lt;a&gt;&lt;/a&gt;标签</span><br>soup.div	<span class="hljs-comment">#返回page_text中第一个出现的&lt;div&gt;&lt;/div&gt;标签</span><br><br><span class="hljs-comment">#soup.find(&#x27;xxx&#x27;)	  参数xxx为tagName，返回第一次出现的</span><br><span class="hljs-comment">#等同于soup.tagName</span><br>soup.find(<span class="hljs-string">&quot;div&quot;</span>)	<span class="hljs-comment">#返回page_text中第一个出现的&lt;div&gt;&lt;/div&gt;标签</span><br>soup.find(<span class="hljs-string">&quot;a&quot;</span>)		<span class="hljs-comment">#返回page_text中第一个出现的&lt;a&gt;&lt;/a&gt;标签</span><br><br><span class="hljs-comment">#soup.find(&#x27;tagName&#x27;,class_=&#x27;xxx&#x27;)加入新参数class_（或者id、attr等），进行特定的属性定位，返回特定的&lt;tagName&gt;&lt;/tagName&gt;标签</span><br><br><span class="hljs-comment">#soup.find_all(&quot;xxx&quot;)返回page_text中出现的xxx所有标签列表</span><br><br><span class="hljs-comment">#soup.select(&#x27;xxx&#x27;)	其中，xxx是选择器，类选择器，标签选择器,id选择器，返回被选择内容的列表形式</span><br>soup.select(<span class="hljs-string">&#x27;.tang&#x27;</span>)	<span class="hljs-comment">#返回tang所包含的内容列表</span><br><br><span class="hljs-comment">#层级选择器soup.select(&quot;x1 &gt; x2 &gt; x3&quot;) x1、x2、x3是标签名称，&gt; 表示标签之间层级关系，一个层级（空格表示多个层级），x1 &gt; x2 x1包含x2。返回所有符合层级选择定位的内容列表</span><br>soup.select(<span class="hljs-string">&quot;.tang &gt; ul &gt; li &gt; a&quot;</span>)<span class="hljs-comment">#返回.tang &gt; ul &gt; li &gt; a层级关系下的所有a标签列表</span><br>soup.select(<span class="hljs-string">&quot;.tang &gt; ul a&quot;</span>)<span class="hljs-comment">#返回.tang &gt; ul a层级关系下的所有a标签列表</span><br><br><span class="hljs-comment">#获取标签之间的文本数据</span><br>soup.a.text/string/get.text() <span class="hljs-comment">#返回标签的文本内容</span><br><span class="hljs-comment">#text/get.text	获取指定标签下的所有文本内容</span><br><span class="hljs-comment">#string					获取指定标签下的直系文本内容</span><br><br><span class="hljs-comment">#获取标签中的属性值,直接用字典形式查找引用即可</span><br>soup.a.[<span class="hljs-string">&quot;href&quot;</span>]	<span class="hljs-comment">#返回href的链接地址</span><br></code></pre></td></tr></table></figure></li>
</ol>
<h4 id="实战案例-2"><a href="#实战案例-2" class="headerlink" title="实战案例"></a>实战案例</h4><h5 id="爬取乱序三国"><a href="#爬取乱序三国" class="headerlink" title="爬取乱序三国"></a>爬取乱序三国</h5><ol>
<li><p>需求：爬取B站视频课中的三国演义全集</p>
</li>
<li><p>地址：<a target="_blank" rel="noopener" href="https://www.shicimingju.com/book/sanguoyanyi.html">https://www.shicimingju.com/book/sanguoyanyi.html</a></p>
</li>
<li><p>说明：该地址截至2021年8月31日14:12分跳转的内容是错乱的</p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>    mulu_url = <span class="hljs-string">&quot;https://www.shicimingju.com/book/sanguoyanyi.html&quot;</span><br>    html = requests.get(url = mulu_url,headers= headers)<br>    <span class="hljs-comment">#在获取响应时，修改编码方式，解决乱码问题</span><br>    html.encoding = <span class="hljs-string">&quot;utf-8&quot;</span><br>    mulu_html = html.text<br>    <br>    mulu_soup = BeautifulSoup(mulu_html,<span class="hljs-string">&quot;lxml&quot;</span>)<br>    mulu_list = mulu_soup.select(<span class="hljs-string">&quot;.book-mulu &gt; ul &gt; li &gt; a&quot;</span>)<br><br>    fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./三国.text&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>    <span class="hljs-keyword">for</span> mulu <span class="hljs-keyword">in</span> mulu_list:<br>        title = mulu.string<br>        url_title = <span class="hljs-string">&quot;https://www.shicimingju.com&quot;</span> + mulu[<span class="hljs-string">&quot;href&quot;</span>]<br>        html = requests.get(url = url_title,headers=headers)<br>        <span class="hljs-comment">#修改乱码</span><br>        html.encoding = <span class="hljs-string">&quot;utf-8&quot;</span><br>        content_html = html.text<br>        content_soup = BeautifulSoup(content_html,<span class="hljs-string">&quot;lxml&quot;</span>)<br>        content = content_soup.find(<span class="hljs-string">&quot;div&quot;</span>,class_=<span class="hljs-string">&quot;chapter_content&quot;</span>).text<br>        write_content = title + <span class="hljs-string">&quot;\n&quot;</span> + content + <span class="hljs-string">&quot;\n&quot;</span><br>        fp.write(write_content)<br>    fp.close()<br></code></pre></td></tr></table></figure>

<p> 相信到这里，大家代码水平都已经很高了，这里便不再多加赘述注释。</p>
</li>
<li><p>中文乱码问题详解</p>
<p>在Requests源码包解析原理中，<code>text</code>返回的是<strong>处理过</strong>的<code>unicode</code>型的数据，而使用<code>content</code>返回的是<code>bytes</code>型的原始数据。也就是说，<code>response.content</code>相对于<code>response.text</code>节省了计算资源：<code>content</code>是直接把内容以<code>bytes</code>类型返回，而<code>text</code>是把内容decode(解码)成<code>unicode</code>类型返回。如果<code>headers</code>没有charset字符集的话，<code>text()</code>会调用<code>chardet</code>来检测字符集编码，计算字符集，增加了计算负担消耗CPU资源。</p>
<p>通过抓包工具对比两个网页的响应，如下图：</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB27.png" srcset="/img/loading.gif" lazyload alt="爬虫27"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB28.png" srcset="/img/loading.gif" lazyload alt="爬虫28"></p>
<p>《HTTP权威指南》里第16章国际化里指出，如果HTTP响应中Content-Type字段没有指定<code>charset</code>，则默认页面是<code>ISO-8859-1</code>编码。这处理英文页面没有问题，但是处理中文页面就会有乱码。</p>
<p>​    </p>
<p>字符串在Python内部的表示是unicode编码，因此，在做编码转换时，通常需要以unicode作为中间编码。即先将其他编码的字符串解码（decode）成unicode，再从unicode编码（encode）成另一种编码。decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gb2312编码的字符串str1转换成unicode编码，encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘gb2312’)，表示将unicode编码的字符串str2转换成gb2312编码。因此，转码的时候一定要先搞明白，字符串str是什么编码，然后decode成unicode，然后再encode成其他编码。</p>
<p>乱码解决方案如下两种：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#方法一：</span><br>response = requests.get(url=url)<br>response.encoding = <span class="hljs-string">&quot;utf-8&quot;</span><br>page_text = response.text<br><br><span class="hljs-comment">#方法二：</span><br>response = requests.get(url=url)<br>response.encoding = response.apparent_encoding<br>page_text = response.text<br></code></pre></td></tr></table></figure></li>
</ol>
<h5 id="爬取三国演义全集"><a href="#爬取三国演义全集" class="headerlink" title="爬取三国演义全集"></a>爬取三国演义全集</h5><ol>
<li><p>需求：练习复习bs4，爬取三国演义全集</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="http://www.purepen.com/sgyy/">http://www.purepen.com/sgyy/</a></p>
</li>
<li><p>说明：第113回，古体繁体字，识别不了，会出现乱码</p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>    mulu_url = <span class="hljs-string">&quot;http://www.purepen.com/sgyy/&quot;</span><br>    html = requests.get(url=mulu_url,headers=headers)<br>    html.encoding = html.apparent_encoding<br>    mulu_html = html.text<br>    <span class="hljs-comment">#print(mulu_html)</span><br>    mulu_soup = BeautifulSoup(mulu_html,<span class="hljs-string">&quot;lxml&quot;</span>)<br>    mulu_list = mulu_soup.find(<span class="hljs-string">&quot;table&quot;</span>,cellpadding=<span class="hljs-string">&quot;3&quot;</span>).select(<span class="hljs-string">&quot;tr &gt; td &gt; a&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(mulu_list),mulu_list)<br><br>    <span class="hljs-comment">#id = 1</span><br>    fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./三国演义.txt&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>    <span class="hljs-keyword">for</span> mulu <span class="hljs-keyword">in</span> mulu_list:<br>        <span class="hljs-comment">#title = mulu.string</span><br>        <span class="hljs-comment">#title_str = f&quot;第&#123;id&#125;回:&#123;title&#125;&quot;</span><br>        <span class="hljs-comment">#id += 1</span><br>        content_url = <span class="hljs-string">&quot;http://www.purepen.com/sgyy/&quot;</span> + mulu[<span class="hljs-string">&quot;href&quot;</span>]<br>        <span class="hljs-comment">#print(content_url)</span><br>        html = requests.get(url=content_url,headers=headers)<br>        html.encoding = html.apparent_encoding<br>        content_html = html.text<br>        content_soup = BeautifulSoup(content_html,<span class="hljs-string">&quot;lxml&quot;</span>)<br>        title = content_soup.find(<span class="hljs-string">&quot;p&quot;</span>,align=<span class="hljs-string">&quot;center&quot;</span>).text[<span class="hljs-number">6</span>:]<br>        content = content_soup.find(<span class="hljs-string">&quot;pre&quot;</span>,style=<span class="hljs-string">&quot;line-height: 200%&quot;</span>).text<br>        fp.write(title+<span class="hljs-string">&quot;\n&quot;</span>+content+<span class="hljs-string">&quot;\n&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;title&#125;</span>         下载完毕！&quot;</span>)<br>    fp.close()<br></code></pre></td></tr></table></figure></li>
</ol>
<h3 id="xpath"><a href="#xpath" class="headerlink" title="xpath"></a>xpath</h3><p>xpath解析是最常用且最便捷高效的一种解析方式，通用性较强。</p>
<h4 id="xpath数据解析原理"><a href="#xpath数据解析原理" class="headerlink" title="xpath数据解析原理"></a>xpath数据解析原理</h4><ol>
<li>实例化一个etree对象，并且需要将被解析的页面源码数据加载到该对象中。</li>
<li>调用etree对象中的xpath方法，结合xpath表达式实现标签的定位和内容的捕获。</li>
</ol>
<h4 id="环境安装-1"><a href="#环境安装-1" class="headerlink" title="环境安装"></a>环境安装</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip3 install lxml<br></code></pre></td></tr></table></figure>

<h4 id="实例化etree对象"><a href="#实例化etree对象" class="headerlink" title="实例化etree对象"></a>实例化etree对象</h4><ol>
<li><p>本地html源码数据加载到etree对象中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><br><span class="hljs-comment">#实例化etree对象，且将源码数据加载到该对象中</span><br>tree = etree.parse(filePath)<span class="hljs-comment">#filePath是HTML文件的存储路径</span><br></code></pre></td></tr></table></figure></li>
<li><p>将互联网的网页数据加载到etree 中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><br>url = <span class="hljs-string">&quot;XXXX&quot;</span><br>page_text = requests.get(url = url).text<br><br><span class="hljs-comment">#实例化etree对象，且将page_text源码数据加载到该对象</span><br>tree = etree.HTML(page_text)<span class="hljs-comment">#page_text是从网络上爬取的HTML文件</span><br></code></pre></td></tr></table></figure></li>
</ol>
<h4 id="xpath表达式"><a href="#xpath表达式" class="headerlink" title="xpath表达式"></a>xpath表达式</h4><p><u><strong>xpath()方法</strong></u>返回的数据类型是<u><strong>列表</strong></u>。<u><strong>xpath()方法</strong></u>返回的数据类型是<u><strong>列表</strong></u>。</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>  <span class="hljs-comment">#实例化etree对象，调用parse()方法读取本地html加载到该对象</span><br>  tree = etree.parse(<span class="hljs-string">&quot;test.html&quot;</span>)<br>  <br>  <span class="hljs-comment">#tree对象调用xpath方法进行标签定位，返回的数据类型是列表</span><br>  <span class="hljs-comment">#	/html/head/title	定位到title标签，第一个反斜杠/表示根目录</span><br>  <span class="hljs-comment">#返回的是tittle标签对应的对象列表:[&lt;Element title at 0x123fg87&gt;]</span><br>  r = tree.xpath(<span class="hljs-string">&quot;/html/head/title&quot;</span>)<br>  <br>  <span class="hljs-comment">#同理，根据test.html源码的层级关系定位div标签</span><br>  <span class="hljs-comment">#返回的是所有同层级的div标签对应的对象（多个对象元素在一个列表中）</span><br>  r = tree.xpath(<span class="hljs-string">&quot;/html/head/div&quot;</span>)<br>  <span class="hljs-comment">#优化xpath()参数的形式，&quot;/html/head/div&quot;等价于以下两个语句</span><br>  r = tree.xpath(<span class="hljs-string">&quot;/html//div&quot;</span>)<br>  r = tree.xpath(<span class="hljs-string">&quot;//div&quot;</span>)<br>  <br>  <span class="hljs-comment">#综上，	/	：从根结点开始定位，表示一个层级</span><br>  <span class="hljs-comment">#			//	：表示多个层级，从任意位置开始定位</span><br>  <br>  <br>  <span class="hljs-comment">#属性定位</span><br>  <span class="hljs-comment">#定位到目的标签列表后，还需要根据实际需求继续筛选</span><br>  <span class="hljs-comment">#使用标签的属性定位可以更加精准定位到所需要的内容</span><br>  <span class="hljs-comment">#定位属性为song的div标签:	tag[@attrName=&quot;attrValue&quot;]</span><br>  r = tree.xpath(<span class="hljs-string">&quot;//div[@class=&quot;</span>song<span class="hljs-string">&quot;]&quot;</span>)<br>  <br>  <br>  <span class="hljs-comment">#索引定位</span><br>  <span class="hljs-comment">#在属性定位中定位到属性为song的div标签后，我们想继续定位该标签层级下的第3个p标签</span><br>  r = tree.xpath(<span class="hljs-string">&quot;//div[@class=&quot;</span>song<span class="hljs-string">&quot;]/p[3]&quot;</span>)<br>  <span class="hljs-comment">#	//div[@class=&quot;song&quot;]/p[3]	索引是从1开始，3表示偏移量，第3个</span><br>  <br>  <br>  <span class="hljs-comment">#获取文本</span><br>  <span class="hljs-comment">#获取第5个li标签中a标签的文本内容,返回文本内容列表：	[&quot;杜牧&quot;]</span><br>  r = tree.xpath(<span class="hljs-string">&quot;//div[@class=&quot;</span>tang<span class="hljs-string">&quot;]/ul/li[5]/a/text()&quot;</span>)<br>  <span class="hljs-comment">#简化层级关系，加上文本内容列表下标索引[0]，则返回：	杜牧</span><br>  r = tree.xpath(<span class="hljs-string">&quot;//div[@class=&quot;</span>tang<span class="hljs-string">&quot;]//li[5]/a/text()&quot;</span>)[<span class="hljs-number">0</span>]<br>  <span class="hljs-comment">#获取第7个li标签中i标签的文本内容,返回文本内容列表：	度蜜月</span><br>  r = tree.xpath(<span class="hljs-string">&quot;//li[7]//text()&quot;</span>)[<span class="hljs-number">0</span>]<br>  <span class="hljs-comment">#综上，/text()	获取的是标签中直系的文本内容列表</span><br>  <span class="hljs-comment">#			//text()	获取的是标签下所有的文本内容列表</span><br>  <span class="hljs-comment">#注意返回类型是列表，列表，列表</span><br>  r = tree.xpath(<span class="hljs-string">&quot;//div[@class=&quot;</span>tang<span class="hljs-string">&quot;]/text()&quot;</span>)<br>  r = tree.xpath(<span class="hljs-string">&quot;//div[@class=&quot;</span>tang<span class="hljs-string">&quot;]//text()&quot;</span>)<br>  <br>  <br>  <span class="hljs-comment">#获取属性</span><br>  <span class="hljs-comment">#在定位到标签后，我们想要获取标签中的网页、图片等地址及其他属性</span><br>  <span class="hljs-comment">#获取属性为song的div标签层级中的img标签中的属性图片地址src</span><br>  r = tree.xpath(<span class="hljs-string">&quot;//div[@class=&quot;</span>song<span class="hljs-string">&quot;]/img/@src&quot;</span>)<br>  <span class="hljs-comment">#	/@attrName	获取标签属性</span><br></code></pre></td></tr></table></figure>

<h4 id="实战案例-3"><a href="#实战案例-3" class="headerlink" title="实战案例"></a>实战案例</h4><h5 id="58同城二手房"><a href="#58同城二手房" class="headerlink" title="58同城二手房"></a>58同城二手房</h5><ol>
<li><p>需求：爬取二手房的信息标题</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="https://www.58.com/ershoufang/">https://www.58.com/ershoufang/</a></p>
</li>
<li><p>说明：爬取时间为2021年9月1日。利用抓包工具找到目标标签后，右键可以复制xpath地址。有些换行等html格式标签并不是xpath表达式识别中的标签，因此不能写入xpath表达式中来进行标签定位。</p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br><br>    url = <span class="hljs-string">&quot;https://www.58.com/ershoufang/&quot;</span><br>    page_text = requests.get(url = url, headers = headers).text<br><br>    <span class="hljs-comment">#实例化etree对象，加载page_text源码数据到tree中</span><br>    tree = etree.HTML(page_text)<br>    <span class="hljs-comment">#tree调用xpath()方法进行标签定位到tr标签</span><br>    <span class="hljs-comment">#在实际使用xpath表达式时，语句非常灵活，*为标签通配符</span><br>    <span class="hljs-comment">#获取tr标签列表</span><br>    <span class="hljs-comment">#根据层级关系有多种写法，这里不再赘述。但值得注意的是，tbody不可以写入xpath表达式中进行定位，否则xpath()方法识别不了，返回空列表</span><br>    tr_list = tree.xpath(<span class="hljs-string">&#x27;//*[@border=&quot;0&quot;]/tr&#x27;</span>)<br>    <br>    <span class="hljs-comment">#遍历tr标签列表</span><br>    <span class="hljs-keyword">for</span> tr <span class="hljs-keyword">in</span> tr_list:<br>        <span class="hljs-comment">#tr标签的局部解析，调用xpath()方法</span><br>        <span class="hljs-comment">#tr为tree的一部分，同样可以调用xpath()进行标签定位</span><br>        <span class="hljs-comment">#此时，根节点为tr标签。</span><br>        <span class="hljs-comment">#	./	表示当前标签位置的下一个层级</span><br>        <span class="hljs-comment">#	.//	表示当前标签位置的下几个层级</span><br>        title = tr.xpath(<span class="hljs-string">&#x27;.//a/text()&#x27;</span>)[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment">#title = tr.xpath(&#x27;./td[2]/a/text()&#x27;)[0]等价于上述xpath()方法标签定位语句</span><br>        <span class="hljs-built_in">print</span>(title)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;完成！&quot;</span>)<br></code></pre></td></tr></table></figure></li>
</ol>
<h5 id="爬取4K图片"><a href="#爬取4K图片" class="headerlink" title="爬取4K图片"></a>爬取4K图片</h5><ol>
<li><p>需求：爬取4K图片，并存储到本地</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="https://pic.netbian.com/4kyouxi/index.html">https://pic.netbian.com/4kyouxi/index.html</a></p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br><br>    url = <span class="hljs-string">&quot;https://pic.netbian.com/4kyouxi/index.html&quot;</span><br>    html = requests.get(url = url, headers = headers)<br>    <span class="hljs-comment">#解决中文乱码，根据需求可以不用考虑乱码问题，直接爬取page_text</span><br>    html.encoding = html.apparent_encoding<br>    page_text = html.text<br><br>    tree = etree.HTML(page_text)<br>    li_list = tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&quot;./4k&quot;</span>):<br>        os.mkdir(<span class="hljs-string">&quot;./4k&quot;</span>)<br>    <span class="hljs-comment">#fp = open()</span><br>    <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>        img_url = <span class="hljs-string">&quot;https://pic.netbian.com&quot;</span> + li.xpath(<span class="hljs-string">&#x27;.//img/@src&#x27;</span>)[<span class="hljs-number">0</span>]<br>        img_name = <span class="hljs-built_in">str</span>(li.xpath(<span class="hljs-string">&#x27;.//b/text()&#x27;</span>)[<span class="hljs-number">0</span>])<br>        img_content = requests.get(url = img_url, headers = headers).content<br>        fileName = <span class="hljs-string">&quot;./4k/&quot;</span> + img_name + <span class="hljs-string">&quot;.jpg&quot;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fileName,<span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(img_content)<br></code></pre></td></tr></table></figure></li>
</ol>
<h5 id="爬取全国城市名称"><a href="#爬取全国城市名称" class="headerlink" title="爬取全国城市名称"></a>爬取全国城市名称</h5><ol>
<li><p>需求：爬取全国城市名称</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="https://www.aqistudy.cn/historydata/">https://www.aqistudy.cn/historydata/</a></p>
</li>
<li><p>说明：可以将目标标签所在的xpath表达式用    | （或）符号连接，并集计算赋值到tree列表，得到目标标签的列表</p>
</li>
<li><p>代码如下：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>    url = <span class="hljs-string">&quot;https://www.aqistudy.cn/historydata/&quot;</span><br>    tree = etree.HTML(requests.get(url = url, headers = headers).text)<br>    all_city = []<br>    <br>    <span class="hljs-comment">#根据层级关系，可以将热门城市和一般城市统一写为如下</span><br>    <span class="hljs-comment">#//*[@class=&quot;bottom&quot;]//a</span><br>    city_a_list = tree.xpath(<span class="hljs-string">&#x27;//*[@class=&quot;bottom&quot;]//a&#x27;</span>)<br>    <span class="hljs-comment">#将目标城市的a标签用 | （或）符号连接，并集计算赋值到列表</span><br>    <span class="hljs-comment">#city_a_list = tree.xpath(&#x27;//div[@class=&quot;bottom&quot;]/ul/li/a | //div[@class=&quot;bottom&quot;]/ul/div[2]/li/a&#x27;)</span><br>    <br>    <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> city_a_list:<br>        city_name = a.xpath(<span class="hljs-string">&#x27;./text()&#x27;</span>)[<span class="hljs-number">0</span>]<br>        all_city.append((city_name))<br>    <span class="hljs-built_in">print</span>(all_city)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">set</span>(all_city))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(all_city)))<br></code></pre></td></tr></table></figure></li>
<li><p>注：如果爬取下载内容是压缩包形式，则数据类型是二进制数据。</p>
</li>
</ol>
<h2 id="验证码识别"><a href="#验证码识别" class="headerlink" title="验证码识别"></a>验证码识别</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>验证码是门户网站的反爬机制，在用户登录时，输入密码后必须手动输入正确的验证码后才能成功登录。</p>
<p>识别验证码的方式：</p>
<ol>
<li>人工肉眼识别（不推荐）</li>
<li><strong>自己手写图片识别程序：tensorflow + 深度学习</strong></li>
<li>第三方线上平台自动识别<ol>
<li>超级鹰：</li>
<li>联众打码</li>
</ol>
</li>
</ol>
<h3 id="超级鹰"><a href="#超级鹰" class="headerlink" title="超级鹰"></a>超级鹰</h3><p><strong>这里使用超级鹰平台，注册登录后，扫码关注微信公众号，绑定账号后，赠送1000题分。</strong></p>
<h4 id="使用流程"><a href="#使用流程" class="headerlink" title="使用流程"></a>使用流程</h4><ol>
<li><p>网址：<a target="_blank" rel="noopener" href="http://www.chaojiying.com/">http://www.chaojiying.com</a></p>
</li>
<li><p>注册登录：以本人账号为例：</p>
<ol>
<li><p>用户名：gujun0918</p>
<p>密码：mypassword0918</p>
</li>
<li><p>选择“开发文档”，在“常用开发语言示例下载”中选择python语言版本，如下图：</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB29.png" srcset="/img/loading.gif" lazyload alt="爬虫29"></p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB30.png" srcset="/img/loading.gif" lazyload alt="爬虫30"></p>
<p>点击下载即可。</p>
</li>
</ol>
</li>
<li><p>解压压缩包：<img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB31.png" srcset="/img/loading.gif" lazyload alt="爬虫31"></p>
<p>原来的代码需要改动：</p>
<ol>
<li><p>主函数中用户名和密码改为本人账号</p>
</li>
<li><p> 最后一行print加上()</p>
</li>
<li><p> 倒数第三行中参数<code>96001</code>需要用<code>软件ID</code>替换</p>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">chaojiying = Chaojiying_Client(<span class="hljs-string">&#x27;gujun0918&#x27;</span>, <span class="hljs-string">&#x27;mypassword0918&#x27;</span>, <span class="hljs-string">&#x27;96001&#x27;</span>)<br><span class="hljs-comment">#用	软件ID	替换	96001</span><br></code></pre></td></tr></table></figure></li>
<li><p>生成软件ID：<img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB32.png" srcset="/img/loading.gif" lazyload alt="爬虫32"></p>
<p>在“用户中心”选择“软件ID”，点击“生成一个软件ID”后，如下图：<img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB33.png" srcset="/img/loading.gif" lazyload alt="爬虫33"></p>
<p>软件名称自定义，提交后，复制“软件KEY”的内容，替换上面第3步中提到的<code>96001</code></p>
</li>
<li><p>确定验证码类型：在“价格体系”中，如下图：<img src="/2021/05/07/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB34.png" srcset="/img/loading.gif" lazyload alt="爬虫34"></p>
<p>选择需要识别的图片验证码类型，这里选择<code>1902</code></p>
</li>
<li><p>修改后完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># coding:utf-8</span><br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> hashlib <span class="hljs-keyword">import</span> md5<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Chaojiying_Client</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, username, password, soft_id</span>):</span><br>        self.username = username<br>        password =  password.encode(<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>        self.password = md5(password).hexdigest()<br>        self.soft_id = soft_id<br>        self.base_params = &#123;<br>            <span class="hljs-string">&#x27;user&#x27;</span>: self.username,<br>            <span class="hljs-string">&#x27;pass2&#x27;</span>: self.password,<br>            <span class="hljs-string">&#x27;softid&#x27;</span>: self.soft_id,<br>        &#125;<br>        self.headers = &#123;<br>            <span class="hljs-string">&#x27;Connection&#x27;</span>: <span class="hljs-string">&#x27;Keep-Alive&#x27;</span>,<br>            <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#x27;</span>,<br>        &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PostPic</span>(<span class="hljs-params">self, im, codetype</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        im: 图片字节</span><br><span class="hljs-string">        codetype: 题目类型 参考 http://www.chaojiying.com/price.html</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        params = &#123;<br>            <span class="hljs-string">&#x27;codetype&#x27;</span>: codetype,<br>        &#125;<br>        params.update(self.base_params)<br>        files = &#123;<span class="hljs-string">&#x27;userfile&#x27;</span>: (<span class="hljs-string">&#x27;ccc.jpg&#x27;</span>, im)&#125;<br>        r = requests.post(<span class="hljs-string">&#x27;http://upload.chaojiying.net/Upload/Processing.php&#x27;</span>, data=params, files=files, headers=self.headers)<br>        <span class="hljs-keyword">return</span> r.json()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReportError</span>(<span class="hljs-params">self, im_id</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        im_id:报错题目的图片ID</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        params = &#123;<br>            <span class="hljs-string">&#x27;id&#x27;</span>: im_id,<br>        &#125;<br>        params.update(self.base_params)<br>        r = requests.post(<span class="hljs-string">&#x27;http://upload.chaojiying.net/Upload/ReportError.php&#x27;</span>, data=params, headers=self.headers)<br>        <span class="hljs-keyword">return</span> r.json()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>	chaojiying = Chaojiying_Client(<span class="hljs-string">&#x27;gujun0918&#x27;</span>, <span class="hljs-string">&#x27;mypassword0918&#x27;</span>, <span class="hljs-string">&#x27;96001&#x27;</span>)	<span class="hljs-comment">#用户中心&gt;&gt;软件ID 生成一个替换 96001</span><br>	im = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;a.jpg&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>).read()													<span class="hljs-comment">#本地图片文件路径 来替换 a.jpg 有时WIN系统须要//</span><br>	<span class="hljs-built_in">print</span>(chaojiying.PostPic(im, <span class="hljs-number">1902</span>)[<span class="hljs-string">&#x27;pic_str&#x27;</span>])<br>  <span class="hljs-comment">#运行可以识别本地的 a.jpg 文件：7261</span><br></code></pre></td></tr></table></figure></li>
</ol>
<h3 id="实战案例-4"><a href="#实战案例-4" class="headerlink" title="实战案例"></a>实战案例</h3><ol>
<li><p>需求：识别古诗文网页面登录验证码，完成登录</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="https://so.gushiwen.cn/user/login.aspx">https://so.gushiwen.cn/user/login.aspx</a></p>
</li>
<li><p>注册个人账户：</p>
<ol>
<li>账号：<a href="mailto:&#x33;&#x34;&#x34;&#x37;&#x37;&#x31;&#53;&#57;&#x39;&#x40;&#113;&#x71;&#46;&#x63;&#x6f;&#x6d;">&#x33;&#x34;&#x34;&#x37;&#x37;&#x31;&#53;&#57;&#x39;&#x40;&#113;&#x71;&#46;&#x63;&#x6f;&#x6d;</a></li>
<li>密码：mypassword0918</li>
<li> 图片验证码类型：4位英文和阿拉伯数字，即<code>1902类型</code></li>
</ol>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># coding:utf-8</span><br><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> hashlib <span class="hljs-keyword">import</span> md5<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Chaojiying_Client</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, username, password, soft_id</span>):</span><br>        self.username = username<br>        password =  password.encode(<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>        self.password = md5(password).hexdigest()<br>        self.soft_id = soft_id<br>        self.base_params = &#123;<br>            <span class="hljs-string">&#x27;user&#x27;</span>: self.username,<br>            <span class="hljs-string">&#x27;pass2&#x27;</span>: self.password,<br>            <span class="hljs-string">&#x27;softid&#x27;</span>: self.soft_id,<br>        &#125;<br>        self.headers = &#123;<br>            <span class="hljs-string">&#x27;Connection&#x27;</span>: <span class="hljs-string">&#x27;Keep-Alive&#x27;</span>,<br>            <span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#x27;</span>,<br>        &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PostPic</span>(<span class="hljs-params">self, im, codetype</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        im: 图片字节</span><br><span class="hljs-string">        codetype: 题目类型 参考 http://www.chaojiying.com/price.html</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        params = &#123;<br>            <span class="hljs-string">&#x27;codetype&#x27;</span>: codetype,<br>        &#125;<br>        params.update(self.base_params)<br>        files = &#123;<span class="hljs-string">&#x27;userfile&#x27;</span>: (<span class="hljs-string">&#x27;ccc.jpg&#x27;</span>, im)&#125;<br>        r = requests.post(<span class="hljs-string">&#x27;http://upload.chaojiying.net/Upload/Processing.php&#x27;</span>, data=params, files=files, headers=self.headers)<br>        <span class="hljs-keyword">return</span> r.json()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReportError</span>(<span class="hljs-params">self, im_id</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        im_id:报错题目的图片ID</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        params = &#123;<br>            <span class="hljs-string">&#x27;id&#x27;</span>: im_id,<br>        &#125;<br>        params.update(self.base_params)<br>        r = requests.post(<span class="hljs-string">&#x27;http://upload.chaojiying.net/Upload/ReportError.php&#x27;</span>, data=params, headers=self.headers)<br>        <span class="hljs-keyword">return</span> r.json()<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">from</span> chaojiying <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br><br>    login_url = <span class="hljs-string">&quot;https://so.gushiwen.cn/user/login.aspx&quot;</span><br>    tree = etree.HTML(requests.get(url = login_url, headers = headers).text)<br>    img_url = <span class="hljs-string">&quot;https://so.gushiwen.cn&quot;</span> + tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;imgCode&quot;]/@src&#x27;</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-built_in">print</span>(img_url)<br>    img_content = requests.get(url = img_url,headers = headers).content<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./code.jpg&quot;</span>,<span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(img_content)<br><br>    chaojiying = Chaojiying_Client(<span class="hljs-string">&#x27;gujun0918&#x27;</span>, <span class="hljs-string">&#x27;mypassword0918&#x27;</span>, <span class="hljs-string">&#x27;bbd2ecdaef2733941685815af047609c&#x27;</span>)  <span class="hljs-comment"># 用户中心&gt;&gt;软件ID 生成一个替换 96001</span><br>    im = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;code.jpg&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>).read()  <span class="hljs-comment"># 本地图片文件路径 有时WIN系统须要//</span><br>    <span class="hljs-built_in">print</span>(chaojiying.PostPic(im, <span class="hljs-number">1902</span>)[<span class="hljs-string">&#x27;pic_str&#x27;</span>])<br></code></pre></td></tr></table></figure></li>
</ol>
<h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><p>本节的目的是爬取基于某些用户的个人信息。</p>
<h3 id="实战案例-5"><a href="#实战案例-5" class="headerlink" title="实战案例"></a>实战案例</h3><h4 id="网站登录"><a href="#网站登录" class="headerlink" title="网站登录"></a>网站登录</h4><ol>
<li><p>需求：对人人网进行模拟登录。</p>
</li>
<li><p>网址：</p>
</li>
<li><p>说明：i. 点击登录按钮会发送一个POST请求</p>
<p>​            ii. POST请求会携带登录前输入的相关登录信息（用户名，密码，                验证码等）</p>
<p>​            iii.  验证码：每次请求都会变化</p>
</li>
<li><p>流程：a. 验证码的识别，获取验证码图片的文字数据，使用超级鹰平台</p>
<p>​            b. 对POST请求进行发送（处理请求参数）</p>
<p>​            c. 对响应数据进行存储处理</p>
</li>
<li><p>代码如下：</p>
</li>
</ol>
<h4 id="爬取用户信息"><a href="#爬取用户信息" class="headerlink" title="爬取用户信息"></a>爬取用户信息</h4><ol>
<li><p>需求：登录成功后，爬取当前用户的个人主页信息</p>
</li>
<li></li>
<li><p>说明：a. http/https协议特性：无状态，即客户端向服务器发送请求后，</p>
<p>​                服务器并不会记录当前客户端的状态。对应到项目中，当客户端</p>
<p>​                登录后，服务器并未记录当前客户端的登录状态。我们再对个人</p>
<p>​                主页发送请求时，并不会得到我们期待的响应，服务器检测到当</p>
<p>​                前请求主页的链接为未登录状态，会响应回登录页面。</p>
<p>​            b. cookie：用来使服务器端记录保留客户端的相关状态。cookie值</p>
<p>​                通常是在模拟登录post请求后，由服务器端创建。</p>
<p>​                i.  手动处理：通过抓包工具获取cookie值，将该值封装到</p>
<p>​                                        headers中。（不建议）</p>
<p>​                ii. 自动处理：这里介绍session会话对象：</p>
<p>​                                        session会话对象的作用：</p>
<p>​                                        1）可以进行请求发送，用法同requests。</p>
<p>​                                        2）如果请求过程中产生了cookie，则该cookie会被</p>
<p>​                                            自动存储/携带在该session会话对象中。</p>
<p>​                                        – 创建session对象：session=requests.Session()</p>
<p>​                                        – 使用session对象进行模拟登录post请求的发送</p>
<p>​                                            （cookie就会被存储在session中）；</p>
<p>​                                        – session对象对个人主页对应的get请求进行发送</p>
<p>​                                            （携带了cookie）</p>
<p>​                                        </p>
</li>
<li><p>代码如下：</p>
</li>
</ol>
<h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><p>代理，即代理服务器。如果门户网站检测到某一IP地址的访问频率超过其规定的合理值，则门户网站会禁止该IP地址客户端的继续访问，即封禁IP。代理可以突破自身IP访问频率的限制，同时可以隐藏自身的真实IP。</p>
<h4 id="相关网站"><a href="#相关网站" class="headerlink" title="相关网站"></a>相关网站</h4><ol>
<li>快代理</li>
<li>西祠代理</li>
<li><a target="_blank" rel="noopener" href="http://www.goubanjia.com/">www.goubanjia.com</a></li>
</ol>
<h4 id="IP的类型"><a href="#IP的类型" class="headerlink" title="IP的类型"></a>IP的类型</h4><ol>
<li>http：只能应用到http协议对应的URL中</li>
<li>https：只能应用到https协议对应的URL中</li>
</ol>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">resp = requests.get(url=url,headers=heardes,proxies=&#123;<span class="hljs-string">&quot;http&quot;</span>:<span class="hljs-string">&quot;IP地址&quot;</span>&#125;).text<br></code></pre></td></tr></table></figure>

<p>注：引入proxies字典类型参数，字典的第一个参数是代理IP类型，第二个参数是代理IP地址。服务器将响应返回给代理IP，代理IP将响应再转交给本地真实IP。</p>
<h4 id="透明度"><a href="#透明度" class="headerlink" title="透明度"></a>透明度</h4><ol>
<li>透明：服务器知道本次请求使用了代理，也知道对应的真实IP。</li>
<li>匿名：服务器知道使用了代理，不知道真实IP。</li>
<li>高匿：服务器不知道使用了代理，更不知道真实IP。</li>
</ol>
<h4 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h4><p>本地的IP为210.30.193.173</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br>url = <span class="hljs-string">&quot;https://www.baidu.com/s?=wd=ip&quot;</span><br>headers = &#123;<br>	<span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>&#125;<br>page_text = requests.get(url=url,headers=headers,proxies=&#123;<span class="hljs-string">&quot;https&quot;</span>:<span class="hljs-string">&quot;IP地址&quot;</span>&#125;).text<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;ip.html&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>  fp.write(page_text)<br></code></pre></td></tr></table></figure>

<h2 id="异步爬虫"><a href="#异步爬虫" class="headerlink" title="异步爬虫"></a>异步爬虫</h2><p>在爬虫中使用异步实现高性能的数据爬取。</p>
<h3 id="同步爬虫"><a href="#同步爬虫" class="headerlink" title="同步爬虫"></a>同步爬虫</h3><p>同步爬虫是单线程下的串行程序，程序是阻塞串行的方式依次执行的，效率较低。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    hearders = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br><br>    urls = [<br>        <span class="hljs-string">&quot;https://www.baidu.com&quot;</span>,<br>        <span class="hljs-string">&quot;https://www.bilibili.com&quot;</span>,<br>        <span class="hljs-string">&quot;https://movie.douban.com&quot;</span><br>    ]<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_content</span>(<span class="hljs-params">url</span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在爬取：&quot;</span>,url)<br>    <span class="hljs-comment">#get方法是一个阻塞的方法，后者必须等待前者执行完毕，才能执行后者</span><br>    response = requests.get(url=url,headers=hearders)<br>    <span class="hljs-keyword">if</span> response.status_code == <span class="hljs-number">200</span>:<br>        <span class="hljs-keyword">return</span> response.text<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">len_content</span>(<span class="hljs-params">content</span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;爬取长度：&quot;</span>,<span class="hljs-built_in">len</span>(content))<br><br><span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls:<br>    content = get_content(url)<br>    len_content(content)<br></code></pre></td></tr></table></figure>

<h3 id="异步爬虫的方式"><a href="#异步爬虫的方式" class="headerlink" title="异步爬虫的方式"></a>异步爬虫的方式</h3><p><strong>注：关于异步爬虫的用法，建议移步至<code>Python并发编程</code>和<code>异步协程编程</code>后，再来学习第6天的爬虫课程。</strong></p>
<ol>
<li><p>多线程、多进程<strong>（不建议使用）</strong></p>
<p>优点：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。</p>
<p>缺点：无法无限制地开启多线程或者多进程，否则频繁地创建和销毁对应的线程或进程，会严重占用CPU资源。</p>
</li>
<li><p>线程池、进程池<strong>（适当使用）</strong></p>
<p>优点：可以降低系统对线程或者进程创建和销毁的频率，从而有效地降低系统的开销。</p>
<p>缺点：池中线程或者进程的数量是有上限的。当阻塞操作的数量远高于池中线程或者进程的数量时，难以明显地提高执行效率。</p>
<p>应用对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#使用单线程串行的方式执行</span><br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_page</span>(<span class="hljs-params"><span class="hljs-built_in">str</span></span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在下载：&quot;</span>,<span class="hljs-built_in">str</span>)<br>    time.sleep(<span class="hljs-number">2</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;下载成功：&quot;</span>,<span class="hljs-built_in">str</span>)<br><br>name_list = [<span class="hljs-string">&quot;1&quot;</span>,<span class="hljs-string">&quot;2&quot;</span>,<span class="hljs-string">&quot;3&quot;</span>,<span class="hljs-string">&quot;4&quot;</span>]<br><br>start_time = time.time()<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">str</span> <span class="hljs-keyword">in</span> name_list:<br>    get_page(<span class="hljs-built_in">str</span>)<br>end_time = time.time()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;end_time-start_time&#125;</span> second&quot;</span>)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">正在下载： 1</span><br><span class="hljs-string">下载成功： 1</span><br><span class="hljs-string">正在下载： 2</span><br><span class="hljs-string">下载成功： 2</span><br><span class="hljs-string">正在下载： 3</span><br><span class="hljs-string">下载成功： 3</span><br><span class="hljs-string">正在下载： 4</span><br><span class="hljs-string">下载成功： 4</span><br><span class="hljs-string">8.017254114151001 second</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-comment">#导入线程池模块对应的类</span><br><span class="hljs-keyword">from</span> multiprocessing.dummy <span class="hljs-keyword">import</span> Pool<br><br>start_time = time.time()<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_page</span>(<span class="hljs-params"><span class="hljs-built_in">str</span></span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在下载：&quot;</span>,<span class="hljs-built_in">str</span>)<br>    time.sleep(<span class="hljs-number">2</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;下载成功：&quot;</span>,<span class="hljs-built_in">str</span>)<br><br>name_list = [<span class="hljs-string">&quot;1&quot;</span>,<span class="hljs-string">&quot;2&quot;</span>,<span class="hljs-string">&quot;3&quot;</span>,<span class="hljs-string">&quot;4&quot;</span>]<br><br><span class="hljs-comment">#实例化一个线程池对象，数量定义为4</span><br><span class="hljs-comment">#get_page是阻塞函数，将会有4个阻塞操作</span><br>pool = Pool(<span class="hljs-number">4</span>)<br><span class="hljs-comment">#map将name_list列表中每一个列表元素传递给get_page函数进行处理</span><br>pool.<span class="hljs-built_in">map</span>(get_page,name_list)<br><span class="hljs-comment">#map第一个参数是阻塞函数，第二个参数是可迭代对象</span><br><br>end_time = time.time()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">正在下载： 1</span><br><span class="hljs-string">正在下载： 2</span><br><span class="hljs-string">正在下载： 3</span><br><span class="hljs-string">正在下载： 4</span><br><span class="hljs-string">下载成功： 2</span><br><span class="hljs-string">下载成功： 4</span><br><span class="hljs-string">下载成功： 3</span><br><span class="hljs-string">下载成功： 1</span><br><span class="hljs-string">2.041308879852295 second</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>

<p>爬取梨视频</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> multiprocessing.dummy <span class="hljs-keyword">import</span> Pool<br><span class="hljs-comment">#需求：爬取梨视频</span><br><span class="hljs-comment">#原则：线程池处理阻塞且耗时的操作</span><br><span class="hljs-comment">#地址：https://www.pearvideo.com/category_5</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    headers = &#123;<br>        <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>    &#125;<br>    home_url = <span class="hljs-string">&quot;https://www.pearvideo.com/category_1&quot;</span><br>    tree = etree.HTML(requests.get(url=home_url,headers=headers).text)<br>    li_list = tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;listvideoListUl&quot;]/li&#x27;</span>)<br><br>    video_urls = []<br>    <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>        detail_url = <span class="hljs-string">&quot;https://www.pearvideo.com/&quot;</span> + li.xpath(<span class="hljs-string">&#x27;./div/a/@href&#x27;</span>)[<span class="hljs-number">0</span>]<br>        video_name = li.xpath(<span class="hljs-string">&#x27;./div/a/div[2]/text()&#x27;</span>)[<span class="hljs-number">0</span>]<br>        video_id = detail_url.split(<span class="hljs-string">&#x27;_&#x27;</span>)[<span class="hljs-number">1</span>]<br>        json_url = <span class="hljs-string">&quot;https://www.pearvideo.com/videoStatus.jsp?contId=&quot;</span> + video_id<br>        headers = &#123;<br>            <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span>,<br>            <span class="hljs-string">&quot;Referer&quot;</span>: <span class="hljs-string">&quot;https://www.pearvideo.com/video_&quot;</span> + video_id,<br>            <span class="hljs-string">&quot;X-Requested-With&quot;</span>: <span class="hljs-string">&quot;XMLHttpRequest&quot;</span><br>        &#125;<br>        video_dict = requests.get(url=json_url, headers=headers).json()<br>        video_url = video_dict[<span class="hljs-string">&quot;videoInfo&quot;</span>][<span class="hljs-string">&quot;videos&quot;</span>][<span class="hljs-string">&quot;srcUrl&quot;</span>]<br>        <span class="hljs-built_in">dict</span> = &#123;<br>            <span class="hljs-string">&quot;name&quot;</span>:video_name,<br>            <span class="hljs-string">&quot;url&quot;</span>:video_url<br>        &#125;<br>        video_urls.append(<span class="hljs-built_in">dict</span>)<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_video_data</span>(<span class="hljs-params"><span class="hljs-built_in">dict</span></span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">dict</span>[<span class="hljs-string">&quot;name&quot;</span>],<span class="hljs-string">&quot;正在下载......&quot;</span>)<br>    video_data = requests.get(url=<span class="hljs-built_in">dict</span>[<span class="hljs-string">&quot;url&quot;</span>],headers=headers).content<br>    name_video = <span class="hljs-built_in">dict</span>[<span class="hljs-string">&quot;name&quot;</span>] + <span class="hljs-string">&quot;.mp4&quot;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-built_in">dict</span>[<span class="hljs-string">&quot;name&quot;</span>],<span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br>        fp.write(video_data)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-built_in">dict</span>[<span class="hljs-string">&quot;name&quot;</span>], <span class="hljs-string">&quot;下载完成！&quot;</span>)<br><br>pool = Pool(<span class="hljs-number">4</span>)<br>pool.<span class="hljs-built_in">map</span>(get_video_data,video_urls)<br><br>pool.close()<br>pool.join()<br></code></pre></td></tr></table></figure>

<p>打包命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#安装pyinstaller</span><br>pip3 install pyinstaller<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#pyinstaller提供了两种将.py文件打包成.exe文件的方式</span><br><span class="hljs-comment">#1.把由.py文件打包成.exe文件及相关文件放在一个目录中</span><br><span class="hljs-comment">#语法：pyinstaller 应用程序</span><br>pyinstaller Hello.py<br><br><span class="hljs-comment">#2.加上 -F 参数后把制作出来的.exe文件打包成一个独立的.exe文件</span><br><span class="hljs-comment">#语法：pyinstaller -F 应用程序</span><br>pyinstaller -F Hello.py<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#pyinsatller打包的文件不跨操作系统，即Mac使用pyinstaller只能打包出Mac系统上运行的可执行脚本，在Windows上无法执行。同理，Windows执行打包命令亦如是。</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#如果你的脚本文件包含其他脚本，如hello.py包含（import）自定义脚本world.py或者是系统脚本sys.py，则需要在打包时加上其他脚本的路径。</span><br><br><span class="hljs-comment">#通过 -p 指定第三方包的路径，一条路径对应一个 -p</span><br>pyinstaller -F -p C:\SystemLib\site-packages -p C:\MyLib Hello.py<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#执行一次打包命令，通常会生成两个目录文件夹（build和dist）和一个附件（xx.spec）。build中的文件是编译过程中的中间产物，dist中的文件是最终可执行程序，spec文件是类似缓存，如果你第二次打包，则需要先把spec删掉，否则第二次打包会受影响。</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#pyinstaller参数介绍</span><br><span class="hljs-comment">#常用的主要是-F、-p、-i、-w这几个参数</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">-a：不包含编码.在支持Unicode的python版本上默认包含所有的编码</span><br><span class="hljs-string">-c：使用控制台子系统执行(默认)(只对Windows有效)</span><br><span class="hljs-string">-d：产生debug版本的可执行文件</span><br><span class="hljs-string">-i ：指定打包程序使用的图标（icon）文件</span><br><span class="hljs-string">-F：打包成可执行程序</span><br><span class="hljs-string">-h：查看帮助</span><br><span class="hljs-string">-p：添加使用的第三方库路径</span><br><span class="hljs-string">-v：查看 PyInstaller 版本</span><br><span class="hljs-string">-w：取消控制台显示（默认是显示控制台的）</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">pyinstaller -F -p C:\SystemLib\site-packages -p C:\MyLib main.py -i C:\image\excel.ico<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">解释：</span><br><span class="hljs-string">打包 main.py 脚本</span><br><span class="hljs-string">main.py包含第三方脚本，一个是系统脚本，一个是自定义脚本</span><br><span class="hljs-string">设置可执行程序的图标为excel.ico</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure></li>
<li><p>单线程+异步协程<strong>（推荐使用）</strong></p>
<p>相关概念：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">event_loop:事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足某些条件时，函数就会被循环执行。<br>  <br>coroutine:协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以使用 <span class="hljs-keyword">async</span> 关键字来定义一个方法，这个方法在调用时不会被立即执行，而是返回一个协程对象。<br>  <br>task:任务，它是对协程对象的进一步封装，包含了任务的各个状态。<br>  <br>future:代表将来执行或者还没执行的任务，实际上和 task 没有本质区别。<br>  <br><span class="hljs-keyword">async</span>:定义一个协程。<br>  <br><span class="hljs-keyword">await</span>:用来挂起阻塞方法的执行。<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> asyncio<br><br><span class="hljs-comment">#async修饰的函数，调用之后返回一个协程对象</span><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">request</span>(<span class="hljs-params">url</span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在请求的URL是&quot;</span>,url)<br>    <span class="hljs-built_in">print</span>(url,<span class="hljs-string">&quot;请求成功！&quot;</span>)<br>    <span class="hljs-keyword">return</span> url<br><br><span class="hljs-comment">#c是一个协程对象</span><br>c = request(<span class="hljs-string">&quot;www.baidu.com&quot;</span>)<br><span class="hljs-comment"># print(c)</span><br><br><span class="hljs-comment"># #创建一个事件循环对象</span><br><span class="hljs-comment"># loop =  asyncio.get_event_loop()</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># #将协程对象c注册到事件循环对象loop中，然后启动事件循环对象loop</span><br><span class="hljs-comment"># loop.run_until_complete(c)</span><br><br><span class="hljs-comment"># #task的使用</span><br><span class="hljs-comment"># loop = asyncio.get_event_loop()</span><br><span class="hljs-comment"># #基于事件循环对象创建一个task对象</span><br><span class="hljs-comment"># task = loop.create_task(c)</span><br><span class="hljs-comment"># print(task)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># loop.run_until_complete(task)</span><br><span class="hljs-comment"># print(task)</span><br><br><span class="hljs-comment"># #future使用</span><br><span class="hljs-comment"># loop = asyncio.get_event_loop()</span><br><span class="hljs-comment"># task = asyncio.ensure_future(c)</span><br><span class="hljs-comment"># print(task)</span><br><span class="hljs-comment"># loop.run_until_complete(task)</span><br><span class="hljs-comment"># print(task)</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">callback_func</span>(<span class="hljs-params">task</span>):</span><br>    <span class="hljs-comment">#result返回的就是任务对象中封装的协程对象对应函数的返回值</span><br>    <span class="hljs-built_in">print</span>(task.result())<br><br><span class="hljs-comment">#绑定回调</span><br>loop = asyncio.get_event_loop()<br>task = asyncio.ensure_future(c)<br><span class="hljs-comment">#将回调函数绑定到任务对象中</span><br>task.add_done_callback(callback_func)<br>loop.run_until_complete(task)<br></code></pre></td></tr></table></figure>

<p>多任务协程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">request</span>(<span class="hljs-params">url</span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在下载：&quot;</span>,url)<br>    <span class="hljs-comment">#在异步模块中，如果出现了同步模块的相关代码，就无法实现异步。time.sleep()是同步模块，需用await asyncio.sleep()代替</span><br>    <span class="hljs-comment"># time.sleep(2)</span><br>    <span class="hljs-comment">#出现asyncio.sleep()，需要使用关键字await修饰，手动挂起，表示异步阻塞IO</span><br>    <span class="hljs-keyword">await</span> asyncio.sleep(<span class="hljs-number">2</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;下载完成！&quot;</span>,url)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    start = time.time()<br>    urls = [<br>        <span class="hljs-string">&quot;www.baidu.com&quot;</span>,<br>        <span class="hljs-string">&quot;www.sogou.com&quot;</span>,<br>        <span class="hljs-string">&quot;www.xiaojiucai.cn&quot;</span><br>    ]<br><br>    tasks = [ request(url) <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls ]<br><br>    asyncio.run(asyncio.wait(tasks))<br>    end = time.time()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;use time seconds:&quot;</span>,end-start)<br></code></pre></td></tr></table></figure>

<p>flask服务</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask<br><span class="hljs-keyword">import</span> time<br><br>app = Flask(__name__)<br><br><span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">&quot;/1&quot;</span></span>)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">index_1</span>():</span><br>    time.sleep(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;1&quot;</span><br><br><span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">&quot;/2&quot;</span></span>)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">index_2</span>():</span><br>    time.sleep(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;2&quot;</span><br><br><span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">&quot;/3&quot;</span></span>)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">index_3</span>():</span><br>    time.sleep(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;3&quot;</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    app.run(threaded=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>requests模块不支持异步示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-keyword">import</span> time<br><br>headers = &#123;<br>    <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>&#125;<br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_page</span>(<span class="hljs-params">url</span>):</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在下载：&quot;</span>,url)<br>    response = requests.get(url,headers=headers)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;下载完成!&quot;</span>,response.text)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    start = time.time()<br>    urls = [<br>        <span class="hljs-string">&quot;http://127.0.0.1:5000/1&quot;</span>,<span class="hljs-string">&quot;http://127.0.0.1:5000/2&quot;</span>,<span class="hljs-string">&quot;http://127.0.0.1:5000/3&quot;</span><br>    ]<br>    tasks = [ get_page(url) <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls]<br>    asyncio.run(asyncio.wait(tasks))<br>    end = time.time()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;use time seconds:&quot;</span>,end-start)<br>    <br><span class="hljs-comment">#正在下载： http://127.0.0.1:5000/2</span><br><span class="hljs-comment">#下载完成! 2</span><br><span class="hljs-comment">#正在下载： http://127.0.0.1:5000/1</span><br><span class="hljs-comment">#下载完成! 1</span><br><span class="hljs-comment">#正在下载： http://127.0.0.1:5000/3</span><br><span class="hljs-comment">#下载完成! 3</span><br><span class="hljs-comment">#use time seconds: 6.038427114486694</span><br></code></pre></td></tr></table></figure>

<p>可以看出，requests模块依旧是同步串行的执行模式。我们需要用支持异步的模块aiohttp。</p>
<p>安装</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip3 install aiohttp<br></code></pre></td></tr></table></figure>

<p>语法</p>
<p>我们需要使用该模块中的<code>ClientSeeion</code>类创建<code>session</code>对象（session对象与<code>requests</code>用法类似）</p>
<p>aiohttp示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> asyncio<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> aiohttp<br><br>headers = &#123;<br>    <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15&quot;</span><br>&#125;<br><span class="hljs-keyword">async</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_page</span>(<span class="hljs-params">url</span>):</span><br>    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> aiohttp.ClientSession() <span class="hljs-keyword">as</span> session:<br>        <span class="hljs-comment">#同理get()、 post()：参数如下：</span><br>        <span class="hljs-comment">#headers、params/data、proxy=&quot;http://ip:port&quot;</span><br>        <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> <span class="hljs-keyword">await</span> session.get(url=url,headers=headers) <span class="hljs-keyword">as</span> response:<br>            <span class="hljs-comment">#text()返回字符串形式的响应数据</span><br>            <span class="hljs-comment">#read()返回二进制形式的响应数据</span><br>            <span class="hljs-comment">#json()返回json()对象</span><br>            <span class="hljs-comment">#注意：获取响应数据操作之前，一定要使用await关键字手动挂起</span><br>            page_text = <span class="hljs-keyword">await</span> response.text()<br>            <span class="hljs-built_in">print</span>(page_text)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    start = time.time()<br>    urls = [<br>        <span class="hljs-string">&quot;http://127.0.0.1:5000/1&quot;</span>,<span class="hljs-string">&quot;http://127.0.0.1:5000/2&quot;</span>,<span class="hljs-string">&quot;http://127.0.0.1:5000/3&quot;</span><br>    ]<br>    tasks = [ get_page(url) <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls]<br>    asyncio.run(asyncio.wait(tasks))<br>    end = time.time()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;use time seconds:&quot;</span>,end-start)<br><br><span class="hljs-comment">#1</span><br><span class="hljs-comment">#3</span><br><span class="hljs-comment">#2</span><br><span class="hljs-comment">#use time seconds: 2.010725975036621</span><br></code></pre></td></tr></table></figure></li>
</ol>
<h2 id="Selenium模块"><a href="#Selenium模块" class="headerlink" title="Selenium模块"></a>Selenium模块</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>Selenium是基于浏览器自动化的一个模块。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。通过编写Selenium代码模拟用户的浏览访问行为，让代码对应的行为动作在浏览器中触发，使得浏览器完成相关自动化的操作。</p>
<h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><ul>
<li>便捷地获取网站中动态加载的数据</li>
<li>便捷地实现模拟登录</li>
</ul>
<h3 id="使用流程-1"><a href="#使用流程-1" class="headerlink" title="使用流程"></a>使用流程</h3><h4 id="环境安装-2"><a href="#环境安装-2" class="headerlink" title="环境安装"></a>环境安装</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip3 install selenium<br></code></pre></td></tr></table></figure>

<h4 id="下载浏览器驱动程序"><a href="#下载浏览器驱动程序" class="headerlink" title="下载浏览器驱动程序"></a>下载浏览器驱动程序</h4><p>在编写程序时，我们需要调用浏览器的驱动来完成自动化的操作。不同的浏览器有不用的驱动程序。</p>
<h5 id="谷歌浏览器"><a href="#谷歌浏览器" class="headerlink" title="谷歌浏览器"></a>谷歌浏览器</h5><p>下载地址：<a target="_blank" rel="noopener" href="http://chromedriver.storage.googleapis.com/index.html">http://chromedriver.storage.googleapis.com/index.html</a></p>
<h5 id="驱动与浏览器版本对应表"><a href="#驱动与浏览器版本对应表" class="headerlink" title="驱动与浏览器版本对应表"></a>驱动与浏览器版本对应表</h5><p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv26ssxyjpj60ow129abr02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>我的谷歌浏览器版本如下：</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv26vq3kppj611u0cuaat02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>找到对应的版本驱动，下载到本地即可。</p>
<p>同理，其他浏览器以此类推，这里不再赘述，地址如下：</p>
<p>IE浏览器：<a target="_blank" rel="noopener" href="http://selenium-release.storage.googleapis.com/index.html">http://selenium-release.storage.googleapis.com/index.html</a></p>
<p>火狐浏览器：<a target="_blank" rel="noopener" href="https://github.com/mozilla/geckodriver/releases/">https://github.com/mozilla/geckodriver/releases/</a></p>
<p>Safari浏览器自带驱动，使用方法如下：</p>
<ol>
<li><p>在偏好设置栏中–&gt;开发–&gt;勾选“允许远程自动化”</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv27a0nkysj60u00xnq5x02.jpg" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p>在/usr/bin/文件路径下找到safaridriver驱动，双击启动即可（首次需要启动，以后不需要）</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv27db6os1j61ey0oc0xs02.jpg" srcset="/img/loading.gif" lazyload></p>
</li>
</ol>
<h4 id="实例化一个浏览器对象"><a href="#实例化一个浏览器对象" class="headerlink" title="实例化一个浏览器对象"></a>实例化一个浏览器对象</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#实例化一个浏览器对象（导入浏览器的驱动）bro，并启动对应的浏览器</span><br>bro = webdriver.Chrome(executable_path=<span class="hljs-string">&quot;./chromedriver&quot;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="编写基于浏览器自动化操作的代码"><a href="#编写基于浏览器自动化操作的代码" class="headerlink" title="编写基于浏览器自动化操作的代码"></a>编写基于浏览器自动化操作的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> etree<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment">#实例化一个浏览器对象（导入浏览器的驱动），并启动对应的浏览器</span><br>bro = webdriver.Chrome(executable_path=<span class="hljs-string">&quot;./chromedriver&quot;</span>)<br><br><span class="hljs-comment">#让浏览器对象发起一个指定url的get请求</span><br>bro.get(<span class="hljs-string">&quot;http://scxk.nmpa.gov.cn:81/xk/&quot;</span>)<br><br><span class="hljs-comment">#获取浏览器对象当前的页面源码数据</span><br>page_text = bro.page_source<br><br><span class="hljs-comment">#解析企业名称</span><br>tree = etree.HTML(page_text)<br><br>li_list = tree.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;gzlist&quot;]/li&#x27;</span>)<br><span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>    name = li.xpath(<span class="hljs-string">&#x27;./dl/@title&#x27;</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-built_in">print</span>(name)<br><br>time.sleep(<span class="hljs-number">5</span>)<br><span class="hljs-comment">#关闭浏览器对象</span><br>bro.quit()<br></code></pre></td></tr></table></figure>

<h3 id="常用语法"><a href="#常用语法" class="headerlink" title="常用语法"></a>常用语法</h3><ol>
<li>基本语法</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#导入包</span><br><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><br><span class="hljs-comment">#创建浏览器对象bro，并启动浏览器bro</span><br>bro = webdriver.Chrome(executable_path=<span class="hljs-string">&quot;./chromedriver&quot;</span>)<br><span class="hljs-comment">#这里只是以谷歌浏览器为例，还有其他浏览器：火狐，IE，Safari等</span><br><span class="hljs-comment">#参数executable_path是对应浏览器的驱动存储路径</span><br><br><span class="hljs-comment">#浏览器bro发送get请求</span><br>bro.get(url)<br><br><span class="hljs-comment">#浏览器发送请求后，获取当前页面的源码数据</span><br>page_text = bro.page_source<br><br><span class="hljs-comment">#浏览器发送请求后，可以使用find_element_by_xx()函数进行标签定位</span><br>search_input = bro.find_element_by_id(<span class="hljs-string">&quot;id标签的值&quot;</span>)<br><span class="hljs-comment">#xx可以是id、class、xpath等</span><br><span class="hljs-comment">#标签交互，使用send_keys(&quot;xxx&quot;)为输入标签传入值xxx</span><br>search_input.send_keys(<span class="hljs-string">&quot;xxx&quot;</span>)<br><br><span class="hljs-comment">#浏览器发送请求后，标签定位到搜索按钮并点击</span><br>btn = bro.find_element_by_css_selector(<span class="hljs-string">&quot;.btn-search&quot;</span>)<br>btn.click()<br><br><span class="hljs-comment">#执行JS命令语句，如window.scrollTo(0,document.body.scrollHeight)</span><br>bro.execute_script(<span class="hljs-string">&quot;window.scrollTo(0,document.body.scrollHeight)&quot;</span>)<br><span class="hljs-comment">#浏览器屏幕向下滚动一个屏幕高度</span><br><br><span class="hljs-comment">#返回上一页</span><br>bro.back()<br><span class="hljs-comment">#前进下一页</span><br>bro.forward()<br><br><span class="hljs-comment">#退出浏览器</span><br>bro.quit()<br></code></pre></td></tr></table></figure>

<ol start="2">
<li>iframe+动作链</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">iframe是网页页面嵌套的标签，我们在标签定位时，需要确定标签在具体的哪个页面。如果目标标签在嵌套页面iframe之中的话，则需要浏览器切换标签定位的作用域，在嵌套页面iframe中进行定位查找。</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">#创建浏览器对象</span><br>bro = webdriver.Chrome(executable_path=driver_url)<br><span class="hljs-comment">#浏览器发送请求</span><br>bro.get(url)<br><br><span class="hljs-comment">#浏览器使用switch_to.frame()方法定位切换到iframe</span><br>bro.switch_to.frame(iframe_id)<br>div = bro.find_element_by_id(div_id)<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">当网页中的验证要求拖拽，长按等连续的动作时，我们称之为动作链。动作链对应的类为ActionChains，它在selenium.webdriver之中</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ActionChains<br><br><span class="hljs-comment">#实例化动作链对象</span><br>action = ActionChains(bro)<br><span class="hljs-comment">#调用点击且长按的函数</span><br>action.click_and_hold(div)<br><span class="hljs-comment">#移动偏移函数move_by_offset(x,y)</span><br>action.move_by_offset()<br><span class="hljs-comment">#动作执行函数perform(x,y)立即执行动作</span><br><span class="hljs-comment">#x是水平方向，y是竖直方向</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>  action.move_by_offset(x,y).perform()<span class="hljs-comment">#执行5次动作</span><br>  <br><span class="hljs-comment">#释放动作链</span><br>action.release()<br></code></pre></td></tr></table></figure>

<ol start="3">
<li>无头浏览器与规避检测</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">当客户端在执行程序爬取数据时，我们不希望客户端会弹出浏览器，而是让程序默默隐藏执行即可。即浏览器无可视化界面，此时，我们称为无头浏览器。</span><br><span class="hljs-string"></span><br><span class="hljs-string">声明无头浏览器时，需要导入相应的包(以谷歌浏览器为例)：</span><br><span class="hljs-string">from selenium.webdriver.chrome.options import Options</span><br><span class="hljs-string">创建无头对象以及设置参数：</span><br><span class="hljs-string">chrome_options = Options()</span><br><span class="hljs-string">chrome_options.add_argument(&quot;--headless&quot;)</span><br><span class="hljs-string">chrome_options.add_argument(&quot;--disable-gpu&quot;)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">#可以视为固定写法搭配，不用特别理解</span><br><span class="hljs-keyword">from</span> selenium.webdriver.chrome.options <span class="hljs-keyword">import</span> Options<br>chrome_options = Options()<br>chrome_options.add_argument(<span class="hljs-string">&quot;--headless&quot;</span>)<br>chrome_options.add_argument(<span class="hljs-string">&quot;--disable-gpu&quot;</span>)<br><br><span class="hljs-comment">#实例化浏览器对象bro，加入无头参数chrome_options</span><br>bro = webdriver.Chrome(executable_path=<span class="hljs-string">&quot;./chromedriver&quot;</span>,chrome_options=chrome_options)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">有些门户网站对于selenium的爬取会有检测技术，类似于前面讲到的UA检测，因此，我们需要进行规避检测。</span><br><span class="hljs-string">导入规避检测的包:</span><br><span class="hljs-string">from selenuim.webdriver import ChromeOptions</span><br><span class="hljs-string">创建规避检测的对象以及设置参数：</span><br><span class="hljs-string">option = ChromeOptions()</span><br><span class="hljs-string">chrome_options.add_argement(&quot;excludeSwitches&quot;,[&quot;enable-automation&quot;])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-keyword">from</span> selenuim.webdriver <span class="hljs-keyword">import</span> ChromeOptions<br>option = ChromeOptions()<br>option.add_experimental_option(<span class="hljs-string">&quot;excludeSwitches&quot;</span>,[<span class="hljs-string">&quot;enable-automation&quot;</span>])<br><br>bro = webdriver.Chrome(executable_path=<span class="hljs-string">&quot;./chromedriver&quot;</span>,chrome_options=chrome_options,options=option)<br></code></pre></td></tr></table></figure>



<h3 id="实战案例-6"><a href="#实战案例-6" class="headerlink" title="实战案例"></a>实战案例</h3><h4 id="淘宝搜索框"><a href="#淘宝搜索框" class="headerlink" title="淘宝搜索框"></a>淘宝搜索框</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">import</span> time<br><span class="hljs-comment">#创建浏览器对象，并启动浏览器</span><br>bro = webdriver.Chrome(executable_path=<span class="hljs-string">&quot;./chromedriver&quot;</span>)<br><br><span class="hljs-comment">#浏览器发送请求</span><br>bro.get(<span class="hljs-string">&quot;https://www.taobao.com&quot;</span>)<br><br><span class="hljs-comment">#定位搜索框的标签位置</span><br>search_input = bro.find_element_by_id(<span class="hljs-string">&#x27;q&#x27;</span>)<br>search_input.send_keys(<span class="hljs-string">&quot;iPad&quot;</span>)<br><span class="hljs-comment">#定位到搜索按钮</span><br>btn = bro.find_element_by_class_name(<span class="hljs-string">&quot;btn-search&quot;</span>)<br><span class="hljs-comment">#btn = bro.find_element_by_css_selector(&quot;.btn-search&quot;)</span><br>btn.click()<br><br>time.sleep(<span class="hljs-number">2</span>)<br>bro.execute_script(<span class="hljs-string">&quot;window.scrollTo(0,document.body.scrollHeight)&quot;</span>)<br><br>bro.quit()<br></code></pre></td></tr></table></figure>

<h4 id="模拟登录LeanCloud"><a href="#模拟登录LeanCloud" class="headerlink" title="模拟登录LeanCloud"></a>模拟登录LeanCloud</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#进入该网址：https://console.leancloud.cn/login</span><br><span class="hljs-comment">#注册账号不需要验证，填写用户名和密码，点击登录即可</span><br><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">import</span> time<br><br>bro = webdriver.Chrome(<span class="hljs-string">&quot;./chromedriver&quot;</span>)<br>bro.get(<span class="hljs-string">&quot;https://console.leancloud.cn/login&quot;</span>)<br><br>username = bro.find_element_by_id(<span class="hljs-string">&quot;email&quot;</span>)<br>username.send_keys(<span class="hljs-string">&quot;344771599@qq.com&quot;</span>)<br>time.sleep(<span class="hljs-number">0.5</span>)<br><br>password = bro.find_element_by_id(<span class="hljs-string">&quot;password&quot;</span>)<br>password.send_keys(<span class="hljs-string">&quot;XXXX&quot;</span>)<br>time.sleep(<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment">#class=&quot;ui primary button&quot;，选择空格中的一个即可，这里选择&quot;button&quot;</span><br>btn = bro.find_element_by_class_name(<span class="hljs-string">&quot;button&quot;</span>)<br>btn.click()<br>time.sleep(<span class="hljs-number">1</span>)<br><br>bro.quit()<br></code></pre></td></tr></table></figure>

<h4 id="模拟登录QQ空间"><a href="#模拟登录QQ空间" class="headerlink" title="模拟登录QQ空间"></a>模拟登录QQ空间</h4><p>加入<code>bro.switch_to.frame(&quot;login_frame&quot;)</code>切换到iframe进行定位</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">import</span> time<br><br>bro = webdriver.Chrome(<span class="hljs-string">&quot;./chromedriver&quot;</span>)<br>bro.get(<span class="hljs-string">&quot;https://qzone.qq.com&quot;</span>)<br><br>time.sleep(<span class="hljs-number">2</span>)<br>bro.switch_to.frame(<span class="hljs-string">&quot;login_frame&quot;</span>)<br><br>a_tag = bro.find_element_by_id(<span class="hljs-string">&quot;switcher_plogin&quot;</span>)<br>a_tag.click()<br>time.sleep(<span class="hljs-number">1</span>)<br><br>username = bro.find_element_by_id(<span class="hljs-string">&quot;u&quot;</span>)<br>username.send_keys(<span class="hljs-string">&quot;344771599&quot;</span>)<br>time.sleep(<span class="hljs-number">1</span>)<br><br>password = bro.find_element_by_id(<span class="hljs-string">&quot;p&quot;</span>)<br>password.send_keys(<span class="hljs-string">&quot;mypassword&quot;</span>)<br>time.sleep(<span class="hljs-number">1</span>)<br><br>btn = bro.find_element_by_id(<span class="hljs-string">&quot;login_button&quot;</span>)<br>btn.click()<br>time.sleep(<span class="hljs-number">2</span>)<br><br>bro.quit()<br></code></pre></td></tr></table></figure>

<h4 id="登录12306（拖动滑块版本）"><a href="#登录12306（拖动滑块版本）" class="headerlink" title="登录12306（拖动滑块版本）"></a>登录12306（拖动滑块版本）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> selenium.webdriver <span class="hljs-keyword">import</span> ActionChains<br><span class="hljs-comment"># # 无头浏览器以及规避检测</span><br><span class="hljs-comment"># from selenium.webdriver.chrome.options import Options</span><br><span class="hljs-comment"># from selenium.webdriver import ChromeOptions</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># chrome_options = Options()</span><br><span class="hljs-comment"># chrome_options.add_argument(&quot;--headless&quot;)</span><br><span class="hljs-comment"># chrome_options.add_argument(&quot;--disable-gpu&quot;)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># option = ChromeOptions()</span><br><span class="hljs-comment"># option.add_experimental_option(&quot;excludeSwitches&quot;,[&quot;enable-automation&quot;])</span><br><br>bro = webdriver.Chrome(executable_path=<span class="hljs-string">&quot;./chromedriver&quot;</span>)<br>bro.get(<span class="hljs-string">&quot;https://kyfw.12306.cn/otn/resources/login.html&quot;</span>)<br><br>a_tag = bro.find_element_by_class_name(<span class="hljs-string">&quot;login-hd-account&quot;</span>)<br>time.sleep(<span class="hljs-number">1</span>)<br>a_tag.click()<br><br>username = bro.find_element_by_id(<span class="hljs-string">&quot;J-userName&quot;</span>)<br>username.send_keys(<span class="hljs-string">&quot;13866699573&quot;</span>)<br>time.sleep(<span class="hljs-number">2</span>)<br><br>password = bro.find_element_by_id(<span class="hljs-string">&quot;J-password&quot;</span>)<br>password.send_keys(<span class="hljs-string">&quot;Mypassword0918&quot;</span>)<br>time.sleep(<span class="hljs-number">2</span>)<br><br>btn = bro.find_element_by_id(<span class="hljs-string">&quot;J-login&quot;</span>)<br>btn.click()<br>time.sleep(<span class="hljs-number">2</span>)<br><br>push = bro.find_element_by_id(<span class="hljs-string">&quot;nc_1__bg&quot;</span>)<br>action = ActionChains(bro)<br>action.click_and_hold(push)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    action.move_by_offset(<span class="hljs-number">100</span>,<span class="hljs-number">0</span>).perform()<br><br>action.release()<br>bro.quit()<br></code></pre></td></tr></table></figure>

<h2 id="Scrapy框架"><a href="#Scrapy框架" class="headerlink" title="Scrapy框架"></a>Scrapy框架</h2><p>框架：<strong>集成了很多功能</strong>且具有很<strong>强通用性</strong>的项目模板，即项目的半成品。</p>
<p>scrapy：爬虫中封装好的框架，使用广泛。优点主要有：<strong>高性能的持久化存储、异步的数据下载、高性能的数据解析、分布式</strong>等。具体如下：</p>
<ul>
<li>Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛。</li>
<li>框架的力量，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。</li>
<li>Scrapy 使用了 Twisted<code>[&#39;twɪstɪd]</code>(其主要对手是Tornado)异步网络框架来处理网络通讯，可以加快下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。</li>
</ul>
<h3 id="scrapy架构图"><a href="#scrapy架构图" class="headerlink" title="scrapy架构图"></a>scrapy架构图</h3><p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6var2nydj60jg0dqjsf02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>tip:绿线表示<strong>数据流向</strong>。</p>
<ul>
<li><code>Scrapy Engine(引擎)</code>: 负责<code>Spider</code>、<code>ItemPipeline</code>、<code>Downloader</code>、<code>Scheduler</code>中间的通讯，信号、数据传递等。</li>
<li><code>Scheduler(调度器)</code>: 它负责接受<code>引擎</code>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<code>引擎</code>需要时，交还给<code>引擎</code>。</li>
<li><code>Downloader（下载器）</code>：负责下载<code>Scrapy Engine(引擎)</code>发送的所有Requests请求，并将其获取到的Responses交还给<code>Scrapy Engine(引擎)</code>，由<code>引擎</code>交给<code>Spider</code>来处理，</li>
<li><code>Spider（爬虫）</code>：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给<code>引擎</code>，再次进入<code>Scheduler(调度器)</code>，</li>
<li><code>Item Pipeline(管道)</code>：它负责处理<code>Spider</code>中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</li>
<li><code>Downloader Middlewares（下载中间件）</code>：你可以当作是一个可以自定义扩展下载功能的组件。</li>
<li><code>Spider Middlewares（Spider中间件）</code>：你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<code>通信</code>的功能组件（比如进入<code>Spider</code>的Responses和从<code>Spider</code>出去的Requests）</li>
</ul>
<h3 id="Scrapy的运作流程"><a href="#Scrapy的运作流程" class="headerlink" title="Scrapy的运作流程"></a>Scrapy的运作流程</h3><p>代码写好，程序开始运行…</p>
<ol>
<li><code>引擎</code>：Hi！<code>Spider</code>, 你要处理哪一个网站？</li>
<li><code>Spider</code>：主人要我处理xxxx.com。</li>
<li><code>引擎</code>：你把第一个需要处理的URL给我吧。</li>
<li><code>Spider</code>：给你，第一个URL是xxxxxxx.com。</li>
<li><code>引擎</code>：Hi！<code>调度器</code>，我这有request请求你帮我排序入队一下。</li>
<li><code>调度器</code>：好的，正在处理你等一下。</li>
<li><code>引擎</code>：Hi！<code>调度器</code>，把你处理好的request请求给我。</li>
<li><code>调度器</code>：给你，这是我处理好的request</li>
<li><code>引擎</code>：Hi！下载器，你按照主人的<code>下载中间件</code>的设置帮我下载一下这个request请求</li>
<li><code>下载器</code>：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载）</li>
<li><code>引擎</code>：Hi！<code>Spider</code>，这是下载好的东西，并且已经按照主人的<code>下载中间件</code>处理过了，你自己处理一下（注意！这儿responses默认是交给<code>def parse()</code>这个函数处理的）</li>
<li><code>Spider</code>：（处理完毕数据之后对于需要跟进的URL），Hi！<code>引擎</code>，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</li>
<li><code>引擎</code>：Hi ！<code>管道</code> 我这儿有个item你帮我处理一下！<code>调度器</code>！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完主人需要全部信息。</li>
<li><code>管道、调度器</code>：好的，现在就做！</li>
</ol>
<p>注意！只有当<code>调度器</code>中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）</p>
<h3 id="制作-Scrapy-爬虫的步骤"><a href="#制作-Scrapy-爬虫的步骤" class="headerlink" title="制作 Scrapy 爬虫的步骤"></a>制作 Scrapy 爬虫的步骤</h3><ul>
<li>新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目</li>
<li>明确目标 （编写items.py）：明确你想要抓取的目标</li>
<li>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</li>
<li>存储内容 （pipelines.py）：设计管道存储爬取内容</li>
</ul>
<h3 id="Scrapy环境安装"><a href="#Scrapy环境安装" class="headerlink" title="Scrapy环境安装"></a>Scrapy环境安装</h3><p>Mac或者Linux：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip3 install scrapy<br></code></pre></td></tr></table></figure>

<p>Windows：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#安装wheel</span><br>pip3 install wheel<br><br><span class="hljs-comment">#下载twisted</span><br>http://www.lfd.uci.edu/~gohlke/pythonlibs/<span class="hljs-comment">#twisted</span><br><br><span class="hljs-comment">#安装twisted</span><br>pip3 install Twisted-<span class="hljs-number">17.1</span><span class="hljs-number">.0</span>-cp36-cp36m-win_amd64.whl<br><br><span class="hljs-comment">#安装pywin32</span><br>pip3 install win32<br><br><span class="hljs-comment">#安装scrapy</span><br>pip3 install scrapy<br></code></pre></td></tr></table></figure>

<h3 id="Scrapy的基本使用"><a href="#Scrapy的基本使用" class="headerlink" title="Scrapy的基本使用"></a>Scrapy的基本使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#命令行创建工程</span><br>scrapy startproject xxxPro<br><br><span class="hljs-comment">#如：创建工程firstBlood</span><br>scrapy startproject firstBlood<br></code></pre></td></tr></table></figure>

<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6prvw9t7j60k80ciaam02.jpg" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#在xxxPro文件下的spiders目录中创建一个爬虫文件spiderName</span><br>cd xxxPro<br>scrapy genspider spiderName www.xxx.com<br><br><span class="hljs-comment">#如：在firstBlood文件下的spiders目录中创建一个爬虫文件first</span><br>cd firstBlood<br>scrapy genspiders first www.xxx.com<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FirstSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    <span class="hljs-comment">#爬虫文件的名称：爬虫源文件的唯一标识，用来指定特定的爬虫文件</span><br>    name = <span class="hljs-string">&#x27;first&#x27;</span><br>    <span class="hljs-comment">#允许的域名：允许爬行的域名字符串的列表。【该参数可选】（如果启用OffItemIDdleware，则不会跟踪不属于此列表中指定的域名（或其子域）的URL请求。）</span><br>    <span class="hljs-comment">#假设目标url是https://www.example.com/1.html，然后需要将“example.com”添加到列表中。</span><br>    allowed_domains = [<span class="hljs-string">&#x27;www.xxx.com&#x27;</span>]<br>    <span class="hljs-comment">#起始的URL列表：当未指定特定URL时，该列表中的地址会被scrapy自动进行请求发送，后续请求将从起始URL中包含的数据连续生成。</span><br>    start_urls = [<span class="hljs-string">&#x27;http://www.xxx.com/&#x27;</span>]<br><br>    <span class="hljs-comment">#用于数据解析：response参数表示的就是请求成功后对应的响应对象</span><br>    <span class="hljs-comment">#执行的次数为 len(start_urls)</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        <span class="hljs-built_in">print</span>(response)<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#执行工程爬虫文件</span><br>scrapy crawl spiderName<br><br><span class="hljs-comment">#如：执行工程爬虫文件first</span><br>scrapy crawl first<br></code></pre></td></tr></table></figure>

<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6zonz4qwj61440a275n02.jpg" srcset="/img/loading.gif" lazyload></p>
<p>在setting文件中，ROBOTSTXT_OBEY设置为False</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#执行工程爬虫文件，命令行不显示日志信息</span><br>scrapy crawl spiderName --nolog<br><br><span class="hljs-comment">#如：执行工程爬虫文件first，命令行不显示日志信息</span><br>scrapy crawl first --nolog<br></code></pre></td></tr></table></figure>

<p>但是报错日志也会被隐藏，我们希望除了错误日志，其他日志不显示，需要在setting文件中添加如下字段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#显示指定类型的日志信息</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br></code></pre></td></tr></table></figure>

<h3 id="Scrapy数据解析"><a href="#Scrapy数据解析" class="headerlink" title="Scrapy数据解析"></a>Scrapy数据解析</h3><p>在scrapy中也有Selectors选择器，有3个定位网页标签的基本方法：</p>
<ol>
<li>xpath()：传入xpath表达式，返回该表达式所对应的所有节点的selector list列表数据类型，该数据类型的数据存储在列表元素的”data”字段对应的内容。</li>
<li>css()：传入CSS表达式，返回该表达式所对应的所有节点的selector list列表数据类型，语法同 BeautifulSoup4。</li>
<li>re()： 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表数据类型。</li>
</ol>
<p>最常用的标签定位方法依然是<code>xpath()</code>方法，返回<code>selector list列表数据类型</code>，同时需要用到<code>extract()</code>方法提取出<strong>selector list列表数据类型中的数据</strong>。即：</p>
<p>extract()：将selector list列表数据类型中的<strong>数据提取出来</strong>后，<strong>返回字符串数据类型</strong>。</p>
<h4 id="实战案例-7"><a href="#实战案例-7" class="headerlink" title="实战案例"></a>实战案例</h4><ol>
<li>需求：爬取糗事百科的文本笑话的<strong>作者</strong>和<strong>内容</strong></li>
<li>网址：<a target="_blank" rel="noopener" href="https://www.qiushibaike.com/text/">https://www.qiushibaike.com/text/</a></li>
<li>说明：</li>
</ol>
<p>第一步：创建项目</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy startproject qiubaiPro<br></code></pre></td></tr></table></figure>

<p>第二步：修改setting文件</p>
<p>在qiubaiPro项目的spiders下setting文件中的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br><span class="hljs-comment">#USER_AGENT = &#x27;qiubaiPro (+http://www.yourdomain.com)&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>

<p>修改为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment">#只显示指定类型的日志信息（额外添加）</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br></code></pre></td></tr></table></figure>

<p>第三步：在qiubaiPro项目下，创建爬虫qiubai文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">cd qiubaiPro<br>scrapy genspider qiubai www.xxx.com<br></code></pre></td></tr></table></figure>

<p>第四步：修改qiubai.py文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1.</span>注释（或者删除）allowed_domains语句<br><span class="hljs-number">2.</span>将start_urls语句修改为<br>    start_urls = [<span class="hljs-string">&#x27;https://www.qiushibaike.com/text/&#x27;</span>]<br><span class="hljs-number">3.</span>编写<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self,response</span>)函数</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>    div_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/div/div[2]/div&#x27;</span>)<br>    <span class="hljs-comment">#print(div_list)</span><br>    <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>        author = div.xpath(<span class="hljs-string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment">#文本内容在网页中，被多个标签分段，xpath解析时，会根据标签节点返回多个列表，同时自然有多个&quot;data&quot;</span><br>        <span class="hljs-comment">#extract()在提取&quot;data&quot;返回列表时，会得到多个元素</span><br>        content = div.xpath(<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(author,content)<br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure>

<p>文本内容在网页中，被多个标签分段，xpath解析时，会根据标签节点返回多个列表，同时自然有多个”data”，extract()在提取”data”返回列表时，会得到多个元素。使用命令行<code>scrapy crawl qiubai</code>执行爬虫时，得到如下结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">狐小乔<br> [&lt;Selector xpath=<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span> data=<span class="hljs-string">&#x27;\n\n\n表哥表嫂一直想生二胎，侄儿不同意。&#x27;</span>&gt;, &lt;Selector xpath=<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span> data=<span class="hljs-string">&#x27;表哥说：“等有了弟弟或者妹妹，or xpath=&#x27;</span>./a[<span class="hljs-number">1</span>]/div/span//text()<span class="hljs-string">&#x27; data=&#x27;</span>小家伙想了想说：“那他得先交保.护.费才行！”<span class="hljs-string">&#x27;&gt;, &lt;Selector xpath=&#x27;</span>./a[<span class="hljs-number">1</span>]/div/span//text()<span class="hljs-string">&#x27; data=&#x27;</span>表嫂：“将来我和你爸老了，不用你一个xpath=<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span> data=<span class="hljs-string">&#x27;小侄歪着头想了想说：“那你们生个双胞胎吧，他们两个一人一半，我就不用出了，...&#x27;</span>&gt;, &lt;Selector xpath=<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span> data=<span class="hljs-string">&#x27;我和我的小伙伴当时就惊呆了……\n\n&#x27;</span>&gt;]<br>(venv) lee@leedeMacBook-Pro qiubaiPro % <br><br></code></pre></td></tr></table></figure>

<p>因此，需要将<code>content = div.xpath(&#39;./a[1]/div/span//text()&#39;)</code>加上<code>extract()</code>方法，提取出所有的”data”文本数据，返回多个元素的列表，再使用列表元素拼接函数<code>join()</code>得到完整文本。对应修改<code>def parse(self,response)</code>函数如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>    div_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/div/div[2]/div&#x27;</span>)<br>    <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>        author = div.xpath(<span class="hljs-string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment">#文本内容在网页中，被多个标签分段，xpath解析时，会根据标签节点返回多个列表，同时自然有多个&quot;data&quot;</span><br>        <span class="hljs-comment">#extract()在提取&quot;data&quot;返回列表时，会得到多个元素</span><br>        content = div.xpath(<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()<br>        content = <span class="hljs-string">&quot;&quot;</span>.join(content)<br>        <span class="hljs-built_in">print</span>(author,content)<br></code></pre></td></tr></table></figure>

<p>第四步：执行爬虫文件qiubai</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy crawl qiubai<br></code></pre></td></tr></table></figure>

<h3 id="Scrapy持久化存储"><a href="#Scrapy持久化存储" class="headerlink" title="Scrapy持久化存储"></a>Scrapy持久化存储</h3><p>基于终端指令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QiubaiSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;qiubai&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;https://www.qiushibaike.com/text/&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://www.qiushibaike.com/text/&#x27;</span>]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        div_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/div/div[2]/div&#x27;</span>)<br><br>        all_data= []<br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>            author = div.xpath(<span class="hljs-string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            <span class="hljs-comment">#文本内容在网页中，被多个标签分段，xpath解析时，会根据标签节点返回多个列表，同时自然有多个&quot;data&quot;</span><br>            <span class="hljs-comment">#extract()在提取&quot;data&quot;返回列表时，会得到多个元素</span><br>            content = div.xpath(<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()<br>            content = <span class="hljs-string">&quot;&quot;</span>.join(content)<br>            <span class="hljs-built_in">dict</span> = &#123;<br>                <span class="hljs-string">&quot;author&quot;</span>:author,<br>                <span class="hljs-string">&quot;content&quot;</span>:content<br>            &#125;<br>            all_data.append(<span class="hljs-built_in">dict</span>)<br>        <span class="hljs-keyword">return</span> all_data<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#只可以将parse()方法的返回值存储到本地的文本文件中，命令如下：</span><br>scrapy crawl qiubai -o ./qiubai.csv<br><span class="hljs-comment">#需要注意的是，文本类型只能是</span><br><span class="hljs-comment">#[&quot;json&quot;、&quot;jsonlines&quot;、&quot;jl&quot;、&quot;csv&quot;、&quot;xml&quot;、&quot;marshal&quot;、&quot;pickle&quot;]</span><br><span class="hljs-comment">#之中的类型，其他文本类型（如&quot;txt&quot;）会报错。</span><br><span class="hljs-comment">#终端指令格式：</span><br>scrapy crawl 爬虫文件 -o 存储路径<br></code></pre></td></tr></table></figure>

<p>基于管道：</p>
<p>基于管道的编码流程共有6步：</p>
<p>第一步：数据解析，在爬虫文件中的parse()函数里解析response</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>    div_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/div/div[2]/div&#x27;</span>)<br><br>    all_data= []<br>    <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>        author = div.xpath(<span class="hljs-string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment">#文本内容在网页中，被多个标签分段，xpath解析时，会根据标签节点返回多个列表，同时自然有多个&quot;data&quot;</span><br>        <span class="hljs-comment">#extract()在提取&quot;data&quot;返回列表时，会得到多个元素</span><br>        content = div.xpath(<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()<br>        content = <span class="hljs-string">&quot;&quot;</span>.join(content)<br></code></pre></td></tr></table></figure>



<p> 第二步：在item类中定义相关的属性，即在<code>items.py</code>文件中定义实例化<code>author</code>和<code>content</code>对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QiubaiproItem</span>(<span class="hljs-params">scrapy.Item</span>):</span><br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    author = scrapy.Field()<br>    content = scrapy.Filed()<br></code></pre></td></tr></table></figure>

<p>第三步：将解析的数据封装并存储到item类型的对象中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">from</span> scrapy.qiubaiPro.qiubaiPro.items <span class="hljs-keyword">import</span> QiubaiproItem<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QiubaiSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;qiubai&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;https://www.qiushibaike.com/text/&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://www.qiushibaike.com/text/&#x27;</span>]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        div_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/div/div[2]/div&#x27;</span>)<br><br>        all_data= []<br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>            author = div.xpath(<span class="hljs-string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            <span class="hljs-comment">#文本内容在网页中，被多个标签分段，xpath解析时，会根据标签节点返回多个列表，同时自然有多个&quot;data&quot;</span><br>            <span class="hljs-comment">#extract()在提取&quot;data&quot;返回列表时，会得到多个元素</span><br>            content = div.xpath(<span class="hljs-string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()<br>            content = <span class="hljs-string">&quot;&quot;</span>.join(content)<br><br>            <span class="hljs-comment">#实例化item()对象</span><br>            item = QiubaiproItem()<br>            <span class="hljs-comment">#将author内容存储在item[&quot;author&quot;]中</span><br>            item[<span class="hljs-string">&quot;author&quot;</span>] = author<br>            <span class="hljs-comment">#将content内容存储在item[&quot;cintent&quot;]中</span><br>            item[<span class="hljs-string">&quot;content&quot;</span>] = content<br><br>            <span class="hljs-comment">#将item提交给管道</span><br>            <span class="hljs-keyword">yield</span> item<br></code></pre></td></tr></table></figure>

<p>第四步：将item类型的对象提交给管道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#导入items类中QiubaiproItem类实例化item对象</span><br><span class="hljs-comment">#将数据存入item对象中</span><br><span class="hljs-comment">#使用关键字yield将item对象提交给pipeslines文件的管道process_item()函数</span><br><span class="hljs-keyword">yield</span> item<br></code></pre></td></tr></table></figure>

<p>第五步：在pipelines管道类文件的process_item()函数中，要将其接收到的item对象中存储的数据进行持久化存储</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QiubaiproPipeline</span>:</span><br>    fp = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#重写父类的一个方法：该方法只在开始爬虫时被调用一次</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">open_spider</span>(<span class="hljs-params">self,spider</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;开始爬虫......&quot;</span>)<br>        self.fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./qiubai.txt&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br><br>    <span class="hljs-comment">#专门用来处理item类型对象</span><br>    <span class="hljs-comment">#该方法可以接收爬虫文件提交过来的item对象</span><br>    <span class="hljs-comment">#每当爬虫文件提交一次item时，该方法就会接收到一个item，就会被调用一次</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span>(<span class="hljs-params">self, item, spider</span>):</span><br>        author = item[<span class="hljs-string">&quot;author&quot;</span>]<br>        content = item[<span class="hljs-string">&quot;content&quot;</span>]<br>        self.fp.write(author+<span class="hljs-string">&quot;:&quot;</span>+content+<span class="hljs-string">&quot;\n&quot;</span>)<br>        <span class="hljs-keyword">return</span> item<br><br>    <span class="hljs-comment">#只在结束时被调用一次</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">close_spider</span>(<span class="hljs-params">self,spider</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;结束爬虫！&quot;</span>)<br>        self.fp.close()<br></code></pre></td></tr></table></figure>



<p>第六步：在setting配置文件中开启管道</p>
<p>在setting文件中的第68-70行pipelines被注释了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#ITEM_PIPELINES = &#123;</span><br><span class="hljs-comment">#    &#x27;qiubaiPro.pipelines.QiubaiproPipeline&#x27;: 300,</span><br><span class="hljs-comment">#&#125;</span><br></code></pre></td></tr></table></figure>

<p>取消注释即可开启管道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">ITEM_PIPELINES = &#123;<br>    <span class="hljs-string">&#x27;qiubaiPro.pipelines.QiubaiproPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br><span class="hljs-comment">#300表示优先级，数值越小优先级越高，当前类越先被执行</span><br><span class="hljs-comment">#如果字典ITEM_PIPELINES中存在多个类，则数值小优先级高的先执行</span><br></code></pre></td></tr></table></figure>

<p>执行爬虫文件即可：<code>scrapy crawl qiubai</code></p>
<h3 id="Scrapy多份存储"><a href="#Scrapy多份存储" class="headerlink" title="Scrapy多份存储"></a>Scrapy多份存储</h3><p>Scrapy多份存储可以实现将爬取到的数据一份存储在本地，一份存储在数据库。实现过程如下：</p>
<p><strong>pipelines管道文件中，一个管道类对应一个数据存储平台</strong></p>
<p>在pipelines中添加<code>class mysqlPileLine(object):</code>，具体如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><span class="hljs-keyword">import</span> pymysql<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QiubaiproPipeline</span>:</span><br>    fp = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#重写父类的一个方法：该方法只在开始爬虫时被调用一次</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">open_spider</span>(<span class="hljs-params">self,spider</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;开始爬虫......&quot;</span>)<br>        self.fp = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./qiubai.txt&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br><br><br>    <span class="hljs-comment">#专门用来处理item类型对象</span><br>    <span class="hljs-comment">#该方法可以接收爬虫文件提交过来的item对象</span><br>    <span class="hljs-comment">#每当爬虫文件提交一次item时，该方法就会接收到一个item，就会被调用一次</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span>(<span class="hljs-params">self, item, spider</span>):</span><br>        author = item[<span class="hljs-string">&quot;author&quot;</span>]<br>        content = item[<span class="hljs-string">&quot;content&quot;</span>]<br>        self.fp.write(author+<span class="hljs-string">&quot;:&quot;</span>+content+<span class="hljs-string">&quot;\n&quot;</span>)<br>        <span class="hljs-keyword">return</span> item <span class="hljs-comment">#返回的item会被传递给下一个即将被执行的管道类</span><br><br>    <span class="hljs-comment">#只在结束时被调用一次</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">close_spider</span>(<span class="hljs-params">self,spider</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;结束爬虫！&quot;</span>)<br>        self.fp.close()<br><br><span class="hljs-comment">#管道文件中一个管道类对应将一组数据存储到一个平台或者载体中</span><br><span class="hljs-comment">#这里以传入MySQL为例</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">mysqlPileLine</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    conn = <span class="hljs-literal">None</span><br>    cursor = <span class="hljs-literal">None</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">open_spider</span>(<span class="hljs-params">self,spider</span>):</span><br>        self.conn = pymysql.connect(host=<span class="hljs-string">&quot;127.0.0.1&quot;</span>, port=<span class="hljs-number">3306</span>, user=<span class="hljs-string">&quot;root&quot;</span>, password=<span class="hljs-string">&quot;root123&quot;</span>, db=<span class="hljs-string">&quot;qiubai&quot;</span>)<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span>(<span class="hljs-params">self, item, spider</span>):</span><br>        self.cursor = self.conn.cursor()<br>        <span class="hljs-keyword">try</span>:<br>            self.cursor.execute(<span class="hljs-string">f&quot;insert into qb(author,content) values(&#x27;<span class="hljs-subst">&#123;item[<span class="hljs-string">&#x27;author&#x27;</span>]&#125;</span>&#x27;,&#x27;<span class="hljs-subst">&#123;item[<span class="hljs-string">&#x27;content&#x27;</span>]&#125;</span>&#x27;)&quot;</span>)<br>            self.conn.commit()<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-built_in">print</span>(e)<br>            self.conn.rollback()<br>        <span class="hljs-keyword">return</span> item <span class="hljs-comment">#返回的item会被传递给下一个即将被执行的管道类（虽然下一个没有管道类了，出于习惯，这里仍加上return item）</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">close_spider</span>(<span class="hljs-params">self, spider</span>):</span><br>        self.cursor.close()<br>        self.conn.close()<br></code></pre></td></tr></table></figure>

<p>问：爬虫文件提交的item类型的对象最终会提交给哪一个管道类呢？<br>答：在setting文件中添加修改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;qiubaiPro.pipelines.QiubaiproPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>   <span class="hljs-string">&#x27;qiubaiPro.pipelines.mysqlPileLine&#x27;</span>:<span class="hljs-number">301</span><br>    <span class="hljs-comment">#300表示优先级，数值越小优先级越高，当前类越先被执行</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>注：<strong>爬虫文件提交的item只会交给pipelines文件中的第一个被执行的管道类，后面即将执行的管道类需要接收得到第一个执行后的管道类返回的item（return item）才能执行类中的<code>process_item()</code>函数。</strong></p>
<p>补充数据库操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs mysql">sudo mysql.server start<br><br>mysql -u root -p<br><br>show databases;<br><br>create badabase qiubai;<br><br>create database qiubai DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br><br>use qiubai;<br><br>create table qb(<br>    author varchar(300) not null,<br>    content varchar(300) not null<br>    )default charset=&quot;utf8&quot;;<br>    <br>f&quot;insert into qb(author,content) values(&#x27;&#123;item[&#x27;author&#x27;]&#125;&#x27;,&#x27;&#123;item[&#x27;content&#x27;]&#125;&#x27;)&quot;<br><br>#这里需要注意：&#123;item[&#x27;author&#x27;]&#125;和&#123;item[&#x27;content&#x27;]&#125;格式化后要用引号括起来<br></code></pre></td></tr></table></figure>

<h3 id="Scrapy爬取图片名称"><a href="#Scrapy爬取图片名称" class="headerlink" title="Scrapy爬取图片名称"></a>Scrapy爬取图片名称</h3><ol>
<li><p>需求：爬取网站分页图片的名称</p>
</li>
<li><p>网址：<a target="_blank" rel="noopener" href="http://www.521609.com/ziliao/index.html">http://www.521609.com/ziliao/index.html</a></p>
</li>
<li><p>说明：创建项目以及修改seeting配置文件不再赘述，参考前文</p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XiaohuaSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;xiaohua&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;http://www.521609.com/ziliao/index.html&#x27;</span>]<br><br>    <span class="hljs-comment">#生成一个通用的url模板</span><br>    url = <span class="hljs-string">&quot;http://www.521609.com/ziliao/index_%d.html&quot;</span><br>    page_num = <span class="hljs-number">2</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        li_list = response.xpath(<span class="hljs-string">&#x27;/html/body/div[4]/div[2]/ul/li&#x27;</span>)<br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>            name = li.xpath(<span class="hljs-string">&#x27;./a[2]/h3/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            <span class="hljs-built_in">print</span>(name)<br><br>        <span class="hljs-keyword">if</span> self.page_num &lt;= <span class="hljs-number">136</span>:<br>            new_url = <span class="hljs-built_in">format</span>(self.url%self.page_num)<br>            self.page_num += <span class="hljs-number">1</span><br>            <span class="hljs-comment">#手动发送请求：callback递归回调函数是专门用于数据解析</span><br>            <span class="hljs-keyword">yield</span> scrapy.Request(url=new_url,callback=self.parse)<br></code></pre></td></tr></table></figure>

<p>地址的添加，不建议逐一放入start_urls列表中，当地址很多时，并不方便。采用如下手动添加的方式：</p>
<p>手动请求发送：<code>yield scrapy.Request(url,callback)</code></p>
</li>
</ol>
<h3 id="Scrapy五大核心组件与两大中间件"><a href="#Scrapy五大核心组件与两大中间件" class="headerlink" title="Scrapy五大核心组件与两大中间件"></a>Scrapy五大核心组件与两大中间件</h3><p>scrapy框架的五大核心组件和两大中间件是：<strong>爬虫（Spider）</strong>、<strong>爬虫中间件（Spider Middlewares）</strong>、<strong>引擎（Engine）</strong>、<strong>调度器（Scheduler）</strong>、<strong>下载中间件（Downloader Middlewares）</strong>、<strong>下载器（Downloader）</strong>、<strong>管道（Pipeline）</strong>，在前文的架构图中也提到过：</p>
<p><img src="/2021/05/07/%E7%88%AC%E8%99%AB/008i3skNly1gv6var2nydj60jg0dqjsf02.jpg" srcset="/img/loading.gif" lazyload></p>
<h4 id="爬虫：Spider"><a href="#爬虫：Spider" class="headerlink" title="爬虫：Spider"></a>爬虫：Spider</h4><p>组件爬虫类Spider的作用主要有两个：</p>
<ul>
<li>产生URL及子URL进行请求</li>
<li>得到响应response进行数据解析</li>
</ul>
<p><strong>第一步：爬虫类Spider将URL封装成请求对象传递给引擎Engine</strong></p>
<h4 id="爬虫中间件：Spider-Middlewares"><a href="#爬虫中间件：Spider-Middlewares" class="headerlink" title="爬虫中间件：Spider Middlewares"></a>爬虫中间件：Spider Middlewares</h4><p>爬虫中间件Spider Middlewares位于组件爬虫和组件Engine之间，很少用到，这里不多介绍。</p>
<h4 id="引擎：Engine"><a href="#引擎：Engine" class="headerlink" title="引擎：Engine"></a>引擎：Engine</h4><p>组件引擎是scrapy框架的中间组件，主要负责其他四个组件之间的数据交互：</p>
<ul>
<li>进行数据流处理和组件之间的数据传递</li>
<li>可以触发事务，感知各个组件中对象的实例化、方法的调用等</li>
</ul>
<p><strong>第二步：引擎Engine将请求对象传递给调度器Scheduler</strong></p>
<h4 id="调度器：Scheduler"><a href="#调度器：Scheduler" class="headerlink" title="调度器：Scheduler"></a>调度器：Scheduler</h4><p>组件调度器Scheduler由两部分组成：</p>
<ul>
<li>过滤器：将请求对象进行过滤处理（去重）后放入<strong>队列</strong>之中</li>
<li>队列：存放由过滤器过滤后的请求对象</li>
</ul>
<p><strong>第三步：调度器Scheduler将请求对象使用过滤器处理后，放入队列，然后调度队列中的请求对象传递给引擎Engine</strong></p>
<p><strong>第四步：引擎Engine将由调度器传递的请求对象传递给下载器Downloader</strong></p>
<h4 id="下载中间件：Downloader-Middlewares"><a href="#下载中间件：Downloader-Middlewares" class="headerlink" title="下载中间件：Downloader Middlewares"></a>下载中间件：Downloader Middlewares</h4><p>下载中间件Downloaders位于组件引擎Engine和组件下载器Downloader之间，主要作用是<strong>批量拦截整个工程的所有请求和响应</strong>。</p>
<ul>
<li><p>拦截请求：i. 如果需要单独对请求对象进行不同的UA伪装时，可以对每个                        请求对象进行UA伪装（<code>def process_request()</code>）</p>
<p>​                    ii. 如果需要单独对请求对象进行不同的代理IP时，可以对每个                        请求对象进行代理IP（<code>def process_exception:return </code></p>
<p>​                        <code>request</code>）</p>
</li>
<li><p>拦截响应：<strong>可以对响应的数据和对象进行篡改</strong>（当网页数据是Ajax的动态加载形式的时候，返回的响应response实际上是拿不到数据的。在使用xpath解析时，返回的是空值。此时，需要我们利用下载中间件手动拦截响应，对响应数据和对象进行修改。）</p>
</li>
</ul>
<h4 id="下载器：Downloader"><a href="#下载器：Downloader" class="headerlink" title="下载器：Downloader"></a>下载器：Downloader</h4><p>组件下载器Downloader的作用是与互联网连接，按照请求对象的要求，<strong>进行数据异步Twisted下载</strong>，封装成响应response。</p>
<p><strong>第五步：下载器Downloader根据请求对象从互联网上下载数据，封装成响应response传递给引擎Engine</strong></p>
<p><strong>第六步：引擎Engine将由下载器Downloader传递的响应response传递给爬虫类Spider（spider类中的<code>parse(self, response)</code></strong></p>
<p><strong>第七步：爬虫类Spider调用spider类中的<code>parse(self, response)</code>方法进行数据解析，解析后的数据封装到item对象中传递给引擎Engine</strong></p>
<p><strong>第八步：引擎Engine将由爬虫类Spider传递的item对象传递给管道Pipeline</strong></p>
<h4 id="管道：Pipeline"><a href="#管道：Pipeline" class="headerlink" title="管道：Pipeline"></a>管道：Pipeline</h4><p>组件管道Pipeline的作用是接收item对象，提供数据存储平台（本地、MySQ L数据库等）进行数据存储。</p>
<p><strong>第九步：管道Pipeline将item对象进行数据存储</strong></p>
<h3 id="Scrapy请求传参"><a href="#Scrapy请求传参" class="headerlink" title="Scrapy请求传参"></a>Scrapy请求传参</h3><p>如果爬取解析的数据不在同一张页面中，则需要用到scrapy请求传参数进行深度爬取。</p>
<h4 id="实战案例：BOSS直聘"><a href="#实战案例：BOSS直聘" class="headerlink" title="实战案例：BOSS直聘"></a>实战案例：BOSS直聘</h4><ol>
<li><p>需求：爬取boss直聘官网的岗位名称和描述</p>
</li>
<li><p>地址：<a target="_blank" rel="noopener" href="https://www.zhipin.com/c100010000-p100109/?ka=search_100109">https://www.zhipin.com/c100010000-p100109/?ka=search_100109</a></p>
</li>
<li><p>说明：创建项目以及修改seeting配置文件不再赘述，参考前文</p>
<p>​            所有的岗位名称在当前页面，而岗位描述在岗位名称各自对应链接页面，即不在同一个页面，需要深度用到请求传参</p>
<p>​            多张页面爬取可以参照上个案例中的手动请求</p>
</li>
<li><p>这里先挖一个坑：官网数据已经封装到Ajax的json数据中了，response.xpath无法获取数据了，后文下载中间件拦截响应可以篡改，详情可以参考后文。</p>
</li>
<li><p>代码如下：</p>
</li>
</ol>
<p>boss.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BossSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;boss&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://www.zhipin.com/wapi/zpgeek/view/job/card.json&#x27;</span>]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        li_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;main&quot;]/div/div[3]/ul/li&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(li_list)<br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>            job_name = li.xpath(<span class="hljs-string">&#x27;/div/div[1]/div[1]/div/div[1]/span[1]/a&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            <span class="hljs-built_in">print</span>(job_name)<br></code></pre></td></tr></table></figure>

<p>这里无法获取网页数据，暂时不再继续讲解，看后文中间件的内容后再来填坑。</p>
<h3 id="ImagesPipeline：Scrapy爬取图片"><a href="#ImagesPipeline：Scrapy爬取图片" class="headerlink" title="ImagesPipeline：Scrapy爬取图片"></a>ImagesPipeline：Scrapy爬取图片</h3><p>基于scrapy爬取字符串类型的数据和爬取图片二进制类型的数据的区别如下：</p>
<ul>
<li>字符串：只需要基于xpath进行解析且提交至管道Pipeline进行存储即可。</li>
<li>图片（二进制）：xpath解析出图片src的属性值，然后单独对图片地址发起请求获取图片二进制类型的数据。图片二进制数据有专门的图片管道ImagesPipeline，将图片地址提交到图片管道ImagesPipeline，而不是使用Pipeline管道。</li>
</ul>
<p>图片管道ImagesPipeline的作用是<strong>只需要将img的src属性值封装到item中，提交到图片管道ImagesPipeline，管道就会对图片的src进行请求发送获取图片的二进制类型数据并存储。</strong></p>
<h4 id="ImagesPipeline使用流程"><a href="#ImagesPipeline使用流程" class="headerlink" title="ImagesPipeline使用流程"></a>ImagesPipeline使用流程</h4><ol>
<li><p>数据解析：使用response.xpath()解析出图片地址</p>
</li>
<li><p>将图片地址提交到管道：在items文件定义图片地址Field对象，在爬虫文件中实例化item对象，存储图片地址，使用<code>yield item</code>提交到自定义的图片管道</p>
</li>
<li><p>在pipelines管道文件中自定义一个基于ImagesPipeLine的管道类：</p>
<ul>
<li>get_media_request(self,item,info)</li>
<li>file_path(self,request,response=None,info=None)</li>
<li>item_completed(self,results,item,info)</li>
</ul>
</li>
<li><p>在配置文件中指定图片存储路径：在setting文件最后添加一行：<code>IMAGES_STORE = &#39;./imgs_save&#39;</code></p>
</li>
<li><p>指定开启的管道：在setting文件中的60-70行类名指定为第3步中自定义的管道类，内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;imgsPro.pipelines.imgsPipeLine&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ol>
<h4 id="实战案例：爬取站长素材的高清图片"><a href="#实战案例：爬取站长素材的高清图片" class="headerlink" title="实战案例：爬取站长素材的高清图片"></a>实战案例：爬取站长素材的高清图片</h4><p>这里有个<strong>图片懒加载</strong>处理，全部文件代码如下：</p>
<p>img.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> imgsPro.items <span class="hljs-keyword">import</span> ImgsproItem<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ImgSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;img&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://sc.chinaz.com/tupian/&#x27;</span>]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        div_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;container&quot;]/div&#x27;</span>)<br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>            <span class="hljs-comment">#src = &#x27;https:&#x27; + div.xpath(&#x27;./div/a/img/@src&#x27;).extract()[0]</span><br>            <span class="hljs-comment">#该网站的图片采用的是懒加载方式，在图片进入可视界面后，才会加载图片地址。所以在获取图片地址时，要获取懒加载形式的伪属性src2</span><br>            src = <span class="hljs-string">&#x27;https:&#x27;</span> + div.xpath(<span class="hljs-string">&#x27;./div/a/img/@src2&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            <span class="hljs-built_in">print</span>(src)<br>            item = ImgsproItem()<br>            item[<span class="hljs-string">&quot;src&quot;</span>] = src<br>            <span class="hljs-keyword">yield</span> item<br></code></pre></td></tr></table></figure>

<p>items.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ImgsproItem</span>(<span class="hljs-params">scrapy.Item</span>):</span><br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    src = scrapy.Field()<br></code></pre></td></tr></table></figure>

<p>seeting文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for imgsPro project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;imgsPro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;imgsPro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;imgsPro.spiders&#x27;</span><br><br><br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment">#只显示指定类型的日志信息</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32</span><br><br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment">#DOWNLOAD_DELAY = 3</span><br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_IP = 16</span><br><br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment">#COOKIES_ENABLED = False</span><br><br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment">#TELNETCONSOLE_ENABLED = False</span><br><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment">#DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span><br><span class="hljs-comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;imgsPro.middlewares.ImgsproSpiderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment"># DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;imgsPro.middlewares.ImgsproDownloaderMiddleware&#x27;: 543,</span><br><span class="hljs-comment"># &#125;</span><br><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment">#EXTENSIONS = &#123;</span><br><span class="hljs-comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;imgsPro.pipelines.imgsPipeLine&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="hljs-comment">#AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment">#AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="hljs-comment"># each remote server</span><br><span class="hljs-comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment">#AUTOTHROTTLE_DEBUG = False</span><br><br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="hljs-comment">#HTTPCACHE_ENABLED = True</span><br><span class="hljs-comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="hljs-comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="hljs-comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="hljs-comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br><br>IMAGES_STORE = <span class="hljs-string">&#x27;./imgs_save&#x27;</span><br><br>MEDIA_ALLOW_REDIRECTS = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>

<p>pipelines.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-comment">#from itemadapter import ItemAdapter</span><br><span class="hljs-keyword">from</span> scrapy.pipelines.images <span class="hljs-keyword">import</span> ImagesPipeline<br><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-comment"># class ImgsproPipeline:</span><br><span class="hljs-comment">#     def process_item(self, item, spider):</span><br><span class="hljs-comment">#         return item</span><br><br><span class="hljs-comment">#ImagesPipeline是专门用于图片文件下载的管道类，下载过程支持异步和多线程</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">imgsPipeLine</span>(<span class="hljs-params">ImagesPipeline</span>):</span><br>    <span class="hljs-comment">#重写父类的三个方法：get_media_request(self,item,info)、file_path(self,request,response=None,info=None)、item_completed(self,results,item,info)</span><br><br>    <span class="hljs-comment">#根据图片地址进行发送数据请求</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_media_request</span>(<span class="hljs-params">self,item,info</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;我是管道里接收的地址：&quot;</span>,item[<span class="hljs-string">&quot;src&quot;</span>])<br>        <span class="hljs-keyword">yield</span> scrapy.Request(item[<span class="hljs-string">&quot;src&quot;</span>])<br><br>    <span class="hljs-comment">#自定义图片的名称</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">file_path</span>(<span class="hljs-params">self,request,response=<span class="hljs-literal">None</span>,info=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;我是管道里的request:&quot;</span>,request)<br>        url = request.url<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;我是管道里的request.url:&quot;</span>, url)<br>        file_name = url.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">return</span> file_name<br><br>    <span class="hljs-comment">#该返回值会传递给下一个即将被执行的管道类</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">item_completed</span>(<span class="hljs-params">self,results,item,info</span>):</span><br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure>

<h4 id="实战案例：中间件的使用"><a href="#实战案例：中间件的使用" class="headerlink" title="实战案例：中间件的使用"></a>实战案例：中间件的使用</h4><p>middle.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MiddleSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;middle&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://www.baidu.com/s?wd=ip&#x27;</span>]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        page_text = response.text<br><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;ip.html&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fp:<br>            fp.write(page_text)<br></code></pre></td></tr></table></figure>

<p>middlewares.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your spider middleware</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> signals<br><span class="hljs-keyword">import</span> random<br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> is_item, ItemAdapter<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MiddleproDownloaderMiddleware</span>:</span><br>    <span class="hljs-comment"># Not all methods need to be defined. If a method is not defined,</span><br>    <span class="hljs-comment"># scrapy acts as if the downloader middleware does not modify the</span><br>    <span class="hljs-comment"># passed objects.</span><br><br>    <span class="hljs-comment">#UA池</span><br>    user_agent_list = [<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span>,<br>        <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 &quot;</span><br>        <span class="hljs-string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span><br>    ]<br><br>    <span class="hljs-comment">#代理池</span><br>    PROXY_http = [<br>        <span class="hljs-string">&#x27;153.180.102.104:80&#x27;</span>,<br>        <span class="hljs-string">&#x27;195.208.131.189:56055&#x27;</span>,<br>    ]<br>    PROXY_https = [<br>        <span class="hljs-string">&#x27;120.83.49.90:9000&#x27;</span>,<br>        <span class="hljs-string">&#x27;95.189.112.214:35508&#x27;</span>,<br>    ]<br><br>    <span class="hljs-comment">#拦截正常的请求（不是所有的请求，还有一部分异常请求在第三个方法中拦截）</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_request</span>(<span class="hljs-params">self, request, spider</span>):</span><br>        <span class="hljs-comment">#UA伪装，重新赋值不同的值</span><br>        request.headers[<span class="hljs-string">&quot;User-Agent&quot;</span>] = random.choice(self.user_agent_list)<br>        <span class="hljs-comment">#IP代理是在拦截到异常请求中进行代理</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment">#拦截所有的响应</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_response</span>(<span class="hljs-params">self, request, response, spider</span>):</span><br>        <span class="hljs-comment"># Called with the response returned from the downloader.</span><br><br>        <span class="hljs-comment"># Must either;</span><br>        <span class="hljs-comment"># - return a Response object</span><br>        <span class="hljs-comment"># - return a Request object</span><br>        <span class="hljs-comment"># - or raise IgnoreRequest</span><br>        <span class="hljs-keyword">return</span> response<br><br>    <span class="hljs-comment">#拦截发生异常的请求</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_exception</span>(<span class="hljs-params">self, request, exception, spider</span>):</span><br>        <span class="hljs-comment">#当门户网站检测到程序访问时，封禁掉异常请求的IP，此时该异常的请求就会被拦截到</span><br>        <span class="hljs-comment">#IP代理</span><br>        <span class="hljs-keyword">if</span> request.url.split(<span class="hljs-string">&quot;:&quot;</span>)[<span class="hljs-number">0</span>] == <span class="hljs-string">&quot;http&quot;</span>:<br>            request.meta[<span class="hljs-string">&quot;proxy&quot;</span>] = <span class="hljs-string">&quot;http://&quot;</span> + random.choice(self.PROXY_http)<br>        <span class="hljs-keyword">else</span>:<br>            request.meta[<span class="hljs-string">&quot;proxy&quot;</span>] = <span class="hljs-string">&quot;https://&quot;</span> + random.choice(self.PROXY_https)<br><br>        <span class="hljs-keyword">return</span> request  <span class="hljs-comment">#将修正后的request返回重新发送</span><br></code></pre></td></tr></table></figure>

<p>setting.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for middlePro project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;middlePro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;middlePro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;middlePro.spiders&#x27;</span><br><br><br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment">#只显示指定类型的日志信息</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32</span><br><br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment">#DOWNLOAD_DELAY = 3</span><br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_IP = 16</span><br><br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment">#COOKIES_ENABLED = False</span><br><br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment">#TELNETCONSOLE_ENABLED = False</span><br><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment">#DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span><br><span class="hljs-comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;middlePro.middlewares.MiddleproSpiderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br>DOWNLOADER_MIDDLEWARES = &#123;<br>   <span class="hljs-string">&#x27;middlePro.middlewares.MiddleproDownloaderMiddleware&#x27;</span>: <span class="hljs-number">543</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment">#EXTENSIONS = &#123;</span><br><span class="hljs-comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><span class="hljs-comment">#ITEM_PIPELINES = &#123;</span><br><span class="hljs-comment">#    &#x27;middlePro.pipelines.MiddleproPipeline&#x27;: 300,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="hljs-comment">#AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment">#AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="hljs-comment"># each remote server</span><br><span class="hljs-comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment">#AUTOTHROTTLE_DEBUG = False</span><br><br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="hljs-comment">#HTTPCACHE_ENABLED = True</span><br><span class="hljs-comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="hljs-comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="hljs-comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="hljs-comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br></code></pre></td></tr></table></figure>

<h4 id="实战案例：爬取网易新闻标题和内容"><a href="#实战案例：爬取网易新闻标题和内容" class="headerlink" title="实战案例：爬取网易新闻标题和内容"></a>实战案例：爬取网易新闻标题和内容</h4><ol>
<li>通过网易新闻首页解析出四大板块对应的详情页的url（非动态加载）</li>
<li>每个板块下的新闻标题都是动态加载的</li>
<li>通过解析出每一条新闻详情页的url获取详情页的源码，解析出新闻内容（非动态加载）</li>
</ol>
<p><strong>注：从下载器到引擎的响应对象是不含动态加载的数据的，因此，需要通过下载器中间件，拦截响应对象进行篡改，使响应数据包含动态加载数据。</strong></p>
<p>wangyi.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> wangyiPro.items <span class="hljs-keyword">import</span> WangyiproItem<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WangyiSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;wangyi&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://news.163.com&#x27;</span>]<br>    models_url = [] <span class="hljs-comment">#存储四大板块对应的详情页地址url</span><br><br>    <span class="hljs-comment">#实例化一个浏览器对象</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        self.bro = webdriver.Chrome(executable_path=<span class="hljs-string">&#x27;/Users/lee/Desktop/MyStudy/PyProjects/Reptile/chromedriver&#x27;</span>)<br><br>    <span class="hljs-comment">#解析出四大板块对应的详情页url</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        li_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;index2016_wrap&quot;]/div[1]/div[2]/div[2]/div[2]/div[2]/div/ul/li&#x27;</span>)<br>        a_list = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]<br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> a_list:<br>            model_url = li_list[li].xpath(<span class="hljs-string">&#x27;./a/@href&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            self.models_url.append(model_url)<br>        <span class="hljs-comment"># print(self.models_url)</span><br><br>        <span class="hljs-comment">#依次对每个板块对应的页面进行请求</span><br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> self.models_url:<br>            <span class="hljs-comment">#对每一个板块进行手动发送请求</span><br>            <span class="hljs-keyword">yield</span> scrapy.Request(url,callback=self.parse_model)<br><br>    <span class="hljs-comment">#每一个板块对应的新闻标题相关的内容都是动态加载的</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_model</span>(<span class="hljs-params">self,response</span>):</span><span class="hljs-comment">#解析每一个板块页面中对应新闻的标题和新闻详情页的url</span><br>        div_list = response.xpath(<span class="hljs-string">&#x27;/html/body/div/div[3]/div[4]/div[1]/div[1]/div/ul/li/div/div&#x27;</span>)<br>             <span class="hljs-comment"># air                &#x27;/html/body/div/div[3]/div[4]/div[1]/div[1]/div/ul/li/div/div&#x27;</span><br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> div_list:<br>            news_title = div.xpath(<span class="hljs-string">&#x27;.//h3/a/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            news_url = div.xpath(<span class="hljs-string">&#x27;.//h3/a/@href&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br><br>            item = WangyiproItem()<br>            item[<span class="hljs-string">&quot;news_title&quot;</span>] = news_title<br><br>            <span class="hljs-comment">#对新闻页详细内容发起请求</span><br>            <span class="hljs-comment">#使用meta进行item的请求传参</span><br>            <span class="hljs-keyword">yield</span> scrapy.Request(news_url,callback=self.parse_news,meta=&#123;<span class="hljs-string">&quot;item&quot;</span>:item&#125;)<br><br>    <span class="hljs-comment">#解析新闻内容</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_news</span>(<span class="hljs-params">self,response</span>):</span><br>        news_content = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;content&quot;]/div[2]//text()&#x27;</span>).extract()<br>        news_content = <span class="hljs-string">&quot;&quot;</span>.join(news_content)<br>        <span class="hljs-comment">#接收请求传参的item</span><br>        item = response.meta[<span class="hljs-string">&quot;item&quot;</span>]<br>        item[<span class="hljs-string">&quot;news_content&quot;</span>] = news_content<br><br>        <span class="hljs-keyword">yield</span> item<br><br>    <span class="hljs-comment">#关闭浏览器</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">closed</span>(<span class="hljs-params">self,spider</span>):</span><br>        self.bro.quit()<br></code></pre></td></tr></table></figure>

<p>setting.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for wangyiPro project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;wangyiPro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;wangyiPro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;wangyiPro.spiders&#x27;</span><br><br><br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment">#只显示指定类型的日志信息</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32</span><br><br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment">#DOWNLOAD_DELAY = 3</span><br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_IP = 16</span><br><br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment">#COOKIES_ENABLED = False</span><br><br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment">#TELNETCONSOLE_ENABLED = False</span><br><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment">#DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span><br><span class="hljs-comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;wangyiPro.middlewares.WangyiproSpiderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br>DOWNLOADER_MIDDLEWARES = &#123;<br>   <span class="hljs-string">&#x27;wangyiPro.middlewares.WangyiproDownloaderMiddleware&#x27;</span>: <span class="hljs-number">543</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment">#EXTENSIONS = &#123;</span><br><span class="hljs-comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;wangyiPro.pipelines.WangyiproPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="hljs-comment">#AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment">#AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="hljs-comment"># each remote server</span><br><span class="hljs-comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment">#AUTOTHROTTLE_DEBUG = False</span><br><br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="hljs-comment">#HTTPCACHE_ENABLED = True</span><br><span class="hljs-comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="hljs-comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="hljs-comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="hljs-comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br></code></pre></td></tr></table></figure>

<p>middlewares.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your spider middleware</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> signals<br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> is_item, ItemAdapter<br><span class="hljs-keyword">from</span> scrapy.http <span class="hljs-keyword">import</span> HtmlResponse<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WangyiproDownloaderMiddleware</span>:</span><br>    <span class="hljs-comment"># Not all methods need to be defined. If a method is not defined,</span><br>    <span class="hljs-comment"># scrapy acts as if the downloader middleware does not modify the</span><br>    <span class="hljs-comment"># passed objects.</span><br><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_request</span>(<span class="hljs-params">self, request, spider</span>):</span><br>        <span class="hljs-comment"># Called for each request that goes through the downloader</span><br>        <span class="hljs-comment"># middleware.</span><br><br>        <span class="hljs-comment"># Must either:</span><br>        <span class="hljs-comment"># - return None: continue processing this request</span><br>        <span class="hljs-comment"># - or return a Response object</span><br>        <span class="hljs-comment"># - or return a Request object</span><br>        <span class="hljs-comment"># - or raise IgnoreRequest: process_exception() methods of</span><br>        <span class="hljs-comment">#   installed downloader middleware will be called</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment">#该方法拦截四大板块对应的响应对象，进行篡改</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_response</span>(<span class="hljs-params">self, request, response, spider</span>):</span><span class="hljs-comment">#spider是爬虫对象</span><br>        <span class="hljs-comment">#获取爬虫文件中的浏览器对象</span><br>        bro = spider.bro<br><br>        <span class="hljs-comment">#凡是经过爬虫文件的正常请求响应对象都会被拦截到，这里有5个响应：起始url的响应+四大板块的响应</span><br>        <span class="hljs-comment">#我们只需要挑选出指定的四大板块的响应对象进行篡改即可</span><br>        <span class="hljs-comment">#通过url指定request</span><br>        <span class="hljs-comment">#通过request指定response</span><br>        <span class="hljs-keyword">if</span> request.url <span class="hljs-keyword">in</span> spider.models_url:    <span class="hljs-comment">#四大板块对应的响应对象</span><br>            <span class="hljs-comment">#四大板块进行url请求</span><br>            bro.get(request.url)<br>            page_text = bro.page_source     <span class="hljs-comment">#获得页面源码数据，包含了动态加载的数据</span><br><br>            <span class="hljs-comment">#针对定位的响应对象response进行篡改</span><br>            <span class="hljs-comment">#实例化一个新的响应对象（符合需求：包含动态加载的响应数据），替代原来的响应对象</span><br>            <span class="hljs-comment">#获取动态加载的响应对象数据：基于selenium可以便捷的获取动态加载的数据</span><br>            new_response = HtmlResponse(url=request.url,body=page_text,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>,request=request)<br>            <span class="hljs-comment">#url是响应对象对应的请求的url</span><br>            <span class="hljs-comment">#body是请求地址url对应响应对象的页面源码</span><br>            <span class="hljs-comment">#encoding编码方式：&quot;utf-8&quot;</span><br>            <span class="hljs-comment">#request是响应对象对应的原来的请求对象request</span><br>            <span class="hljs-keyword">return</span> new_response<br>        <span class="hljs-keyword">else</span>:                                  <span class="hljs-comment">#其他请求的响应对象</span><br>            <span class="hljs-keyword">return</span> response<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_exception</span>(<span class="hljs-params">self, request, exception, spider</span>):</span><br>        <span class="hljs-comment"># Called when a download handler or a process_request()</span><br>        <span class="hljs-comment"># (from other downloader middleware) raises an exception.</span><br><br>        <span class="hljs-comment"># Must either:</span><br>        <span class="hljs-comment"># - return None: continue processing this exception</span><br>        <span class="hljs-comment"># - return a Response object: stops process_exception() chain</span><br>        <span class="hljs-comment"># - return a Request object: stops process_exception() chain</span><br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>

<p>items.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WangyiproItem</span>(<span class="hljs-params">scrapy.Item</span>):</span><br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    news_title = scrapy.Field()<br>    news_content = scrapy.Field()<br></code></pre></td></tr></table></figure>

<p>pipelines.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WangyiproPipeline</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span>(<span class="hljs-params">self, item, spider</span>):</span><br>        <span class="hljs-built_in">print</span>(item)<br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure>

<h4 id="实战案例：填坑BOSS直聘"><a href="#实战案例：填坑BOSS直聘" class="headerlink" title="实战案例：填坑BOSS直聘"></a>实战案例：填坑BOSS直聘</h4><p>boss.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver<br><span class="hljs-keyword">from</span> bossPro.items <span class="hljs-keyword">import</span> BossproItem<br><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BossSpider</span>(<span class="hljs-params">scrapy.Spider</span>):</span><br>    name = <span class="hljs-string">&#x27;boss&#x27;</span><br>    <span class="hljs-comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span><br>    start_urls = [<span class="hljs-string">&#x27;https://www.zhipin.com/c100010000-p100109/?ka=search_100109&#x27;</span>]<br>    job_urls = []<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        self.bro = webdriver.Chrome(executable_path=<span class="hljs-string">&#x27;/Users/lee/Desktop/MyStudy/PyProjects/Reptile/chromedriver&#x27;</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span>(<span class="hljs-params">self, response</span>):</span><br>        li_list = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;main&quot;]/div/div[3]/ul/li&#x27;</span>)<br>        <span class="hljs-comment">#print(li_list)</span><br>        <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> li_list:<br>            job_name = li.xpath(<span class="hljs-string">&#x27;./div/div[1]/div[1]/div/div[1]/span[1]/a/text()&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            job_href = <span class="hljs-string">&#x27;https://www.zhipin.com&#x27;</span> + li.xpath(<span class="hljs-string">&#x27;./div/div[1]/div[1]/div/div[1]/span[1]/a/@href&#x27;</span>).extract()[<span class="hljs-number">0</span>]<br>            self.job_urls.append(job_href)<br>            <span class="hljs-built_in">print</span>(job_name)<br>            <span class="hljs-built_in">print</span>(job_href)<br>            item = BossproItem()<br>            item[<span class="hljs-string">&quot;job_name&quot;</span>] = job_name<br><br>        <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> self.job_urls:<br>            <span class="hljs-keyword">yield</span> scrapy.Request(url,callback=self.parse_job,meta=&#123;<span class="hljs-string">&quot;item&quot;</span>:item&#125;)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_job</span>(<span class="hljs-params">self,response</span>):</span><br>        job_content = response.xpath(<span class="hljs-string">&#x27;//*[@id=&quot;main&quot;]/div[3]/div/div[2]/div[2]/div[1]/div//text()&#x27;</span>).extract()<br>        job_content = <span class="hljs-string">&quot;&quot;</span>.join(job_content)<br>        item = response.meta[<span class="hljs-string">&quot;item&quot;</span>]<br>        item[<span class="hljs-string">&quot;job_content&quot;</span>] = job_content<br><br>        <span class="hljs-keyword">yield</span> item<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">closed</span>(<span class="hljs-params">self,spider</span>):</span><br>        self.bro.quit()<br></code></pre></td></tr></table></figure>

<p>setting.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for bossPro project</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For simplicity, this file contains only settings considered important or</span><br><span class="hljs-comment"># commonly used. You can find more settings consulting the documentation:</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="hljs-comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br>BOT_NAME = <span class="hljs-string">&#x27;bossPro&#x27;</span><br><br>SPIDER_MODULES = [<span class="hljs-string">&#x27;bossPro.spiders&#x27;</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">&#x27;bossPro.spiders&#x27;</span><br><br><br><span class="hljs-comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br>USER_AGENT = <span class="hljs-string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15&#x27;</span><br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span><br><br><span class="hljs-comment">#只显示指定类型的日志信息</span><br>LOG_LEVEL = <span class="hljs-string">&#x27;ERROR&#x27;</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32</span><br><br><span class="hljs-comment"># Configure a delay for requests for the same website (default: 0)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span><br><span class="hljs-comment"># See also autothrottle settings and docs</span><br><span class="hljs-comment">#DOWNLOAD_DELAY = 3</span><br><span class="hljs-comment"># The download delay setting will honor only one of:</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS_PER_IP = 16</span><br><br><span class="hljs-comment"># Disable cookies (enabled by default)</span><br><span class="hljs-comment">#COOKIES_ENABLED = False</span><br><br><span class="hljs-comment"># Disable Telnet Console (enabled by default)</span><br><span class="hljs-comment">#TELNETCONSOLE_ENABLED = False</span><br><br><span class="hljs-comment"># Override the default request headers:</span><br><span class="hljs-comment"># DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="hljs-comment">#   #&#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span><br><span class="hljs-comment">#    &#x27;Accept&#x27;: &#x27;application/json, text/javascript, */*; q=0.01&#x27;,</span><br><span class="hljs-comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span><br><span class="hljs-comment">#  &#x27;Referer&#x27;: &#x27;https://www.zhipin.com/c100010000-p100109/?ka=search_100109&#x27;,</span><br><span class="hljs-comment">#    &#x27;Cookie&#x27;:&#x27;__zp_stoken__=d494dPEs6fyEfIWdDECA6RXYeH1Z5fX4sGTU9fyEBIHZveRpiWSg3awBiIms8V3JNFyA2c099fWsMPjcVYzt%2BUjJsaDZUK3QPDQEfWkFefXhlR3JJdVxKUiNGRRxPbAkGVRkEWxwGDUBBZTk%3D; Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1633883873; Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1633882294; __a=35150882.1633882294..1633882294.3.1.3.3; __c=1633882294; __l=l=%2Fwww.zhipin.com%2Fc100010000-p100109%2F%3Fka%3Dsearch_100109&amp;r=https%3A%2F%2Fbaidu.com%2F&amp;g=%2Fwww.zhipin.com%2F%3Fsid%3Dsem_pz_bdpc_dasou_title&amp;s=3&amp;friend_source=0&amp;s=3&amp;friend_source=0; lastCity=100010000; __g=sem_pz_bdpc_dasou_title; sid=sem_pz_bdpc_dasou_title; acw_tc=0bdcced916338822954806045ed141f0d468adf805a2d298a60796992876bc&#x27;</span><br><span class="hljs-comment"># &#125;</span><br><br><span class="hljs-comment"># Enable or disable spider middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = &#123;</span><br><span class="hljs-comment">#    &#x27;bossPro.middlewares.BossproSpiderMiddleware&#x27;: 543,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Enable or disable downloader middlewares</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br>DOWNLOADER_MIDDLEWARES = &#123;<br>   <span class="hljs-string">&#x27;bossPro.middlewares.BossproDownloaderMiddleware&#x27;</span>: <span class="hljs-number">543</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable or disable extensions</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="hljs-comment">#EXTENSIONS = &#123;</span><br><span class="hljs-comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="hljs-comment">#&#125;</span><br><br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br>ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;bossPro.pipelines.BossproPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br><br><span class="hljs-comment"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="hljs-comment">#AUTOTHROTTLE_ENABLED = True</span><br><span class="hljs-comment"># The initial download delay</span><br><span class="hljs-comment">#AUTOTHROTTLE_START_DELAY = 5</span><br><span class="hljs-comment"># The maximum download delay to be set in case of high latencies</span><br><span class="hljs-comment">#AUTOTHROTTLE_MAX_DELAY = 60</span><br><span class="hljs-comment"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="hljs-comment"># each remote server</span><br><span class="hljs-comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="hljs-comment"># Enable showing throttling stats for every response received:</span><br><span class="hljs-comment">#AUTOTHROTTLE_DEBUG = False</span><br><br><span class="hljs-comment"># Enable and configure HTTP caching (disabled by default)</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="hljs-comment">#HTTPCACHE_ENABLED = True</span><br><span class="hljs-comment">#HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="hljs-comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span><br><span class="hljs-comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="hljs-comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span><br></code></pre></td></tr></table></figure>

<p>middlewares.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your spider middleware</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span><br><br><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> signals<br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> is_item, ItemAdapter<br><span class="hljs-keyword">from</span> scrapy.http <span class="hljs-keyword">import</span> HtmlResponse<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BossproDownloaderMiddleware</span>:</span><br>    <span class="hljs-comment"># Not all methods need to be defined. If a method is not defined,</span><br>    <span class="hljs-comment"># scrapy acts as if the downloader middleware does not modify the</span><br>    <span class="hljs-comment"># passed objects.</span><br><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_request</span>(<span class="hljs-params">self, request, spider</span>):</span><br>        <span class="hljs-comment"># Called for each request that goes through the downloader</span><br>        <span class="hljs-comment"># middleware.</span><br><br>        <span class="hljs-comment"># Must either:</span><br>        <span class="hljs-comment"># - return None: continue processing this request</span><br>        <span class="hljs-comment"># - or return a Response object</span><br>        <span class="hljs-comment"># - or return a Request object</span><br>        <span class="hljs-comment"># - or raise IgnoreRequest: process_exception() methods of</span><br>        <span class="hljs-comment">#   installed downloader middleware will be called</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_response</span>(<span class="hljs-params">self, request, response, spider</span>):</span><br>        bro = spider.bro<br>        <span class="hljs-comment"># if request.url in spider.start_urls:</span><br>        bro.get(request.url)<br>        page_text = bro.page_source<br>        new_response = HtmlResponse(url=request.url, body=page_text, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>, request=request)<br>        <span class="hljs-keyword">return</span> new_response<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_exception</span>(<span class="hljs-params">self, request, exception, spider</span>):</span><br>        <span class="hljs-comment"># Called when a download handler or a process_request()</span><br>        <span class="hljs-comment"># (from other downloader middleware) raises an exception.</span><br><br>        <span class="hljs-comment"># Must either:</span><br>        <span class="hljs-comment"># - return None: continue processing this exception</span><br>        <span class="hljs-comment"># - return a Response object: stops process_exception() chain</span><br>        <span class="hljs-comment"># - return a Request object: stops process_exception() chain</span><br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>

<p>items.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BossproItem</span>(<span class="hljs-params">scrapy.Item</span>):</span><br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    job_name = scrapy.Field()<br>    job_content = scrapy.Field()<br></code></pre></td></tr></table></figure>

<p>pipelines文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BossproPipeline</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span>(<span class="hljs-params">self, item, spider</span>):</span><br>        <span class="hljs-built_in">print</span>(item)<br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure>

<p>注：设置访问频率，否则会被封禁IP。</p>
<h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ol>
<li> <a target="_blank" rel="noopener" href="https://blog.csdn.net/feixuedongji/article/details/82984583">https://blog.csdn.net/feixuedongji/article/details/82984583</a></li>
<li> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/85760495">https://zhuanlan.zhihu.com/p/85760495</a></li>
<li> <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/8e78dfa7c368">https://www.jianshu.com/p/8e78dfa7c368</a></li>
<li> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/alexzhang92/p/9409243.html">https://www.cnblogs.com/alexzhang92/p/9409243.html</a></li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，只作学习笔记和分享交流作用，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/09/12/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">MySQL数据库【更新中】</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/01/02/Typora%E6%95%99%E7%A8%8B/">
                        <span class="hidden-mobile">Typora教程【已完结】</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"cTKjlyyUHtaSnlWTUahyiu2y-gzGzoHsz","appKey":"aNtyxf95epQJYLQAkuHkbd6K","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->

  <div class="col-lg-7 mx-auto nopadding-x-md">
    <div class="container custom post-custom mx-auto">
      <img src="https://tva1.sinaimg.cn/large/008i3skNly1gv1h4ngfimj60hm0hkmyf02.jpg" srcset="/img/loading.gif" lazyload title="打赏一下！" class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
    </div>
  </div>


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <i class="iconfont icon-love"></i> <a href="http://www.xiaojiucai.cn" target="_blank" rel="nofollow noopener"><span>一棵小韭菜</span></a> <i class="iconfont icon-love"></i> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
